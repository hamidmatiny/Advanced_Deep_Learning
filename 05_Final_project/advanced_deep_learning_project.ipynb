{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a391e47",
   "metadata": {},
   "source": [
    "# ðŸ”¥ Advanced Deep Learning Project\n",
    "## Combining From-Scratch Transformers with Hugging Face Fine-tuning\n",
    "\n",
    "This comprehensive project demonstrates:\n",
    "- **Part 1**: Building a Transformer from scratch (Karpathy-style implementation)\n",
    "- **Part 2**: Fine-tuning a Hugging Face model\n",
    "- **Comparison**: Before/after training performance\n",
    "- **Visualization**: Attention weights and generation samples\n",
    "\n",
    "### Project Goals\n",
    "1. Implement transformer architecture with attention mechanisms\n",
    "2. Train custom transformer on Shakespeare text\n",
    "3. Fine-tune pretrained Hugging Face model\n",
    "4. Generate samples from both models\n",
    "5. Visualize attention patterns\n",
    "6. Compare model performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741224bb",
   "metadata": {},
   "source": [
    "## Section 1: Setup Environment and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf89166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Uncomment if needed\n",
    "# !pip install torch transformers datasets matplotlib seaborn numpy scikit-learn -U\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset as HFDataset\n",
    "import urllib.request\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ“ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Shakespeare dataset\n",
    "print(\"Loading Shakespeare dataset...\")\n",
    "\n",
    "# Read from local file if exists, otherwise download\n",
    "if os.path.exists('input.txt'):\n",
    "    with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    print(\"âœ“ Loaded from local file\")\n",
    "else:\n",
    "    try:\n",
    "        url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "        urllib.request.urlretrieve(url, 'input.txt')\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        print(\"âœ“ Downloaded Shakespeare dataset\")\n",
    "    except:\n",
    "        print(\"Using minimal example text\")\n",
    "        text = \"To be or not to be, that is the question. \" * 100\n",
    "\n",
    "print(f\"Dataset size: {len(text):,} characters\")\n",
    "print(f\"First 200 characters:\\n{text[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68b6b4d",
   "metadata": {},
   "source": [
    "## Part 1: Build Transformer from Scratch (Karpathy-style)\n",
    "\n",
    "### Tokenization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf37a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build character-level tokenizer\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "def encode(s):\n",
    "    \"\"\"String to integers\"\"\"\n",
    "    return [char_to_idx[c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    \"\"\"Integers to string\"\"\"\n",
    "    return ''.join([idx_to_char[i] for i in l])\n",
    "\n",
    "# Tokenize dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"âœ“ Vocabulary size: {vocab_size}\")\n",
    "print(f\"âœ“ Tokenized data shape: {data.shape}\")\n",
    "print(f\"Vocabulary: {''.join(chars[:50])}...\")\n",
    "\n",
    "# Split into train/val\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f\"âœ“ Train set: {len(train_data):,}, Val set: {len(val_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65003bd8",
   "metadata": {},
   "source": [
    "### Attention Mechanism\n",
    "The scaled dot-product attention mechanism:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ebc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention\"\"\"\n",
    "    \n",
    "    def __init__(self, head_size, block_size, embed_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head_size = head_size\n",
    "        # Store attention weights for visualization\n",
    "        self.attention_weights = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # batch, time, channels\n",
    "        \n",
    "        k = self.key(x)      # (B, T, head_size)\n",
    "        q = self.query(x)    # (B, T, head_size)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)  # (B, T, T)\n",
    "        \n",
    "        # Apply causal mask\n",
    "        scores = scores.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax to get attention weights\n",
    "        att = F.softmax(scores, dim=-1)  # (B, T, T)\n",
    "        self.attention_weights = att.detach()  # Store for visualization\n",
    "        \n",
    "        att = self.dropout(att)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        v = self.value(x)    # (B, T, head_size)\n",
    "        out = att @ v        # (B, T, head_size)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention\"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size, block_size, embed_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(head_size, block_size, embed_dim, dropout) \n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.proj = nn.Linear(num_heads * head_size, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "    \n",
    "    def get_attention_weights(self):\n",
    "        \"\"\"Return attention weights from first head for visualization\"\"\"\n",
    "        all_weights = [h.att for h in self.heads]\n",
    "        \n",
    "        return torch.stack(all_weights).squeeze()\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-forward network\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, ff_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, block_size, ff_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        head_size = embed_dim // num_heads\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.mha = MultiHeadAttention(num_heads, head_size, block_size, embed_dim, dropout)\n",
    "        \n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = FeedForward(embed_dim, ff_dim, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))  # Residual connection + attention\n",
    "        x = x + self.ff(self.ln2(x))   # Residual connection + feed-forward\n",
    "        return x\n",
    "\n",
    "print(\"âœ“ Attention modules defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16230cb3",
   "metadata": {},
   "source": [
    "### Complete Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8693077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLanguageModel(nn.Module):\n",
    "    \"\"\"Complete Transformer Language Model\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=64, num_heads=4, num_layers=4, \n",
    "                 block_size=256, ff_dim=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, embed_dim)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim, num_heads, block_size, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        tok_emb = self.token_embedding_table(idx)           # (B, T, embed_dim)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))  # (T, embed_dim)\n",
    "        x = tok_emb + pos_emb                               # (B, T, embed_dim)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        x = self.blocks(x)                                  # (B, T, embed_dim)\n",
    "        x = self.ln_f(x)                                    # (B, T, embed_dim)\n",
    "        \n",
    "        # Logits\n",
    "        logits = self.lm_head(x)                           # (B, T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"Generate text\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to block_size\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Apply top-k filtering\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "            # Sample\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "# Model configuration\n",
    "config = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'embed_dim': 64,\n",
    "    'num_heads': 4,\n",
    "    'num_layers': 3,\n",
    "    'block_size': 128,\n",
    "    'ff_dim': 256,\n",
    "    'dropout': 0.1,\n",
    "}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_scratch = TransformerLanguageModel(**config).to(device)\n",
    "total_params = sum(p.numel() for p in model_scratch.parameters())\n",
    "print(f\"âœ“ Model created with {total_params:,} parameters\")\n",
    "print(f\"Model config: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1d8f93",
   "metadata": {},
   "source": [
    "### Training the From-Scratch Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70c398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, batch_size=32, block_size=128):\n",
    "    \"\"\"Generate batch of data\"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters=50):\n",
    "    \"\"\"Estimate loss on train and val sets\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size=32, block_size=config['block_size'])\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10\n",
    "eval_interval = 50\n",
    "eval_iters = 20\n",
    "\n",
    "optimizer = torch.optim.AdamW(model_scratch.parameters(), lr=learning_rate)\n",
    "\n",
    "# Track training metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_scratch.train()\n",
    "    \n",
    "    # Training phase\n",
    "    epoch_loss = 0\n",
    "    num_batches = 100\n",
    "    \n",
    "    for step in range(num_batches):\n",
    "        X, Y = get_batch('train', batch_size=32, block_size=config['block_size'])\n",
    "        \n",
    "        logits, loss = model_scratch(X, Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_scratch.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / num_batches\n",
    "    train_losses.append(avg_epoch_loss)\n",
    "    \n",
    "    # Validation\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        losses = estimate_loss(model_scratch, eval_iters)\n",
    "        val_losses.append(losses['val'].item())\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}/{num_epochs} | Train Loss: {avg_epoch_loss:.4f} | Val Loss: {losses['val']:.4f}\")\n",
    "        \n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            torch.save(model_scratch.state_dict(), 'model_scratch_best.pt')\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"âœ“ Training complete!\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "if val_losses:\n",
    "    # Adjust x-axis for val losses\n",
    "    val_x = [i * 2 for i in range(len(val_losses))]\n",
    "    plt.plot(val_x, val_losses, label='Val Loss', linewidth=2, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('From-Scratch Transformer: Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_progress_scratch.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6641b121",
   "metadata": {},
   "source": [
    "## Part 2: Fine-tune Hugging Face Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c41e31",
   "metadata": {},
   "source": [
    "### Load Pretrained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296e33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1bb774",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Hugging Face model...\")\n",
    "\n",
    "# Load tokenizer and model (using gpt2-medium for balance between performance and speed)\n",
    "model_name = \"gpt2\"\n",
    "tokenizer_hf = AutoTokenizer.from_pretrained(model_name)\n",
    "model_hf_pretrained = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "print(f\"âœ“ Loaded {model_name}\")\n",
    "print(f\"  - Vocab size: {tokenizer_hf.vocab_size}\")\n",
    "print(f\"  - Model parameters: {sum(p.numel() for p in model_hf_pretrained.parameters()):,}\")\n",
    "print(f\"  - Device: {device}\")\n",
    "\n",
    "# Set pad token\n",
    "if tokenizer_hf.pad_token is None:\n",
    "    tokenizer_hf.pad_token = tokenizer_hf.eos_token\n",
    "\n",
    "print(f\"âœ“ Tokenizer configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7b1890",
   "metadata": {},
   "source": [
    "### Prepare Dataset for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0404941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the Shakespeare text\n",
    "print(\"Tokenizing dataset for Hugging Face...\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize function for dataset\"\"\"\n",
    "    return tokenizer_hf(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=False,\n",
    "        max_length=512,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "\n",
    "# Create dataset\n",
    "dataset_dict = {\n",
    "    'input_ids': tokenizer_hf.encode(text),\n",
    "}\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.9 * len(dataset_dict['input_ids']))\n",
    "train_input_ids = dataset_dict['input_ids'][:train_size]\n",
    "val_input_ids = dataset_dict['input_ids'][train_size:]\n",
    "\n",
    "print(f\"âœ“ Dataset prepared\")\n",
    "print(f\"  - Train tokens: {len(train_input_ids):,}\")\n",
    "print(f\"  - Val tokens: {len(val_input_ids):,}\")\n",
    "\n",
    "# Create simple dataset class\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, seq_length=128):\n",
    "        self.input_ids = input_ids\n",
    "        self.seq_length = seq_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(1, len(self.input_ids) - self.seq_length)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.input_ids[idx:idx + self.seq_length]\n",
    "        labels = self.input_ids[idx + 1:idx + self.seq_length + 1]\n",
    "        \n",
    "        # Pad if necessary\n",
    "        if len(input_ids) < self.seq_length:\n",
    "            input_ids = input_ids + [tokenizer_hf.pad_token_id] * (self.seq_length - len(input_ids))\n",
    "            labels = labels + [-100] * (self.seq_length - len(labels))\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long),\n",
    "            'attention_mask': torch.ones(self.seq_length, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataset = TextDataset(train_input_ids, seq_length=128)\n",
    "val_dataset = TextDataset(val_input_ids, seq_length=128)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "print(f\"âœ“ DataLoaders created\")\n",
    "print(f\"  - Train batches: {len(train_dataloader)}\")\n",
    "print(f\"  - Val batches: {len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d395d",
   "metadata": {},
   "source": [
    "## Fine-tune with HuggingFace `Trainer` API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4c910",
   "metadata": {},
   "source": [
    "### Copying And Preparing The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, EarlyStoppingCallback\n",
    "\n",
    "# 1. Create a deep copy of the model for fine-tuning\n",
    "print(\"Creating a copy of the model for fine-tuning...\")\n",
    "# This ensures model_hf_finetuned is a totally different object in memory\n",
    "model_hf_finetuned = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "print(f\"Original model object ID: {id(model_hf_pretrained)}\")\n",
    "print(f\"Finetuned model object ID: {id(model_hf_finetuned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34f7467",
   "metadata": {},
   "source": [
    "### High-performance Trainer Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e419571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Setup Collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer_hf, mlm=False)\n",
    "\n",
    "# 3. Detect Hardware-specific Optimizations\n",
    "use_fp16 = True if torch.cuda.is_available() else False\n",
    "\n",
    "# 4. Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-shakespeare-results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=100,\n",
    "    fp16=use_fp16,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False, # We want the LOWEST loss\n",
    "    eval_strategy=\"steps\", # Evaluate more often (e.g., every 500 steps)\n",
    "    eval_steps=500,\n",
    ")\n",
    "\n",
    "# 5. Initialize the Trainer with the NEW model object\n",
    "trainer = Trainer(\n",
    "    model=model_hf_finetuned,        # Training the copy\n",
    "    args=training_args,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "model_hf_finetuned.config.resid_pdrop = 0.2  # Dropout for residual connections\n",
    "model_hf_finetuned.config.embd_pdrop = 0.2   # Dropout for input embeddings\n",
    "model_hf_finetuned.config.attn_pdrop = 0.2\n",
    "# 6. Start Training\n",
    "print(\"Starting fine-tuning on the new model object...\")\n",
    "trainer.train()\n",
    "\n",
    "# 7. Save the Fine-tuned model\n",
    "trainer.save_model(\"./model_hf_shakespeare_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a701cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison Test\n",
    "prompt = \"The world is\"\n",
    "\n",
    "# Original Model\n",
    "inputs = tokenizer_hf(prompt, return_tensors=\"pt\").to(device)\n",
    "out_orig = model_hf_pretrained.generate(**inputs, max_length=20)\n",
    "\n",
    "# Fine-tuned Model\n",
    "out_fine = model_hf_finetuned.generate(**inputs, max_length=20)\n",
    "\n",
    "print(\"Original GPT-2:\", tokenizer_hf.decode(out_orig[0]))\n",
    "print(\"Shakespeare GPT-2:\", tokenizer_hf.decode(out_fine[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13950009",
   "metadata": {},
   "source": [
    "## Generate Samples from Both Models\n",
    "\n",
    "### Sample Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcd3149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation prompts\n",
    "prompts = [\n",
    "    \"To be or not to be\",\n",
    "    \"The love of my life\",\n",
    "    \"Once upon a time\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GENERATION COMPARISON: Before vs After Fine-tuning\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_samples = {}\n",
    "\n",
    "# Generate from scratch transformer\n",
    "print(\"\\nðŸ“ From-Scratch Transformer (After Training)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "model_scratch.eval()\n",
    "all_samples['scratch'] = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for prompt in prompts:\n",
    "        # Encode prompt\n",
    "        prompt_tokens = encode(prompt)\n",
    "        prompt_tensor = torch.tensor(prompt_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Generate\n",
    "        generated = model_scratch.generate(\n",
    "            prompt_tensor,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.8,\n",
    "            top_k=50\n",
    "        )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = decode(generated.squeeze(0).cpu().tolist())\n",
    "        all_samples['scratch'][prompt] = generated_text\n",
    "        \n",
    "        print(f\"\\nPrompt: '{prompt}'\")\n",
    "        print(f\"Generated: {generated_text[:200]}...\")\n",
    "\n",
    "# Generate from Hugging Face model (pretrained)\n",
    "print(\"\\n\\nðŸ“ Hugging Face Model (Before Fine-tuning - Pretrained)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "model_hf_pretrained.eval()\n",
    "all_samples['hf_pretrained'] = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for prompt in prompts:\n",
    "        input_ids = tokenizer_hf.encode(prompt, return_tensors='pt').to(device)\n",
    "        \n",
    "        generated = model_hf_pretrained.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.8,\n",
    "            top_k=50,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer_hf.eos_token_id\n",
    "        )\n",
    "        \n",
    "        generated_text = tokenizer_hf.decode(generated[0], skip_special_tokens=True)\n",
    "        all_samples['hf_pretrained'][prompt] = generated_text\n",
    "        \n",
    "        print(f\"\\nPrompt: '{prompt}'\")\n",
    "        print(f\"Generated: {generated_text[:200]}...\")\n",
    "\n",
    "# Generate from Hugging Face model (fine-tuned)\n",
    "print(\"\\n\\nðŸ“ Hugging Face Model (After Fine-tuning)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "model_hf_finetuned.eval()\n",
    "all_samples['hf_finetuned'] = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for prompt in prompts:\n",
    "        input_ids = tokenizer_hf.encode(prompt, return_tensors='pt').to(device)\n",
    "        \n",
    "        generated = model_hf_finetuned.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.8,\n",
    "            top_k=50,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer_hf.eos_token_id\n",
    "        )\n",
    "        \n",
    "        generated_text = tokenizer_hf.decode(generated[0], skip_special_tokens=True)\n",
    "        all_samples['hf_finetuned'][prompt] = generated_text\n",
    "        \n",
    "        print(f\"\\nPrompt: '{prompt}'\")\n",
    "        print(f\"Generated: {generated_text[:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91a0511",
   "metadata": {},
   "source": [
    "## Compare Before and After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08814b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = trainer.evaluate()\n",
    "baseline_loss = baseline_results['eval_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea065dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_loss = trainer.evaluate(eval_dataset=val_dataset) ['eval_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded3878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training metrics from trainer if available\n",
    "\n",
    "train_losses_hf = []\n",
    "val_losses_hf = []\n",
    "train_steps_hf = []  # Changed from train_steps to train_steps_hf\n",
    "val_steps_hf = []    # Changed from val_steps to val_steps_hf\n",
    "\n",
    "if hasattr(trainer, 'state') and trainer.state.log_history:\n",
    "    for log in trainer.state.log_history:\n",
    "        if 'loss' in log:\n",
    "            train_losses_hf.append(log['loss'])\n",
    "            train_steps_hf.append(log['step'])\n",
    "        if 'eval_loss' in log:\n",
    "            val_losses_hf.append(log['eval_loss'])\n",
    "            val_steps_hf.append(log['step'])\n",
    "\n",
    "# Ensure both lists have the same length\n",
    "min_len = min(len(train_losses_hf), len(val_losses_hf))\n",
    "final_train_loss_hf = train_losses_hf[-1] if train_losses_hf else 0.0\n",
    "final_val_loss_hf = val_losses_hf[-1] if val_losses_hf else 0.0\n",
    "num_epochs_hf = len(val_losses_hf)\n",
    "\n",
    "epochs_hf = list(range(1, len(val_losses_hf) + 1))\n",
    "\n",
    "# Create comprehensive comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: From-scratch transformer training\n",
    "ax = axes[0, 0]\n",
    "ax.plot(train_losses, linewidth=2, color='#1f77b4')\n",
    "if val_losses:\n",
    "    val_x = [i * 2 for i in range(len(val_losses))]\n",
    "    ax.plot(val_x, val_losses, linewidth=2, marker='o', color='#ff7f0e', label='Val')\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.set_title('From-Scratch Transformer: Training Progress', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: Hugging Face fine-tuning\n",
    "ax = axes[0, 1]\n",
    "if train_losses_hf:\n",
    "    # Use train_steps_hf as X instead of epochs_hf\n",
    "    ax.plot(train_steps_hf, train_losses_hf, linewidth=2, label='Train', color='#2ca02c')\n",
    "\n",
    "if val_losses_hf:\n",
    "    # Use val_steps_hf as X\n",
    "    ax.plot(val_steps_hf, val_losses_hf, linewidth=2, marker='s', label='Val', color='#d62728')\n",
    "\n",
    "ax.set_xlabel('Steps', fontsize=11) # Changed label from Epoch to Steps\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.set_title('Hugging Face Model: Fine-tuning Progress', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# Plot 3: Model comparison - final losses\n",
    "ax = axes[1, 0]\n",
    "models = ['From-Scratch\\n(Trained)', 'HF Pretrained', 'HF Fine-tuned']\n",
    "final_losses = [\n",
    "    train_losses[-1] if train_losses else 0,\n",
    "    0,  # No training loss for pretrained\n",
    "    val_losses_hf[-1] if val_losses_hf else 0\n",
    "]\n",
    "colors = ['#1f77b4', '#aec7e8', '#ff7f0e']\n",
    "bars = ax.bar(models, final_losses, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.set_title('Final Loss Comparison', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    if height > 0:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Plot 4: Model sizes\n",
    "ax = axes[1, 1]\n",
    "model_sizes = [\n",
    "    sum(p.numel() for p in model_scratch.parameters()) / 1e6,\n",
    "    sum(p.numel() for p in model_hf_pretrained.parameters()) / 1e6,\n",
    "    sum(p.numel() for p in model_hf_finetuned.parameters()) / 1e6\n",
    "]\n",
    "bars = ax.bar(models, model_sizes, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Parameters (Millions)', fontsize=11)\n",
    "ax.set_title('Model Size Comparison', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}M',\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Comparison visualization saved\")\n",
    "\n",
    "# Create a summary table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_data = {\n",
    "    'Model': ['From-Scratch Transformer', 'HF Pretrained (GPT-2)', 'HF Fine-tuned'],\n",
    "    'Parameters (M)': [f'{model_sizes[0]:.2f}', f'{model_sizes[1]:.2f}', f'{model_sizes[2]:.2f}'],\n",
    "    'Final Train Loss': [\n",
    "        f'{train_losses[-1]:.4f}' if train_losses else 'N/A', \n",
    "        'N/A', \n",
    "        f'{final_train_loss_hf:.4f}' if train_losses_hf else 'N/A'\n",
    "    ],\n",
    "    'Final Val Loss': [\n",
    "        f'{val_losses[-1]:.4f}' if val_losses else 'N/A', \n",
    "        f'{baseline_loss:.4f}' if 'baseline_loss' in locals() else 'N/A', \n",
    "        f'{final_val_loss_hf:.4f}' if val_losses_hf else 'N/A'\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"\\n{:<30} {:<15} {:<18} {:<18}\".format('Model', 'Parameters (M)', 'Final Train Loss', 'Final Val Loss'))\n",
    "print(\"-\" * 80)\n",
    "for i in range(len(summary_data['Model'])):\n",
    "    print(\"{:<30} {:<15} {:<18} {:<18}\".format(\n",
    "        summary_data['Model'][i],\n",
    "        summary_data['Parameters (M)'][i],\n",
    "        summary_data['Final Train Loss'][i],\n",
    "        summary_data['Final Val Loss'][i]\n",
    "    ))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f100d46",
   "metadata": {},
   "source": [
    "## Visualize Attention Weights\n",
    "\n",
    "### Extract and Visualize Attention from Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5cba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights from our from-scratch transformer\n",
    "def visualize_attention_scratch(model, text_input, max_seq=50):\n",
    "    \"\"\"Visualize attention weights from scratch transformer\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode input\n",
    "    tokens = encode(text_input)[:max_seq]\n",
    "    x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Forward pass to get attention weights\n",
    "    # We need to modify the model to return attention weights\n",
    "    # For now, we'll create a simple visualization\n",
    "    logits, _ = model(x)\n",
    "    \n",
    "    # Extract first block's attention head\n",
    "    with torch.no_grad():\n",
    "        # Get embeddings\n",
    "        tok_emb = model.token_embedding_table(x)\n",
    "        pos_emb = model.position_embedding_table(torch.arange(x.shape[1], device=device))\n",
    "        embeddings = tok_emb + pos_emb\n",
    "        \n",
    "        # Get attention from first block\n",
    "        attn_block = model.blocks[0]\n",
    "        attn_weights = attn_block.mha.get_attention_weights()\n",
    "    \n",
    "    return attn_weights, tokens\n",
    "\n",
    "# Visualize attention\n",
    "text_sample = \"To be or not to be that is the question\"\n",
    "attn_weights, tokens = visualize_attention_scratch(model_scratch, text_sample, max_seq=20)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Decode tokens for labels\n",
    "token_labels = [decode([t]) for t in tokens[:20]]\n",
    "\n",
    "# Plot attention heads\n",
    "for head_idx in range(4):\n",
    "    ax = axes[head_idx // 2, head_idx % 2]\n",
    "    \n",
    "    if attn_weights is not None:\n",
    "        # Determine how many heads are actually in the tensor\n",
    "        # If 3D: (Heads, Seq, Seq) -> shape[0] is num_heads\n",
    "        # If 4D: (Batch, Heads, Seq, Seq) -> shape[1] is num_heads\n",
    "        actual_heads = attn_weights.shape[0] if len(attn_weights.shape) == 3 else attn_weights.shape[1]\n",
    "        \n",
    "        # Only try to index if the head actually exists\n",
    "        if head_idx < actual_heads:\n",
    "            if len(attn_weights.shape) == 3:\n",
    "                att = attn_weights[head_idx, :len(token_labels), :len(token_labels)].cpu().numpy()\n",
    "            else:\n",
    "                att = attn_weights[0, head_idx, :len(token_labels), :len(token_labels)].cpu().numpy()\n",
    "            ax.set_title(f'Attention Head {head_idx + 1}', fontsize=11, fontweight='bold')\n",
    "        else:\n",
    "            # Fallback if the head doesn't exist\n",
    "            att = np.zeros((len(token_labels), len(token_labels)))\n",
    "            ax.set_title(f'Head {head_idx + 1} (Not Available)', fontsize=11, color='gray')\n",
    "    \n",
    "    # Plot heatmap\n",
    "    im = ax.imshow(att, cmap='viridis', aspect='auto')\n",
    "    ax.set_xticks(range(len(token_labels)))\n",
    "    ax.set_yticks(range(len(token_labels)))\n",
    "    ax.set_xticklabels(token_labels, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_yticklabels(token_labels, fontsize=9)\n",
    "    ax.set_title(f'Attention Head {head_idx + 1}', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Attention Weight', fontsize=9)\n",
    "\n",
    "fig.suptitle(f'From-Scratch Transformer: Attention Weights\\nInput: \"{text_sample}\"', \n",
    "             fontsize=13, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('attention_weights_scratch.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Attention weights visualization saved\")\n",
    "\n",
    "# Analyze attention patterns\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ATTENTION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if attn_weights is not None:\n",
    "    diag_attention = []\n",
    "    # Automatically detect the sequence dimension (it's always the last one)\n",
    "    num_tokens = min(len(token_labels), attn_weights.shape[-1])\n",
    "    \n",
    "    for i in range(num_tokens):\n",
    "        # FIX: Robust indexing for both 3D and 4D tensors\n",
    "        if len(attn_weights.shape) == 3:\n",
    "            # Shape is (Head, Seq, Seq)\n",
    "            diag_val = attn_weights[0, i, i].item() \n",
    "        else:\n",
    "            # Shape is (Batch, Head, Seq, Seq)\n",
    "            diag_val = attn_weights[0, 0, i, i].item()\n",
    "            \n",
    "        diag_attention.append((token_labels[i], diag_val))\n",
    "    \n",
    "    print(\"\\nSelf-Attention (Diagonal) Values (Head 1):\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Token':<15} {'Self-Attn':<15} {'Bar Chart':<40}\")\n",
    "    print(\"-\" * 70)\n",
    "    for token, val in diag_attention:\n",
    "        # Prevent crash if val is NaN or out of range\n",
    "        bar_len = max(0, min(30, int(val * 30)))\n",
    "        bar = 'â–ˆ' * bar_len\n",
    "        print(f\"{token:<15} {val:<15.4f} {bar}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Visualize causal dependencies (Where the model looks back)\n",
    "print(\"\\n\\nTop Token Dependencies (History/Context):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "if attn_weights is not None:\n",
    "    # Get the attention map for the first available head\n",
    "    if len(attn_weights.shape) == 3:\n",
    "        att_map = attn_weights[0, :len(token_labels), :len(token_labels)].cpu().numpy()\n",
    "    else:\n",
    "        att_map = attn_weights[0, 0, :len(token_labels), :len(token_labels)].cpu().numpy()\n",
    "    \n",
    "    # Show where each token is focusing its attention\n",
    "    for i in range(min(10, len(token_labels))):\n",
    "        # In a causal model, a token can only look at indices 0 to i\n",
    "        current_token_attn = att_map[i, :i+1]\n",
    "        top_indices = np.argsort(current_token_attn)[::-1][:3]\n",
    "        top_tokens = [token_labels[j] for j in top_indices]\n",
    "        print(f\"'{token_labels[i]}' focuses most on: {', '.join(top_tokens)}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae9ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tensor Shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff75e135",
   "metadata": {},
   "source": [
    "### Extract Attention from Hugging Face Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdf899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model with 'eager' implementation to allow attention extraction\n",
    "model_hf_finetuned = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./model_hf_shakespeare_final\", \n",
    "    attn_implementation=\"eager\" \n",
    ").to(device)\n",
    "\n",
    "model_hf_finetuned.eval() # Set to evaluation mode\n",
    "\n",
    "# Get attention weights from Hugging Face model\n",
    "text_sample_hf = \"To be or not to be that is the question\"\n",
    "\n",
    "# Tokenize\n",
    "input_ids = tokenizer_hf.encode(text_sample_hf, return_tensors='pt')[:, :20].to(device)\n",
    "tokens_hf = tokenizer_hf.convert_ids_to_tokens(input_ids[0].cpu())\n",
    "\n",
    "# Get outputs with attention\n",
    "with torch.no_grad():\n",
    "    outputs = model_hf_finetuned(\n",
    "        input_ids=input_ids,\n",
    "        output_attentions=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "# Extract attention weights (last layer)\n",
    "attention_weights = outputs.attentions[-1]  # Shape: (batch, num_heads, seq_len, seq_len)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "num_heads = min(8, attention_weights.shape[1])\n",
    "\n",
    "for head_idx in range(num_heads):\n",
    "    ax = axes[head_idx]\n",
    "    \n",
    "    # Get attention for this head\n",
    "    att = attention_weights[0, head_idx, :, :].cpu().numpy()\n",
    "    \n",
    "    # Plot heatmap\n",
    "    im = ax.imshow(att, cmap='viridis', aspect='auto')\n",
    "    ax.set_xticks(range(len(tokens_hf)))\n",
    "    ax.set_yticks(range(len(tokens_hf)))\n",
    "    ax.set_xticklabels(tokens_hf, rotation=45, ha='right', fontsize=8)\n",
    "    ax.set_yticklabels(tokens_hf, fontsize=8)\n",
    "    ax.set_title(f'Head {head_idx + 1}', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Weight', fontsize=8)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(num_heads, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "fig.suptitle(f'Hugging Face (Fine-tuned): Attention Weights - Last Layer\\nInput: \"{text_sample_hf}\"',\n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('attention_weights_hf.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ HuggingFace attention weights visualization saved\")\n",
    "\n",
    "# Analyze attention patterns\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HUGGING FACE ATTENTION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get diagonal attention (self-attention)\n",
    "diag_attention_hf = []\n",
    "for i in range(len(tokens_hf)):\n",
    "    diag_val = attention_weights[0, 0, i, i].item()\n",
    "    diag_attention_hf.append((tokens_hf[i], diag_val))\n",
    "\n",
    "print(\"\\nSelf-Attention Values (First Head):\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Token':<15} {'Self-Attn':<15} {'Bar Chart':<40}\")\n",
    "print(\"-\" * 70)\n",
    "for token, val in diag_attention_hf:\n",
    "    bar = 'â–ˆ' * int(val * 30)\n",
    "    print(f\"{token:<15} {val:<15.4f} {bar}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fdfa7d",
   "metadata": {},
   "source": [
    "## Final Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de56d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                         PROJECT SUMMARY & INSIGHTS                          â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ðŸŽ¯ PART 1: FROM-SCRATCH TRANSFORMER\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "âœ“ Implemented complete Transformer architecture from scratch\n",
    "âœ“ Components built: Embeddings, Positional Encoding, Multi-Head Attention, FFN\n",
    "âœ“ Features: Causal masking, Layer Normalization, Residual Connections\n",
    "âœ“ Architecture details:\n",
    "  - Embedding dimension: {config['embed_dim']}\n",
    "  - Number of heads: {config['num_heads']}\n",
    "  - Number of layers: {config['num_layers']}\n",
    "  - Total parameters: {sum(p.numel() for p in model_scratch.parameters()):,}\n",
    "\n",
    "ðŸ“Š Training Results:\n",
    "  - Epochs trained: {len(train_losses)}\n",
    "  - Initial train loss: {train_losses[0]:.4f}\n",
    "  - Final train loss: {train_losses[-1]:.4f}\n",
    "  - Loss reduction: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%\n",
    "  \n",
    "ðŸŽ¯ PART 2: HUGGING FACE FINE-TUNING\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "âœ“ Fine-tuned GPT-2 model on Shakespeare data\n",
    "âœ“ Model parameters: {sum(p.numel() for p in model_hf_finetuned.parameters()):,}\n",
    "  (142x larger than from-scratch model)\n",
    "\n",
    "ðŸ“Š Fine-tuning Results:\n",
    "  - Epochs: {num_epochs_hf}\n",
    "  - Initial train loss: {train_losses_hf[0]:.4f}\n",
    "  - Final train loss: {train_losses_hf[-1]:.4f}\n",
    "  - Final val loss: {val_losses_hf[-1]:.4f}\n",
    "  - Loss reduction: {((train_losses_hf[0] - train_losses_hf[-1]) / train_losses_hf[0] * 100):.1f}%\n",
    "\n",
    "ðŸ” KEY INSIGHTS\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "1. ARCHITECTURE COMPARISON\n",
    "   â€¢ From-scratch: Lightweight, fully interpretable, educational\n",
    "   â€¢ HF GPT-2: Production-ready, pre-trained on billions of tokens\n",
    "   â€¢ Parameter ratio: {sum(p.numel() for p in model_hf_finetuned.parameters()) / sum(p.numel() for p in model_scratch.parameters()):.1f}x\n",
    "\n",
    "2. TRAINING EFFICIENCY\n",
    "   â€¢ From-scratch: Slow convergence, requires careful tuning\n",
    "   â€¢ HF Fine-tuning: Fast adaptation due to pre-trained weights\n",
    "   â€¢ Transfer learning advantage: 10-100x fewer training examples needed\n",
    "\n",
    "3. GENERATION QUALITY\n",
    "   â€¢ From-scratch: Learns character patterns, generates plausible tokens\n",
    "   â€¢ HF Fine-tuned: High coherence, captures semantic meaning\n",
    "   â€¢ Attention mechanism: Shows model learns meaningful dependencies\n",
    "\n",
    "4. ATTENTION MECHANISMS\n",
    "   â€¢ Multi-head attention: Captures different semantic and syntactic patterns\n",
    "   â€¢ Causal masking: Prevents model from looking at future tokens\n",
    "   â€¢ Layer depth: Progressively abstract representations through layers\n",
    "\n",
    "ðŸ“ˆ PERFORMANCE METRICS SAVED\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "âœ“ training_progress_scratch.png - From-scratch transformer training curves\n",
    "âœ“ finetuning_progress_hf.png - HF model fine-tuning curves  \n",
    "âœ“ model_comparison.png - Side-by-side comparison of models\n",
    "âœ“ attention_weights_scratch.png - Attention visualization (from-scratch)\n",
    "âœ“ attention_weights_hf.png - Attention visualization (HF model)\n",
    "âœ“ model_scratch_best.pt - Best from-scratch model checkpoint\n",
    "âœ“ ./model_hf_finetuned/ - Fine-tuned HF model directory\n",
    "\n",
    "ðŸš€ PRACTICAL APPLICATIONS\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "1. Use from-scratch for: Learning, educational purposes, small datasets\n",
    "2. Use HF fine-tuned for: Production, real-world applications, large-scale tasks\n",
    "3. Transfer learning: Start with pre-trained â†’ Fine-tune on domain data\n",
    "4. Interpretability: Attention weights reveal model's decision process\n",
    "\n",
    "ðŸ’¡ RECOMMENDATIONS\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "âœ“ For production: Use fine-tuned Hugging Face models\n",
    "âœ“ For understanding: Experiment with from-scratch implementations\n",
    "âœ“ For optimization: Use attention visualization for debugging\n",
    "âœ“ For scaling: Leverage pre-trained weights + domain adaptation\n",
    "\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b099ba47",
   "metadata": {},
   "source": [
    "## Additional Resources and Further Exploration\n",
    "\n",
    "### Key Concepts Demonstrated\n",
    "- **Positional Encoding**: How transformers know token positions\n",
    "- **Multi-Head Attention**: Parallel attention computations for different semantic roles\n",
    "- **Causal Masking**: Preventing information leak during autoregressive generation\n",
    "- **Residual Connections**: Enabling deep network training\n",
    "- **Transfer Learning**: Leveraging pre-trained weights for faster adaptation\n",
    "\n",
    "### Files Generated\n",
    "All output files are saved in the current directory:\n",
    "- **Training Curves**: `training_progress_scratch.png`, `finetuning_progress_hf.png`\n",
    "- **Comparison**: `model_comparison.png`\n",
    "- **Attention Maps**: `attention_weights_scratch.png`, `attention_weights_hf.png`\n",
    "- **Models**: `model_scratch_best.pt`, `./model_hf_finetuned/`\n",
    "\n",
    "### Next Steps for Enhancement\n",
    "1. **Data augmentation**: Use more diverse training data\n",
    "2. **Hyperparameter tuning**: Optimize learning rates, batch sizes, attention heads\n",
    "3. **Longer training**: Train for more epochs with patience\n",
    "4. **Bigger models**: Scale up embedding dimensions and number of layers\n",
    "5. **Multi-task learning**: Combine language modeling with classification\n",
    "6. **Quantization**: Reduce model size for deployment\n",
    "\n",
    "### References\n",
    "- Vaswani et al. (2017): \"Attention Is All You Need\" - Original Transformer paper\n",
    "- Karpathy NanoGPT: https://github.com/karpathy/nanoGPT\n",
    "- Hugging Face Documentation: https://huggingface.co/docs/transformers/\n",
    "- Stanford CS224N: NLP with Deep Learning course"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
