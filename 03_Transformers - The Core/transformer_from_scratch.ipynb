{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a GPT-Style Transformer from Scratch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hamidmatiny/ML_Portfolio/blob/main/04_Advanced%20deep%20learning/transformer_from_scratch.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a **GPT-style transformer** from scratch using PyTorch. We'll build a character-level language model that can generate text in the style of Shakespeare.\n",
    "\n",
    "### What You'll Learn:\n",
    "- Understanding the transformer architecture\n",
    "- Self-attention mechanism and multi-head attention\n",
    "- Positional embeddings and their importance\n",
    "- Layer normalization and residual connections\n",
    "- Feed-forward networks in transformers\n",
    "- Training a language model from scratch\n",
    "\n",
    "### Key Mathematical Concepts:\n",
    "- **Attention Formula**: $\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
    "- **Multi-Head Attention**: Parallel attention heads that capture different types of relationships\n",
    "- **Positional Encoding**: Adding position information to token embeddings\n",
    "- **Layer Normalization**: $\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation and Tokenization\n",
    "\n",
    "We'll use Shakespeare's text as our training data and implement character-level tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Set device and random seeds for reproducibility\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(2971)\n",
    "np.random.seed(2971)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the Shakespeare dataset\n",
    "filename = 'input.txt'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Dataset length: {len(text):,} characters\")\n",
    "print(f\"First 500 characters:\\n{text[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-Level Tokenization\n",
    "\n",
    "We'll create a simple character-level tokenizer that maps each unique character to an integer ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary from unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Vocabulary: {''.join(chars)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Create character-to-index and index-to-character mappings\n",
    "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
    "\n",
    "def encode(text):\n",
    "    \"\"\"Convert text to list of token IDs\"\"\"\n",
    "    return [char_to_idx[char] for char in text]\n",
    "\n",
    "def decode(token_ids):\n",
    "    \"\"\"Convert list of token IDs back to text\"\"\"\n",
    "    return ''.join([idx_to_char[idx] for idx in token_ids])\n",
    "\n",
    "# Test the tokenizer\n",
    "test_text = \"Hello, World!\"\n",
    "encoded = encode(test_text)\n",
    "decoded = decode(encoded)\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting and Batch Generation\n",
    "\n",
    "We'll split the data into training and validation sets, then create a function to generate batches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "block_size = 256      # Maximum context length\n",
    "batch_size = 64       # Number of sequences per batch\n",
    "max_iters = 5000      # Training iterations\n",
    "eval_interval = 500   # Evaluation frequency\n",
    "learning_rate = 3e-4  # Learning rate\n",
    "eval_iters = 200      # Number of iterations for evaluation\n",
    "n_embd = 384          # Embedding dimension\n",
    "n_head = 6            # Number of attention heads\n",
    "n_layer = 6           # Number of transformer blocks\n",
    "dropout = 0.2         # Dropout rate\n",
    "\n",
    "# Convert text to tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Data tensor shape: {data.shape}\")\n",
    "\n",
    "# Split into train and validation\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Training data: {len(train_data):,} tokens\")\n",
    "print(f\"Validation data: {len(val_data):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Generate a batch of input-target pairs for training.\n",
    "    \n",
    "    Args:\n",
    "        split: 'train' or 'val' to select the dataset\n",
    "    \n",
    "    Returns:\n",
    "        x: Input sequences of shape (batch_size, block_size)\n",
    "        y: Target sequences of shape (batch_size, block_size)\n",
    "    \"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # Generate random starting positions\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # Create input and target sequences\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Test batch generation\n",
    "xb, yb = get_batch('train')\n",
    "print(f\"Input batch shape: {xb.shape}\")\n",
    "print(f\"Target batch shape: {yb.shape}\")\n",
    "print(f\"\\nExample sequence:\")\n",
    "print(f\"Input:  {decode(xb[0][:20].tolist())}\")\n",
    "print(f\"Target: {decode(yb[0][:20].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Self-Attention Mechanism\n",
    "\n",
    "The core of the transformer is the **self-attention mechanism**. It allows each position in the sequence to attend to all positions in the previous layers.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "Given input embeddings $X \\in \\mathbb{R}^{n \\times d}$, we compute:\n",
    "\n",
    "- **Query**: $Q = XW_Q$\n",
    "- **Key**: $K = XW_K$ \n",
    "- **Value**: $V = XW_V$\n",
    "\n",
    "The attention output is:\n",
    "$$\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "The scaling factor $\\frac{1}{\\sqrt{d_k}}$ prevents the softmax from saturating for large embedding dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"Single head of self-attention\"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        \n",
    "        # Linear projections for queries, keys, and values\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        \n",
    "        # Register causal mask as buffer (not a parameter)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # Batch, Time, Channels\n",
    "        \n",
    "        # Compute queries, keys, and values\n",
    "        k = self.key(x)    # (B, T, head_size)\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "        v = self.value(x)  # (B, T, head_size)\n",
    "        \n",
    "        # Compute attention scores with scaling\n",
    "        wei = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)  # (B, T, T)\n",
    "        \n",
    "        # Apply causal mask (decoder-style attention)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = wei @ v  # (B, T, head_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "Multi-head attention runs multiple attention heads in parallel and concatenates their outputs:\n",
    "\n",
    "$$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel\"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)  # Projection layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Concatenate outputs from all heads\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        \n",
    "        # Apply projection and dropout\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feed-Forward Network\n",
    "\n",
    "After attention, each transformer block applies a position-wise feed-forward network:\n",
    "\n",
    "$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "This is applied to each position separately and identically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # Expand by factor of 4\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),  # Project back down\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformer Block\n",
    "\n",
    "A transformer block combines multi-head attention and feed-forward networks with residual connections and layer normalization:\n",
    "\n",
    "$$\\text{Block}(x) = x + \\text{FFN}(\\text{LayerNorm}(x + \\text{MultiHead}(\\text{LayerNorm}(x))))$$\n",
    "\n",
    "### Layer Normalization\n",
    "\n",
    "Layer normalization normalizes across the feature dimension:\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta$$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ are the mean and standard deviation computed across the feature dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)  # Self-attention\n",
    "        self.ffwd = FeedForward(n_embd)                  # Feed-forward\n",
    "        self.ln1 = nn.LayerNorm(n_embd)                  # Layer norm 1\n",
    "        self.ln2 = nn.LayerNorm(n_embd)                  # Layer norm 2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pre-norm formulation (more stable training)\n",
    "        x = x + self.sa(self.ln1(x))    # Residual connection around attention\n",
    "        x = x + self.ffwd(self.ln2(x))  # Residual connection around FFN\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete GPT Model\n",
    "\n",
    "Now we'll combine all components into a complete GPT-style language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\"GPT-style transformer language model\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        \n",
    "        # Final layer norm and language modeling head\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        x = self.blocks(x)  # (B, T, C)\n",
    "        x = self.ln_f(x)    # (B, T, C)\n",
    "        \n",
    "        # Language modeling head\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generate new tokens using the trained model.\n",
    "        \n",
    "        Args:\n",
    "            idx: Starting context of shape (B, T)\n",
    "            max_new_tokens: Number of tokens to generate\n",
    "        \n",
    "        Returns:\n",
    "            Generated sequence of shape (B, T + max_new_tokens)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context to last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # (B, C)\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            \n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "# Initialize model and move to device\n",
    "model = GPTLanguageModel()\n",
    "model = model.to(device)\n",
    "\n",
    "# Print number of parameters\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Setup and Evaluation\n",
    "\n",
    "We'll set up the training loop with proper evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"Estimate loss on train and validation sets\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Test generation before training\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(\"\\nGeneration before training:\")\n",
    "print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n",
    "\n",
    "Now we'll train our GPT model using the AdamW optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # Evaluate loss periodically\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        loss_dict = estimate_loss()\n",
    "        print(f\"Step {iter:4d}: train loss {loss_dict['train']:.4f}, val loss {loss_dict['val']:.4f}\")\n",
    "        losses.append(loss_dict['train'].item())\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results and Text Generation\n",
    "\n",
    "Let's visualize the training progress and generate some text with our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "steps = [i * eval_interval for i in range(len(losses))]\n",
    "plt.plot(steps, losses, 'b-', linewidth=2, label='Training Loss')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('GPT Training Progress')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with the trained model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(model.generate(context, max_new_tokens=500)[0].tolist())\n",
    "\n",
    "print(\"Generated Shakespeare-style text:\")\n",
    "print(\"=\" * 50)\n",
    "print(generated_text)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Analysis and Attention Visualization\n",
    "\n",
    "Let's analyze what our model has learned by examining the attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(model, text, layer_idx=0, head_idx=0):\n",
    "    \"\"\"\n",
    "    Visualize attention weights for a given text.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained GPT model\n",
    "        text: Input text to analyze\n",
    "        layer_idx: Which transformer layer to examine\n",
    "        head_idx: Which attention head to examine\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode text and add batch dimension\n",
    "    tokens = torch.tensor(encode(text), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Forward pass with hooks to capture attention weights\n",
    "    attention_weights = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        # This is a simplified version - in practice you'd need to modify\n",
    "        # the attention modules to return weights\n",
    "        pass\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(tokens)\n",
    "    \n",
    "    print(f\"Input text: '{text}'\")\n",
    "    print(f\"Model prediction for next character: '{decode([logits[0, -1].argmax().item()])}'\") \n",
    "\n",
    "# Example usage\n",
    "sample_text = \"To be or not to be\"\n",
    "visualize_attention(model, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Comparison and Analysis\n",
    "\n",
    "Let's compare our model's performance with different architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count the number of trainable parameters in a model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def model_summary():\n",
    "    \"\"\"Print a summary of the model architecture\"\"\"\n",
    "    print(\"GPT Model Architecture Summary\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Vocabulary size: {vocab_size:,}\")\n",
    "    print(f\"Block size (context length): {block_size}\")\n",
    "    print(f\"Embedding dimension: {n_embd}\")\n",
    "    print(f\"Number of layers: {n_layer}\")\n",
    "    print(f\"Number of attention heads: {n_head}\")\n",
    "    print(f\"Head size: {n_embd // n_head}\")\n",
    "    print(f\"Dropout rate: {dropout}\")\n",
    "    print(f\"Total parameters: {count_parameters(model):,}\")\n",
    "    \n",
    "    # Calculate model size in MB\n",
    "    param_size = count_parameters(model) * 4  # 4 bytes per float32\n",
    "    print(f\"Model size: {param_size / (1024**2):.2f} MB\")\n",
    "\n",
    "model_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Insights and Improvements\n",
    "\n",
    "### What We've Learned:\n",
    "\n",
    "1. **Self-Attention**: The model learns to attend to relevant parts of the input sequence\n",
    "2. **Positional Encoding**: Essential for the model to understand sequence order\n",
    "3. **Multi-Head Attention**: Different heads capture different types of relationships\n",
    "4. **Residual Connections**: Enable training of deep networks\n",
    "5. **Layer Normalization**: Stabilizes training and improves convergence\n",
    "\n",
    "### Potential Improvements:\n",
    "\n",
    "- **Larger Model**: More layers, heads, and embedding dimensions\n",
    "- **Better Tokenization**: Subword tokenization (BPE, SentencePiece)\n",
    "- **Advanced Techniques**: \n",
    "  - Gradient clipping\n",
    "  - Learning rate scheduling\n",
    "  - Weight decay\n",
    "  - Rotary positional embeddings (RoPE)\n",
    "- **Data Augmentation**: More diverse training data\n",
    "- **Regularization**: Dropout, weight decay, early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "We've successfully implemented a GPT-style transformer from scratch! This model demonstrates the key concepts:\n",
    "\n",
    "- **Self-attention mechanism** for capturing long-range dependencies\n",
    "- **Multi-head attention** for learning diverse representations\n",
    "- **Positional embeddings** for sequence understanding\n",
    "- **Residual connections and layer normalization** for stable training\n",
    "\n",
    "The transformer architecture has revolutionized natural language processing and continues to be the foundation for state-of-the-art language models like GPT-3, GPT-4, and beyond.\n",
    "\n",
    "### Next Steps:\n",
    "1. Experiment with different hyperparameters\n",
    "2. Try different datasets and tokenization schemes\n",
    "3. Implement more advanced transformer variants\n",
    "4. Explore fine-tuning pre-trained models\n",
    "5. Study attention patterns and model interpretability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
