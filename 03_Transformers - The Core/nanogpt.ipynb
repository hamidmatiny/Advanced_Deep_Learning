{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228d804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/ng-video-lecture/refs/heads/master/input.txt\"\n",
    "filename = \"input.txt\"\n",
    "\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "print(f\"{filename} has been downloaded.\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200bef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'input.txt'\n",
    "with open (filename, 'r', encoding= 'utf-8' ) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbdd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"length of dataset in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f6d196",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (text[:1000])  # print the first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print (f\"vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ce736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vocabulary mapping for tokenization\n",
    "char_to_index = {char: idx for idx, char in enumerate(chars)}\n",
    "index_to_char = {idx: char for idx, char in enumerate(chars)}\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Converts a string into a list of integer token IDs.\"\"\"\n",
    "    return [char_to_index[char] for char in text]\n",
    "\n",
    "def detokenize(token_ids):\n",
    "    \"\"\"Converts a list of integer token IDs back into a string.\"\"\"\n",
    "    return ''.join([index_to_char[idx] for idx in token_ids])\n",
    "tokenized_text = tokenize(\"hello world\")\n",
    "print(tokenized_text)\n",
    "print(detokenize(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data = torch.tensor(tokenize(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])  # print the first 1000 token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab5cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8  # context length\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9918bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] # 'context' is the \"history\" the model sees so far (from start to current index t)\n",
    "    target = y[t] # 'target' is the specific token that comes immediately after that context\n",
    "    print(f\"when input is {context.tolist()} the target: {target.item()}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16afa719",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    \n",
    "    context_str = detokenize(context.tolist())\n",
    "    target_char = detokenize([target.item()])\n",
    "    \n",
    "    # Notice the str() wrapping the numeric values\n",
    "    print(f\"Context IDs: {str(context.tolist()):<30} | Text: '{context_str}'\")\n",
    "    print(f\"Target ID:   {str(target.item()):<10} | Next: '{target_char}'\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c2470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2971)\n",
    "batch_size = 4  # Number of independent sequences to process in parallel\n",
    "block_size = 8  # Maximum length of the sequence (context) for predictions\n",
    "\n",
    "def get_batch(split_type):\n",
    "    \"\"\"\n",
    "    Constructs a batch of inputs (x) and targets (y).\n",
    "    Each target y is the input x shifted by one character.\n",
    "    \"\"\"\n",
    "    # Select the appropriate dataset split\n",
    "    dataset = train_data if split_type == 'train' else val_data\n",
    "    \n",
    "    # Generate 'batch_size' number of random starting points in the dataset\n",
    "    # We subtract block_size to ensure we don't go out of bounds\n",
    "    random_offsets = torch.randint(len(dataset) - block_size, (batch_size,))\n",
    "    \n",
    "    # Stack individual sequences into a single 2D Tensor (Batch, Block)\n",
    "    input_batch = torch.stack([dataset[i : i + block_size] for i in random_offsets])\n",
    "    \n",
    "    # Target batch is shifted forward by one index\n",
    "    target_batch = torch.stack([dataset[i + 1 : i + block_size + 1] for i in random_offsets])\n",
    "    \n",
    "    return input_batch, target_batch\n",
    "\n",
    "# Generate a sample training batch\n",
    "batch_inputs, batch_targets = get_batch('train')\n",
    "\n",
    "print(f\"Batch Inputs Shape: {batch_inputs.shape}\")\n",
    "print(f\"Batch Targets Shape: {batch_targets.shape}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Demonstrate the autoregressive training examples within the batch\n",
    "for b_idx in range(batch_size):\n",
    "    print(f\"--- Processing Sequence {b_idx} in the Batch ---\")\n",
    "    \n",
    "    for t_idx in range(block_size):\n",
    "        # Extract the sequence history up to current time t\n",
    "        context_tokens = batch_inputs[b_idx, : t_idx + 1]\n",
    "        target_token = batch_targets[b_idx, t_idx]\n",
    "        \n",
    "        # Convert numeric IDs back to human-readable text\n",
    "        context_text = detokenize(context_tokens.tolist())\n",
    "        target_text = detokenize([target_token.item()])\n",
    "        \n",
    "        # Log the relationship between context and prediction\n",
    "        print(f\"Seq {b_idx}, Step {t_idx} | Context: {str(context_tokens.tolist()):<30} | Text: '{context_text}'\")\n",
    "        print(f\"              | Next ID: {str(target_token.item()):<10} | Next Char: '{target_text}'\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f39fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(2971)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Bigram Language Model. \n",
    "    It predicts the next character based solely on the current character.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token's ID acts as an index to look up the 'logits' (scores) \n",
    "        # for what character should come next.\n",
    "        # Shape: (vocab_size, vocab_size)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        \"\"\"\n",
    "        Calculates the scores (logits) for the next character in a sequence.\n",
    "        indices: (Batch, Time) tensor of integers\n",
    "        targets: (Batch, Time) tensor of integers\n",
    "        \"\"\"\n",
    "        # We look up the 'logits' for the next token directly from the table\n",
    "        # Output shape: (Batch, Time, Channels/Vocab_Size)\n",
    "        logits = self.token_embedding_table(indices)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # PyTorch's cross_entropy expects the 'Channels' to be the 2nd dimension\n",
    "            # We flatten the Batch and Time dimensions to calculate loss across the whole batch\n",
    "            batch_size, sequence_length, vocab_channels = logits.shape\n",
    "            \n",
    "            # Reshape to (Batch * Time, Vocab_Size)\n",
    "            logits_flattened = logits.view(batch_size * sequence_length, vocab_channels)\n",
    "            \n",
    "            # Reshape targets to (Batch * Time)\n",
    "            targets_flattened = targets.view(batch_size * sequence_length)\n",
    "            \n",
    "            loss = F.cross_entropy(logits_flattened, targets_flattened)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, indices, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generates new text by repeatedly sampling from the model's predictions.\n",
    "        indices: (Batch, Time) current context of tokens\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 1. Get predictions for the current sequence\n",
    "            logits, _ = self(indices)\n",
    "\n",
    "            # 2. Bigram models only care about the very last token in the sequence\n",
    "            # We pluck out the last 'Time' step: (Batch, Channels)\n",
    "            last_token_logits = logits[:, -1, :] \n",
    "\n",
    "            # 3. Convert raw scores to probabilities\n",
    "            probabilities = F.softmax(last_token_logits, dim=-1) \n",
    "\n",
    "            # 4. Randomly pick the next token based on the probability distribution\n",
    "            next_token_index = torch.multinomial(probabilities, num_samples=1) \n",
    "\n",
    "            # 5. Concatenate the new token to the existing sequence and repeat\n",
    "            indices = torch.cat((indices, next_token_index), dim=1) \n",
    "\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426819c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the model with the size of our character vocabulary\n",
    "# 'vocab_size' should be len(chars)\n",
    "language_model = BigramLanguageModel(vocab_size)\n",
    "\n",
    "# 2. Perform a forward pass using the training batch we generated earlier\n",
    "# 'batch_inputs' is the context (X), 'batch_targets' is the correct next character (Y)\n",
    "next_token_logits, training_loss = language_model(batch_inputs, batch_targets)\n",
    "\n",
    "# 3. Output the results\n",
    "print(f\"Logits Tensor Shape (Batch, Time, Channels): {next_token_logits.shape}\")\n",
    "print(f\"Current Training Loss: {training_loss.item():.4f}\")\n",
    "\n",
    "# --- Validation Logic ---\n",
    "# Expected initial loss for a random model should be -ln(1/vocab_size)\n",
    "import math\n",
    "expected_initial_loss = -math.log(1/vocab_size)\n",
    "print(f\"Expected loss for a random model: {expected_initial_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368dd672",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detokenize(language_model.generate(indices=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baebf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(language_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c085a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, training_steps=10000, eval_interval=1000):\n",
    "    \"\"\"\n",
    "    Trains the language model for a fixed number of steps.\n",
    "    Returns the history of the training loss.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "    \n",
    "    print(f\"=== Starting Training for {training_steps} steps ===\")\n",
    "    \n",
    "    for step in range(training_steps):\n",
    "        # 1. Fetch a fresh batch of training data\n",
    "        input_batch, target_batch = get_batch('train')\n",
    "        \n",
    "        # 2. Forward pass: compute predictions and current loss\n",
    "        _, loss = model(input_batch, target_batch)\n",
    "        \n",
    "        # 3. Backward pass: compute gradients and update weights\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Periodically log progress and save history\n",
    "        if step % eval_interval == 0 or step == training_steps - 1:\n",
    "            loss_value = loss.item()\n",
    "            loss_history.append(loss_value)\n",
    "            print(f\"Step [{step:5d}/{training_steps}] - Training Loss: {loss_value:.4f}\")\n",
    "            \n",
    "    print(f\"Training Complete. Final Loss: {loss.item():.4f}\")\n",
    "    return loss_history\n",
    "\n",
    "\n",
    "def evaluate_model(model, eval_iterations=200):\n",
    "    \"\"\"\n",
    "    Evaluates the model on both training and validation splits to check for overfitting.\n",
    "    This replaces the 'test_model' function for Generative Models.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    split_losses = {}\n",
    "    with torch.no_grad():\n",
    "        for split in ['train', 'val']:\n",
    "            # Accumulate loss over multiple random batches for a stable average\n",
    "            batch_losses = torch.zeros(eval_iterations)\n",
    "            for k in range(eval_iterations):\n",
    "                inputs, targets = get_batch(split)\n",
    "                _, loss = model(inputs, targets)\n",
    "                batch_losses[k] = loss.item()\n",
    "        \n",
    "            split_losses[split] = batch_losses.mean().item()\n",
    "    \n",
    "        print(f\"=== Evaluation Results ===\")\n",
    "        print(f\"Train Loss: {split_losses['train']:.4f}\")\n",
    "        print(f\"Val Loss:   {split_losses['val']:.4f}\")\n",
    "    \n",
    "        model.train() # Switch back to training mode\n",
    "    return split_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358b29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detokenize(language_model.generate(indices=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b362f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Optimizer\n",
    "optimizer = torch.optim.AdamW(language_model.parameters(), lr=1e-3)\n",
    "\n",
    "# 2. Train the model\n",
    "history = train_model(language_model, optimizer, training_steps=5000)\n",
    "\n",
    "# 3. Evaluate the results\n",
    "final_metrics = evaluate_model(language_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92cc778",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detokenize(language_model.generate(indices=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e235a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curve(history, eval_interval):\n",
    "    \"\"\"\n",
    "    Visualizes the loss reduction over time.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create the x-axis based on the evaluation intervals\n",
    "    steps = [i * eval_interval for i in range(len(history))]\n",
    "    \n",
    "    plt.plot(steps, history, label='Training Loss', color='#2ecc71', linewidth=2)\n",
    "    \n",
    "    # Formatting the chart\n",
    "    plt.title('Bigram Model Learning Curve', fontsize=14)\n",
    "    plt.xlabel('Training Steps', fontsize=12)\n",
    "    plt.ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Call the function using the history from your training\n",
    "plot_learning_curve(history, eval_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dee076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for dimensions: Batch (sentences), Time (words), Channels (features)\n",
    "torch.manual_seed(2971)\n",
    "\n",
    "batch_size, sequence_length, feature_dim = 4, 8, 2\n",
    "x= torch.randn((batch_size, sequence_length, feature_dim))\n",
    "# Initialize a tensor to store our 'Bag of Words' averages\n",
    "# Shape: (4, 8, 2)\n",
    "sequence_averages = torch.zeros((batch_size, sequence_length, feature_dim))\n",
    "\n",
    "for b_idx in range(batch_size):\n",
    "    for t_idx in range(sequence_length):\n",
    "        # 1. Look at all information from the start up to the current time 't'\n",
    "        # x_previous represents the \"context\" or \"history\"\n",
    "        x_previous = x[b_idx, :t_idx+1] # Shape: (t_idx+1, feature_dim)\n",
    "\n",
    "        # 2. Average the features across the time dimension\n",
    "        # We want to summarize all previous words into one single average vector\n",
    "        sequence_averages[b_idx, t_idx] = torch.mean(x_previous, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bf07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f622ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_averages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e236995",
   "metadata": {},
   "outputs": [],
   "source": [
    "tril_ones = torch.tril(torch.ones((3, 3)))\n",
    "ones = torch.ones((3, 3))\n",
    "random_matrix = torch.randn((3, 2))\n",
    "print(f\"Tril Matrix:\\n{tril_ones}\\n\")\n",
    "print(f\"Ones Matrix:\\n{ones}\\n\")\n",
    "print(f\"Random Matrix:\\n{random_matrix}\\n\")\n",
    "print(f\"Tril Matrix * Random Matrix:\\n{tril_ones @ random_matrix}\\n\")\n",
    "print(f\"Ones Matrix * Random Matrix:\\n{ones @ random_matrix}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b1383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now if we normalize the tril matrix\n",
    "tril_normalized = tril_ones / tril_ones.sum(dim=1, keepdim=True)\n",
    "print(f\"Normalized Tril Matrix:\\n{tril_normalized}\\n\")\n",
    "print(f\"Normalized Tril Matrix * Random Matrix:\\n{tril_normalized @ random_matrix}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f395111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's vectorize it and use it in our model since this is the mathematical trick used in attention mechanisms\n",
    "torch.manual_seed(2971)\n",
    "weighted_sum = torch.tril(torch.ones((sequence_length, sequence_length)))\n",
    "weighted_sum = weighted_sum / weighted_sum.sum(dim=1, keepdim=True)\n",
    "weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4615e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sequence_averages = weighted_sum @ x\n",
    "torch.allclose(new_sequence_averages, sequence_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d0a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's do it again but this time using softmax to get the weights\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(2971)\n",
    "tril = torch.tril(torch.ones((sequence_length, sequence_length)))\n",
    "new_weights = torch.zeros((sequence_length, sequence_length))\n",
    "new_weights = new_weights.masked_fill(tril == 0, float('-inf'))\n",
    "new_weights = F.softmax(new_weights, dim=-1)\n",
    "another_sequence_averages = new_weights @ x\n",
    "torch.allclose(another_sequence_averages, sequence_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502e7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's do it again but this time lets see how a single head perform self-attention\n",
    "torch.manual_seed(2971)\n",
    "batch_size, sequence_length, feature_dim = 4, 8, 2\n",
    "x= torch.randn((batch_size, sequence_length, feature_dim))\n",
    "head_size = 16\n",
    "# now imagine token is speaking: it's telling every other tokens the following:\n",
    "key = nn.Linear(feature_dim, head_size, bias=False)   # here is what I have to say\n",
    "query = nn.Linear(feature_dim, head_size, bias=False) # here is what I'm looking for\n",
    "value = nn.Linear(feature_dim, head_size, bias=False) # if you find me interesting here is the information I carry\n",
    "\n",
    "\n",
    "k = key(x)  # (Batch, Time, Head_Size)\n",
    "q = query(x)  # (Batch, Time, Head_Size)\n",
    "v = value(x)  # when we apply value on the current x it gives us self-attention, but we can also apply it on other tokens to get cross-attention, something like we do for decoding an encoder\n",
    "\n",
    "new_weighted_matrix = q @ k.transpose(-2, -1) # (Batch, Time, Head_Size) @ (Batch, Head_Size, Time) -> (Batch, Time, Time)\n",
    "new_weighted_matrix = new_weighted_matrix.masked_fill(tril == 0, float('-inf')) # we use this so the current token does not speak to future tokens and only learns from previous tokens\n",
    "# if we delete the above line, the model will be non-causal and will be able to look into the future tokens, like encoder blocks in transformers, but now this structure is more like decoder blocks in transformers\n",
    "new_weighted_matrix = F.softmax(new_weighted_matrix, dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "out = new_weighted_matrix @ v  # (Batch, Time, Time) @ (Batch, Time, Feature_Dim) -> (Batch, Time, Feature_Dim)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weighted_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b31c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one important thing we have to do for the weighted matrix is to scale it, because the dot products can get large in magnitude which can push the softmax into regions where it has extremely small gradients\n",
    "# and that may cause the model to not learn properly. by scaling the weighted matrix we are going to sharpen the distribution of important nodes.\n",
    "# we do it by deviding by the square root of head_size before sending it to softmax\n",
    "\n",
    "new_weighted_matrix = new_weighted_matrix / (head_size ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210cd5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weighted_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68d7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create one head of a self-attention mechanism\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(feature_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(feature_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(feature_dim, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones((block_size, block_size))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, sequence_length, feature_dim = x.shape\n",
    "\n",
    "        k = self.key(x)    # (Batch, Time, Head_Size)\n",
    "        q = self.query(x)  # (Batch, Time, Head_Size)\n",
    "        v = self.value(x)  # (Batch, Time, Head_Size)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_scores = q @ k.transpose(-2, -1)  # (Batch, Time, Time)\n",
    "        attn_scores = attn_scores.masked_fill(self.tril[:sequence_length, :sequence_length] == 0, float('-inf'))\n",
    "        attn_scores = attn_scores / (k.size(-1) ** 0.5)  # Scale the scores\n",
    "\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # (Batch, Time, Time)\n",
    "\n",
    "        out = attn_weights @ v  # (Batch, Time, Head_Size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa62002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we know how self attention works, let's implement it in our model\n",
    "\n",
    "\n",
    "# let's build a biagram model with self attention\n",
    "\n",
    "class SelfAttentionLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A Bigram Language Model enhanced with Self-Attention mechanism.\n",
    "    It predicts the next character based on the entire context using self-attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, feature_dim):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, feature_dim)\n",
    "        self.language_model_head = nn.Linear(feature_dim, vocab_size)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, feature_dim)  # Assuming max sequence length of block_size\n",
    "        self.feature_dim = feature_dim\n",
    "        self.self_attention_head = Head(feature_dim, head_size=feature_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        batch_size, sequence_length = indices.shape\n",
    "        \n",
    "        # 1. Embed the input tokens to get their feature representations\n",
    "        token_embedding = self.token_embedding_table(indices)  # Shape: (Batch, Time, Feature_Dim)\n",
    "        posiitional_indices = self.positional_embedding_table(torch.arange(sequence_length))  # Shape: (Time, Feature_Dim)\n",
    "        x = token_embedding + posiitional_indices  # Shape: (Batch, Time, Feature_Dim)\n",
    "        x = self.self_attention_head(x)  # Apply self-attention\n",
    "        logits = self.language_model_head(x)  # Initial logits (not used further)\n",
    "\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, sequence_length, vocab_channels = logits.shape\n",
    "            logits_flattened = logits.view(batch_size * sequence_length, vocab_channels)\n",
    "            targets_flattened = targets.view(batch_size * sequence_length)\n",
    "            loss = F.cross_entropy(logits_flattened, targets_flattened)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, indices, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            indices_condensed = indices[:, -block_size:]\n",
    "            logits, _ = self(indices_condensed)\n",
    "            last_token_logits = logits[:, -1, :]\n",
    "            probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "            next_token_index = torch.multinomial(probabilities, num_samples=1)\n",
    "            indices = torch.cat((indices, next_token_index), dim=1)\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Self_Attention_model = SelfAttentionLanguageModel(vocab_size, feature_dim=32)\n",
    "\n",
    "\n",
    "optimizer_Self_Attention = torch.optim.AdamW(Self_Attention_model.parameters(), lr=1e-3)\n",
    "\n",
    "# 2. Train the model\n",
    "history = train_model(Self_Attention_model, optimizer_Self_Attention, training_steps=5000)\n",
    "# 3. Evaluate the results\n",
    "final_metrics = evaluate_model(Self_Attention_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e5c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detokenize(Self_Attention_model.generate(indices=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b42628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve_with_Self_Attention(history, eval_interval):\n",
    "    \"\"\"\n",
    "    Visualizes the loss reduction over time.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create the x-axis based on the evaluation intervals\n",
    "    steps = [i * eval_interval for i in range(len(history))]\n",
    "    \n",
    "    plt.plot(steps, history, label='Training Loss', color='#2ecc71', linewidth=2)\n",
    "    \n",
    "    # Formatting the chart\n",
    "    plt.title('Self-Attention Model Learning Curve', fontsize=14)\n",
    "    plt.xlabel('Training Steps', fontsize=12)\n",
    "    plt.ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Call the function using the history from your training\n",
    "plot_learning_curve_with_Self_Attention(history, eval_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ad5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result still isn't good enough!!! one thing we can do is to stack multiple heads of self-attention and then stack multiple layers of such heads to make it deeper\n",
    "# so let's try that\n",
    "# let's create a multi-head self-attention mechanism and stack multiple layers of it\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, feature_dim):\n",
    "\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(feature_dim, head_size) for _ in range(num_heads)])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "class SelfAttentionMultiHead(nn.Module): \n",
    "    \"\"\" Multi-head self-attention followed by a feed-forward network \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, feature_dim):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, feature_dim)\n",
    "        self.language_model_head = nn.Linear(feature_dim, vocab_size)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, feature_dim)  # Assuming max sequence length of block_size\n",
    "        self.multihead_attention = MultiHeadAttention(num_heads=4, head_size=feature_dim // 4, feature_dim= feature_dim)\n",
    "\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        batch_size, sequence_length = indices.shape\n",
    "        \n",
    "        # 1. Embed the input tokens to get their feature representations\n",
    "        token_embedding = self.token_embedding_table(indices)  # Shape: (Batch, Time, Feature_Dim)\n",
    "        posiitional_indices = self.positional_embedding_table(torch.arange(sequence_length))  # Shape: (Time, Feature_Dim)\n",
    "        x = token_embedding + posiitional_indices  # Shape: (Batch, Time, Feature_Dim)\n",
    "        x = self.multihead_attention(x)  # Apply self-attention\n",
    "        logits = self.language_model_head(x)  # Initial logits (not used further)\n",
    "\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, sequence_length, vocab_channels = logits.shape\n",
    "            logits_flattened = logits.view(batch_size * sequence_length, vocab_channels)\n",
    "            targets_flattened = targets.view(batch_size * sequence_length)\n",
    "            loss = F.cross_entropy(logits_flattened, targets_flattened)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, indices, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            indices_condensed = indices[:, -block_size:]\n",
    "            logits, _ = self(indices_condensed)\n",
    "            last_token_logits = logits[:, -1, :]\n",
    "            probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "            next_token_index = torch.multinomial(probabilities, num_samples=1)\n",
    "            indices = torch.cat((indices, next_token_index), dim=1)\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b0e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiHead = SelfAttentionMultiHead(vocab_size, feature_dim=32)  \n",
    "\n",
    "optimizer_Multi_Head = torch.optim.AdamW(multiHead.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "history = train_model(multiHead, optimizer_Multi_Head, training_steps=5000)\n",
    "\n",
    "final_metrics = evaluate_model(multiHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detokenize(multiHead.generate(indices=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve_Multi_Head(history, eval_interval):\n",
    "    \"\"\"\n",
    "    Visualizes the loss reduction over time.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create the x-axis based on the evaluation intervals\n",
    "    steps = [i * eval_interval for i in range(len(history))]\n",
    "    \n",
    "    plt.plot(steps, history, label='Training Loss', color='#2ecc71', linewidth=2)\n",
    "    \n",
    "    # Formatting the chart\n",
    "    plt.title('Multi Head Model Learning Curve', fontsize=14)\n",
    "    plt.xlabel('Training Steps', fontsize=12)\n",
    "    plt.ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Call the function using the history from your training\n",
    "plot_learning_curve_Multi_Head(history, eval_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a222dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#up until now the model goes to calculate the logit too soon and because of that token doesn't have enough time to learn anything\n",
    "# so let's add a single layer to it and give it the ability to think and learn more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7130687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadThinker(nn.Module):\n",
    "    \"\"\" Multi-head self-attention followed by a feed-forward network \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, feature_dim):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, feature_dim)\n",
    "        self.language_model_head = nn.Linear(feature_dim, vocab_size)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, feature_dim)  # Assuming max sequence length of block_size\n",
    "        self.multihead_attention = MultiHeadAttention(num_heads=4, head_size=feature_dim // num_heads, feature_dim= feature_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim * 4, feature_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        batch_size, sequence_length = indices.shape\n",
    "\n",
    "        # 1. Embed the input tokens to get their feature representations\n",
    "        token_embedding = self.token_embedding_table(indices)  # Shape: (Batch, Time, Feature_Dim)\n",
    "        posiitional_indices = self.positional_embedding_table(torch.arange(sequence_length))  # Shape: (Time, Feature_Dim)\n",
    "        x = token_embedding + posiitional_indices  # Shape: (Batch, Time, Feature_Dim)\n",
    "        x = self.multihead_attention(x)  # Apply self-attention\n",
    "        x = self.feed_forward(x)  # Apply feed-forward network\n",
    "        logits = self.language_model_head(x)  # Initial logits (not used further)\n",
    "\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, sequence_length, vocab_channels = logits.shape\n",
    "            logits_flattened = logits.view(batch_size * sequence_length, vocab_channels)\n",
    "            targets_flattened = targets.view(batch_size * sequence_length)\n",
    "            loss = F.cross_entropy(logits_flattened, targets_flattened)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, indices, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            indices_condensed = indices[:, -block_size:]\n",
    "            logits, _ = self(indices_condensed)\n",
    "            last_token_logits = logits[:, -1, :]\n",
    "            probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "            next_token_index = torch.multinomial(probabilities, num_samples=1)\n",
    "            indices = torch.cat((indices, next_token_index), dim=1)\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ff357",
   "metadata": {},
   "outputs": [],
   "source": [
    "thinker = MultiHeadThinker(vocab_size, feature_dim=32)  \n",
    "\n",
    "optimizer_Thinker = torch.optim.AdamW(thinker.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "history = train_model(thinker, optimizer_Thinker, training_steps=5000)\n",
    "\n",
    "final_metrics = evaluate_model(thinker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eb89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detokenize(thinker.generate(indices=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d0916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve_Thinker(history, eval_interval):\n",
    "    \"\"\"\n",
    "    Visualizes the loss reduction over time.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create the x-axis based on the evaluation intervals\n",
    "    steps = [i * eval_interval for i in range(len(history))]\n",
    "    \n",
    "    plt.plot(steps, history, label='Training Loss', color='#2ecc71', linewidth=2)\n",
    "    \n",
    "    # Formatting the chart\n",
    "    plt.title('Thinker Model Learning Curve', fontsize=14)\n",
    "    plt.xlabel('Training Steps', fontsize=12)\n",
    "    plt.ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Call the function using the history from your training\n",
    "plot_learning_curve_Thinker(history, eval_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8913666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's go even deeper to the network and apply multiple layers of multihead\n",
    "#let's build our block first\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" A simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim * 4, feature_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transform block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim , num_head):\n",
    "        super().__init__()\n",
    "        head_size = feature_dim // num_head\n",
    "        self.self_attention_head = MultiHeadAttention(num_head, head_size, feature_dim)\n",
    "        self.ffwd = FeedForward(feature_dim)\n",
    "    def forward(self, x):\n",
    "        x =self.self_attention_head(x)\n",
    "        x =self.ffwd(x)\n",
    "        return x   \n",
    "\n",
    "     \n",
    "class MultiHeadMultiLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, feature_dim):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, feature_dim)\n",
    "        self.language_model_head = nn.Linear(feature_dim, vocab_size)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, feature_dim)  # Assuming max sequence length of block_size\n",
    "        self.multihead_attention = MultiHeadAttention(num_heads=4, head_size=feature_dim // 4, feature_dim= feature_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim * 4, feature_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        batch_size, sequence_length = indices.shape\n",
    "\n",
    "        # 1. Embed the input tokens to get their feature representations\n",
    "        token_embedding = self.token_embedding_table(indices)  # Shape: (Batch, Time, Feature_Dim)\n",
    "        posiitional_indices = self.positional_embedding_table(torch.arange(sequence_length))  # Shape: (Time, Feature_Dim)\n",
    "        x = token_embedding + posiitional_indices  # Shape: (Batch, Time, Feature_Dim)\n",
    "        x = self.multihead_attention(x)  # Apply self-attention\n",
    "        x = self.feed_forward(x)  # Apply feed-forward network\n",
    "        logits = self.language_model_head(x)  # Initial logits (not used further)\n",
    "\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, sequence_length, vocab_channels = logits.shape\n",
    "            logits_flattened = logits.view(batch_size * sequence_length, vocab_channels)\n",
    "            targets_flattened = targets.view(batch_size * sequence_length)\n",
    "            loss = F.cross_entropy(logits_flattened, targets_flattened)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, indices, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            indices_condensed = indices[:, -block_size:]\n",
    "            logits, _ = self(indices_condensed)\n",
    "            last_token_logits = logits[:, -1, :]\n",
    "            probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "            next_token_index = torch.multinomial(probabilities, num_samples=1)\n",
    "            indices = torch.cat((indices, next_token_index), dim=1)\n",
    "        return indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac0245",
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodel = MultiHeadMultiLayer(vocab_size, feature_dim=32)\n",
    "optimizer_MultiModel = torch.optim.AdamW(multimodel.parameters(), lr=1e-3)\n",
    "history = train_model(multimodel, optimizer_MultiModel, training_steps=5000)\n",
    "final_metrics = evaluate_model(multimodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac708dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detokenize(multimodel.generate(indices=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b30ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve_Multi_Layer(history, eval_interval):\n",
    "    \"\"\"\n",
    "    Visualizes the loss reduction over time.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create the x-axis based on the evaluation intervals\n",
    "    steps = [i * eval_interval for i in range(len(history))]\n",
    "    \n",
    "    plt.plot(steps, history, label='Training Loss', color='#2ecc71', linewidth=2)\n",
    "    \n",
    "    # Formatting the chart\n",
    "    plt.title('Multi Head Model Learning Curve', fontsize=14)\n",
    "    plt.xlabel('Training Steps', fontsize=12)\n",
    "    plt.ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Call the function using the history from your training\n",
    "plot_learning_curve_Multi_Layer(history, eval_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewMultiHead(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, feature_dim):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.feature_dim = feature_dim\n",
    "        self.heads = nn.ModuleList([Head(feature_dim, head_size) for _ in range(num_heads)])  # Pass feature_dim first\n",
    "        self.projection = nn.Linear(feature_dim, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.projection(out)\n",
    "\n",
    "class NewBlock(nn.Module):\n",
    "    \"\"\" Transform block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim, num_head):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_head = num_head\n",
    "        head_size = feature_dim // num_head\n",
    "        self.self_attention_head = NewMultiHead(num_heads=num_head, head_size=head_size, feature_dim=feature_dim)\n",
    "        self.ffwd = NewFeedForward(feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attention_head(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        return x\n",
    "\n",
    "class NewFeedForward(nn.Module):\n",
    "    \"\"\" A simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim * 4, feature_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class NonLinearMultiHead(nn.Module):\n",
    "    def __init__(self, vocab_size, feature_dim):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.blocks = nn.Sequential(\n",
    "            NewBlock(feature_dim, num_head=4),\n",
    "            NewBlock(feature_dim, num_head=4),\n",
    "            NewBlock(feature_dim, num_head=4)\n",
    "        )\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, feature_dim)\n",
    "        self.language_model_head = nn.Linear(feature_dim, vocab_size)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, feature_dim)\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        batch_size, sequence_length = indices.shape\n",
    "\n",
    "        token_embedding = self.token_embedding_table(indices)\n",
    "        posiitional_indices = self.positional_embedding_table(torch.arange(sequence_length))\n",
    "        x = token_embedding + posiitional_indices\n",
    "        x = self.blocks(x)\n",
    "        logits = self.language_model_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, sequence_length, vocab_channels = logits.shape\n",
    "            logits_flattened = logits.view(batch_size * sequence_length, vocab_channels)\n",
    "            targets_flattened = targets.view(batch_size * sequence_length)\n",
    "            loss = F.cross_entropy(logits_flattened, targets_flattened)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, indices, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            indices_condensed = indices[:, -block_size:]\n",
    "            logits, _ = self(indices_condensed)\n",
    "            last_token_logits = logits[:, -1, :]\n",
    "            probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "            next_token_index = torch.multinomial(probabilities, num_samples=1)\n",
    "            indices = torch.cat((indices, next_token_index), dim=1)\n",
    "        return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0850d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLMH = NonLinearMultiHead(vocab_size, feature_dim=32)\n",
    "optimizer_NLMH = torch.optim.AdamW(NLMH.parameters(), lr=1e-3)\n",
    "NLMH_history = train_model(NLMH, optimizer_NLMH, training_steps=5000)\n",
    "NLMH_metrics = evaluate_model(NLMH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detokenize(NLMH.generate(indices=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48790e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve_NLMH(history, eval_interval):\n",
    "    \"\"\"\n",
    "    Visualizes the loss reduction over time.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create the x-axis based on the evaluation intervals\n",
    "    steps = [i * eval_interval for i in range(len(history))]\n",
    "    \n",
    "    plt.plot(steps, history, label='Training Loss', color='#2ecc71', linewidth=2)\n",
    "    \n",
    "    # Formatting the chart\n",
    "    plt.title('NLMH Model Learning Curve', fontsize=14)\n",
    "    plt.xlabel('Training Steps', fontsize=12)\n",
    "    plt.ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Call the function using the history from your training\n",
    "plot_learning_curve_NLMH(NLMH_history, eval_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c6e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's add a layernorm to the model\n",
    "class LayerNorm1d(nn.Module):\n",
    "    def __init__(self, feature_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(feature_dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(feature_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6599f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to define block with layernorm\n",
    "class BlockWithLayerNorm(nn.Module):\n",
    "    def __init__(self, feature_dim, num_head):\n",
    "        super().__init__()\n",
    "        head_size = feature_dim // num_head\n",
    "        self.self_attention_head = NewMultiHead(num_head, head_size, feature_dim)\n",
    "        self.ffwd = NewFeedForward(feature_dim)\n",
    "        self.ln1 = LayerNorm1d(feature_dim)\n",
    "        self.ln2 = LayerNorm1d(feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attention_head(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d36aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets build a model and instead of previous block call blockwithlayernorm on it\n",
    "\n",
    "class NonLinearLayerNorm(nn.Module):\n",
    "    def __init__(self, vocab_size, feature_dim):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.blocks = nn.Sequential(\n",
    "            BlockWithLayerNorm(feature_dim, num_head=4),\n",
    "            BlockWithLayerNorm(feature_dim, num_head=4),\n",
    "            BlockWithLayerNorm(feature_dim, num_head=4),\n",
    "            LayerNorm1d(feature_dim)\n",
    "        )\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, feature_dim)\n",
    "        self.language_model_head = nn.Linear(feature_dim, vocab_size)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, feature_dim)\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        batch_size, sequence_length = indices.shape\n",
    "\n",
    "        token_embedding = self.token_embedding_table(indices)\n",
    "        posiitional_indices = self.positional_embedding_table(torch.arange(sequence_length))\n",
    "        x = token_embedding + posiitional_indices\n",
    "        x = self.blocks(x)\n",
    "        logits = self.language_model_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, sequence_length, vocab_channels = logits.shape\n",
    "            logits_flattened = logits.view(batch_size * sequence_length, vocab_channels)\n",
    "            targets_flattened = targets.view(batch_size * sequence_length)\n",
    "            loss = F.cross_entropy(logits_flattened, targets_flattened)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, indices, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            indices_condensed = indices[:, -block_size:]\n",
    "            logits, _ = self(indices_condensed)\n",
    "            last_token_logits = logits[:, -1, :]\n",
    "            probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "            next_token_index = torch.multinomial(probabilities, num_samples=1)\n",
    "            indices = torch.cat((indices, next_token_index), dim=1)\n",
    "        return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445d5c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLLN = NonLinearLayerNorm(vocab_size, feature_dim=32)\n",
    "optimizer_NLLN = torch.optim.AdamW(NLLN.parameters(), lr=1e-3)\n",
    "NLLN_history = train_model(NLLN, optimizer_NLLN, training_steps=5000)\n",
    "NLLN_metrics = evaluate_model(NLLN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f61902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detokenize(NLLN.generate(indices=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04400b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve_NLLN(history, eval_interval):\n",
    "    \"\"\"\n",
    "    Visualizes the loss reduction over time.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create the x-axis based on the evaluation intervals\n",
    "    steps = [i * eval_interval for i in range(len(history))]\n",
    "    \n",
    "    plt.plot(steps, history, label='Training Loss', color='#2ecc71', linewidth=2)\n",
    "    \n",
    "    # Formatting the chart\n",
    "    plt.title('NLLN Model Learning Curve', fontsize=14)\n",
    "    plt.xlabel('Training Steps', fontsize=12)\n",
    "    plt.ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Call the function using the history from your training\n",
    "plot_learning_curve_NLLN(NLLN_history, eval_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's add dropout to feedforward and multiheadattention and our block\n",
    "class FeedForwardWithDropout(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim * 4, feature_dim),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class MultiHeadWithDropout(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, feature_dim):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.feature_dim = feature_dim\n",
    "        self.heads = nn.ModuleList([HeadWithDropout(feature_dim, head_size) for _ in range(num_heads)])  # Pass feature_dim first\n",
    "        self.projection = nn.Linear(feature_dim, feature_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.projection(out)\n",
    "        return self.dropout(out)    \n",
    "    \n",
    "\n",
    "class BlockWithDropout(nn.Module):\n",
    "    def __init__(self, feature_dim, num_head):\n",
    "        super().__init__()\n",
    "        head_size = feature_dim // num_head\n",
    "        self.self_attention_head = MultiHeadWithDropout(num_head, head_size, feature_dim)\n",
    "        self.ffwd = FeedForwardWithDropout(feature_dim)\n",
    "        self.ln1 = LayerNorm1d(feature_dim)\n",
    "        self.ln2 = LayerNorm1d(feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attention_head(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x    \n",
    "    \n",
    "\n",
    "# we can also add dropout to Head\n",
    "\n",
    "class HeadWithDropout(nn.Module):\n",
    "    def __init__(self, feature_dim, head_size):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(feature_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(feature_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(feature_dim, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, sequence_length, feature_dim = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        wei = q @ k.transpose(-2, -1) * feature_dim ** -0.5\n",
    "        wei = wei.masked_fill(self.tril[:sequence_length, :sequence_length] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        out = wei @ v\n",
    "        return out    \n",
    "    \n",
    "#Let's tune some of our Hyperparameters\n",
    "\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "feature_dim = 384\n",
    "num_head = 6\n",
    "n_layers = 6\n",
    "dropout = 0.2\n",
    "# now let's build a model and instead of previous block call blockwithlayernorm on it\n",
    "\n",
    "class NonLinearDropout(nn.Module):\n",
    "    def __init__(self, vocab_size, feature_dim):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[BlockWithDropout(feature_dim, num_head) for _ in range(n_layers)],\n",
    "            LayerNorm1d(feature_dim)\n",
    "        )\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, feature_dim)\n",
    "        self.language_model_head = nn.Linear(feature_dim, vocab_size)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, feature_dim)\n",
    "\n",
    "    def forward(self, indices, targets=None):\n",
    "        batch_size, sequence_length = indices.shape\n",
    "\n",
    "        token_embedding = self.token_embedding_table(indices)\n",
    "        posiitional_indices = self.positional_embedding_table(torch.arange(sequence_length))\n",
    "        x = token_embedding + posiitional_indices\n",
    "        x = self.blocks(x)\n",
    "        logits = self.language_model_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, sequence_length, vocab_channels = logits.shape\n",
    "            logits_flattened = logits.view(batch_size * sequence_length, vocab_channels)\n",
    "            targets_flattened = targets.view(batch_size * sequence_length)\n",
    "            loss = F.cross_entropy(logits_flattened, targets_flattened)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, indices, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            indices_condensed = indices[:, -block_size:]\n",
    "            logits, _ = self(indices_condensed)\n",
    "            last_token_logits = logits[:, -1, :]\n",
    "            probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "            next_token_index = torch.multinomial(probabilities, num_samples=1)\n",
    "            indices = torch.cat((indices, next_token_index), dim=1)\n",
    "        return indices    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a64cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLDO = NonLinearDropout(vocab_size, feature_dim=feature_dim)\n",
    "optimizer_NLDO = torch.optim.AdamW(NLDO.parameters(), lr=learning_rate)\n",
    "NLDO_history = train_model(NLDO, optimizer_NLDO, training_steps=max_iters)\n",
    "NLDO_metrics = evaluate_model(NLDO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5d1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detokenize(NLDO.generate(indices=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e5557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve_NLDO(history, eval_interval):\n",
    "    \"\"\"\n",
    "    Visualizes the loss reduction over time.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create the x-axis based on the evaluation intervals\n",
    "    steps = [i * eval_interval for i in range(len(history))]\n",
    "    \n",
    "    plt.plot(steps, history, label='Training Loss', color='#2ecc71', linewidth=2)\n",
    "    \n",
    "    # Formatting the chart\n",
    "    plt.title('NLDO Model Learning Curve', fontsize=14)\n",
    "    plt.xlabel('Training Steps', fontsize=12)\n",
    "    plt.ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Call the function using the history from your training\n",
    "plot_learning_curve_NLLN(NLDO_history, eval_interval=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
