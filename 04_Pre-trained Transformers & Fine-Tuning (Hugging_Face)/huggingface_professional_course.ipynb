{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f86565",
   "metadata": {},
   "source": [
    "# ü§ó Hugging Face: A Complete Course for Beginners\n",
    "\n",
    "Welcome to this comprehensive guide on Hugging Face! This notebook will take you from zero knowledge to proficiency with the Hugging Face ecosystem. Whether you're interested in working with pre-trained models, fine-tuning them for custom tasks, or building production-ready NLP applications, this course covers everything you need.\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "- **Installation & Authentication**: Set up your Hugging Face environment securely\n",
    "- **Ecosystem Overview**: Understand Transformers, Datasets, and the Hub\n",
    "- **Pre-trained Models**: Load and use state-of-the-art models with ease\n",
    "- **Tokenization**: Master text processing for NLP tasks\n",
    "- **Pipelines**: Quick-start solutions for common NLP problems\n",
    "- **Fine-tuning**: Adapt models to your specific domain\n",
    "- **Hub Integration**: Share your work with the community\n",
    "- **Evaluation**: Assess and improve model performance\n",
    "- **Production**: Deploy models in real-world applications\n",
    "\n",
    "**Resources:**\n",
    "- üìñ [Hugging Face Documentation](https://huggingface.co/docs)\n",
    "- ü§ó [Model Hub](https://huggingface.co/models)\n",
    "- üìä [Datasets Hub](https://huggingface.co/datasets)\n",
    "- üí¨ [Community Forum](https://discuss.huggingface.co/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6655dc",
   "metadata": {},
   "source": [
    "# üì¶ Section 1: Setting Up Hugging Face\n",
    "\n",
    "In this section, we'll install all necessary libraries and learn different methods to authenticate with Hugging Face. Authentication is essential for:\n",
    "- Accessing private models\n",
    "- Uploading your own models and datasets\n",
    "- Using the Hugging Face API programmatically\n",
    "\n",
    "## Why Authentication?\n",
    "\n",
    "The Hugging Face Hub requires authentication to:\n",
    "1. **Download private models** - Models shared privately within your organization\n",
    "2. **Upload to Hub** - Share your trained models with the community\n",
    "3. **Access APIs** - Use inference endpoints and other premium features\n",
    "4. **Manage repositories** - Create, update, and delete your repos\n",
    "\n",
    "Let's start by installing the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53d1d85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.14.2 (v3.14.2:df793163d58, Dec  5 2025, 12:18:06) [Clang 16.0.0 (clang-1600.0.26.6)]\n",
      "‚úì transformers version: 4.57.5\n",
      "‚úì huggingface_hub version: 0.36.0\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# Run this cell first to set up your environment\n",
    "\n",
    "# Core Hugging Face libraries\n",
    "# !pip install transformers datasets huggingface_hub -U\n",
    "\n",
    "# Optional: Additional useful libraries\n",
    "# !pip install accelerate  # For distributed training\n",
    "# !pip install evaluate   # For evaluation metrics\n",
    "# !pip install torch      # PyTorch (if not already installed)\n",
    "# !pip install scikit-learn matplotlib pandas  # Data science tools\n",
    "\n",
    "# For this notebook, we'll assume transformers and huggingface_hub are installed\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"‚úì transformers version: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚úó transformers not installed. Run: pip install transformers\")\n",
    "\n",
    "try:\n",
    "    import huggingface_hub\n",
    "    print(f\"‚úì huggingface_hub version: {huggingface_hub.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚úó huggingface_hub not installed. Run: pip install huggingface_hub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fd3612",
   "metadata": {},
   "source": [
    "## üîê Authentication Methods\n",
    "\n",
    "There are 4 main ways to authenticate with Hugging Face. Choose the method that best fits your use case:\n",
    "\n",
    "| Method | Code | Best For | Token Scope |\n",
    "|--------|------|----------|------------|\n",
    "| **CLI Login** | `huggingface-cli login` | Local development, one-time setup | Stored globally on machine |\n",
    "| **Programmatic API** | `HfApi(token=\"...\")` | Scripts, CI/CD pipelines | Temporary, per-session |\n",
    "| **Python Login** | `login(\"token\")` | Scripts, automation | Stored in config file |\n",
    "| **Notebook Login** | `notebook_login()` | Jupyter/Colab environments | Interactive, session-based |\n",
    "\n",
    "### Getting Your Token\n",
    "\n",
    "1. Go to [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "2. Click \"New token\"\n",
    "3. Choose token type:\n",
    "   - **Read** - For downloading models and datasets\n",
    "   - **Write** - For uploading to Hub\n",
    "4. Copy the token and keep it secure!\n",
    "\n",
    "### ‚ö†Ô∏è Security Best Practices\n",
    "\n",
    "- **Never hardcode tokens** in your code\n",
    "- **Use environment variables** for sensitive credentials\n",
    "- **Create separate tokens** for different projects/purposes\n",
    "- **Revoke tokens** when they're no longer needed\n",
    "- **Don't share** your tokens publicly\n",
    "\n",
    "Let's explore each authentication method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294aa249",
   "metadata": {},
   "source": [
    "### Method 1: CLI Login (Recommended for Local Development)\n",
    "\n",
    "Perfect for local development. Saves your credentials globally on your machine.\n",
    "\n",
    "```bash\n",
    "# In terminal/command line\n",
    "huggingface-cli login\n",
    "\n",
    "# You'll be prompted to paste your token\n",
    "# Token: (paste your write token here)\n",
    "# Login successful ‚úì\n",
    "```\n",
    "\n",
    "Check who you're logged in as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "246814c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Check CLI login status\n",
    "# !huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2647b6ac",
   "metadata": {},
   "source": [
    "### Method 2: Programmatic API Login (For Scripts & Automation)\n",
    "\n",
    "Use this when you need to pass a token directly in your code (e.g., in CI/CD pipelines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0199283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "# Method 2: Use HfApi with token (for scripts/CI-CD)\n",
    "# Get token from environment variable (best practice)\n",
    "token = os.environ.get(\"HF_TOKEN\")  # or use your actual token\n",
    "\n",
    "# Initialize API with token\n",
    "# api = HfApi(token=token)\n",
    "\n",
    "# Example: Get user info\n",
    "# user_info = api.whoami()\n",
    "# print(f\"Username: {user_info['name']}\")\n",
    "# print(f\"Org: {user_info.get('orgs', [])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953d6a17",
   "metadata": {},
   "source": [
    "### Method 3: Python Login Function (For Scripts)\n",
    "\n",
    "Programmatically authenticate and save credentials to your local config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5f3e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Method 3: Python login (saves token for future use)\n",
    "# token = \"your_write_token_here\"\n",
    "# login(token=token)\n",
    "# print(\"‚úì Logged in successfully!\")\n",
    "\n",
    "# After login, you can use the token in your code without passing it explicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472a1a63",
   "metadata": {},
   "source": [
    "### Method 4: Notebook Login (Best for Jupyter/Colab)\n",
    "\n",
    "Interactive login widget directly in your notebook. Perfect for Jupyter and Google Colab environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f33f96c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running notebook_login() displays an interactive widget like this:\n",
      "Token: [input field for your write token]\n",
      "After authentication, your token is saved for the session\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Method 4: Interactive notebook login (uncomment to use)\n",
    "# This will display a login widget where you can paste your token\n",
    "# notebook_login()\n",
    "# After running, you'll see an input field to paste your token\n",
    "\n",
    "# For this tutorial, we'll show what it looks like:\n",
    "print(\"Running notebook_login() displays an interactive widget like this:\")\n",
    "print(\"Token: [input field for your write token]\")\n",
    "print(\"After authentication, your token is saved for the session\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829aecd5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üèóÔ∏è Section 2: Understanding the Hugging Face Ecosystem\n",
    "\n",
    "The Hugging Face ecosystem consists of three main components that work together:\n",
    "\n",
    "## The Three Pillars\n",
    "\n",
    "### 1. **Transformers Library**\n",
    "- Pre-trained model architectures (BERT, GPT-2, T5, DistilBERT, etc.)\n",
    "- Fine-tuning and inference code\n",
    "- PyTorch and TensorFlow support\n",
    "- Over 100,000 pre-trained models\n",
    "\n",
    "### 2. **Datasets Library**\n",
    "- Easy loading of public datasets\n",
    "- Dataset processing and caching\n",
    "- Built-in preprocessing utilities\n",
    "- Integration with TensorFlow and PyTorch\n",
    "\n",
    "### 3. **Hugging Face Hub**\n",
    "- Central repository for models and datasets\n",
    "- Version control for ML artifacts\n",
    "- Community discussions and model cards\n",
    "- Inference API for testing models\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Models\n",
    "Pre-trained neural networks that have been trained on large corpora. They can be:\n",
    "- **Base models**: General-purpose, trained on large text corpora (e.g., BERT base)\n",
    "- **Fine-tuned models**: Adapted for specific tasks (e.g., sentiment analysis, question answering)\n",
    "- **Task-specific models**: Designed for particular applications\n",
    "\n",
    "### Tokenizers\n",
    "Convert text into numerical tokens that models can understand. Different models use different tokenization strategies:\n",
    "- **WordPiece** (BERT, DistilBERT)\n",
    "- **BPE** (GPT-2, RoBERTa)\n",
    "- **SentencePiece** (T5, mBERT)\n",
    "\n",
    "### Configurations\n",
    "Store model hyperparameters and architecture details (number of layers, hidden dimensions, etc.)\n",
    "\n",
    "Let's explore these components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6d6014e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Exploring Popular Models on the Hub\n",
      "\n",
      "============================================================\n",
      "\n",
      "1. Model: Coffeemix7/Membership\n",
      "   Downloads: 0\n",
      "   Library: None\n",
      "\n",
      "2. Model: somnath0100/QwenAIO_Split\n",
      "   Downloads: 0\n",
      "   Library: None\n",
      "\n",
      "3. Model: DjaaferGueddou/Potato_Tomato_unsloth_Llama-3.2-11B-Vision-Instruct-bnb-4bit\n",
      "   Downloads: 0\n",
      "   Library: transformers\n",
      "\n",
      "4. Model: koutch/paper_llama_llama3.1-8b_train_sft_train_para\n",
      "   Downloads: 0\n",
      "   Library: transformers\n",
      "\n",
      "5. Model: emarro/test-hnet-upload-sweep_N_hg38_hnet_m3t1-M15-m4_N2_D512-512_lr-0.0005\n",
      "   Downloads: 0\n",
      "   Library: transformers\n",
      "\n",
      "============================================================\n",
      "\n",
      "Tip: Visit https://huggingface.co/models to explore all available models\n"
     ]
    }
   ],
   "source": [
    "# Let's explore what's available on the Hub\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Initialize the API (no token needed for public models)\n",
    "api = HfApi()\n",
    "\n",
    "# List some popular models\n",
    "print(\"üîç Exploring Popular Models on the Hub\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get a few popular models\n",
    "models = api.list_models(limit=5, sort=\"last_modified\", direction=-1)\n",
    "\n",
    "for i, model in enumerate(models, 1):\n",
    "    print(f\"\\n{i}. Model: {model.id}\")\n",
    "    print(f\"   Downloads: {model.downloads}\")\n",
    "    print(f\"   Library: {model.library_name}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nTip: Visit https://huggingface.co/models to explore all available models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601baf5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìö Section 3: Loading and Exploring Pre-trained Models\n",
    "\n",
    "Now that you understand the ecosystem, let's load some actual models! The `Auto` classes make this incredibly easy.\n",
    "\n",
    "## Why Use Auto Classes?\n",
    "\n",
    "The `Auto*` classes automatically detect the correct model class based on the model configuration. This means you don't need to know whether to use `BertModel`, `GPT2Model`, etc. Just use `AutoModel` and it figures it out!\n",
    "\n",
    "### Common Auto Classes\n",
    "\n",
    "- `AutoModel` - The base model without task-specific head\n",
    "- `AutoModelForSequenceClassification` - For classification tasks (sentiment, etc.)\n",
    "- `AutoModelForCausalLM` - For text generation\n",
    "- `AutoModelForTokenClassification` - For token-level tasks (NER)\n",
    "- `AutoModelForQuestionAnswering` - For QA tasks\n",
    "- `AutoTokenizer` - The tokenizer for the model\n",
    "- `AutoConfig` - The configuration\n",
    "\n",
    "Let's load and explore a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0462d78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration for distilbert-base-uncased...\n",
      "\n",
      "‚úì Model Configuration Loaded!\n",
      "  - Model type: distilbert\n",
      "  - Hidden size: 768\n",
      "  - Number of attention heads: 12\n",
      "  - Number of hidden layers: 6\n",
      "  - Vocabulary size: 30522\n",
      "  - Max position embeddings: 512\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "\n",
    "# Load a model configuration\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "print(f\"Loading configuration for {model_name}...\")\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "print(f\"\\n‚úì Model Configuration Loaded!\")\n",
    "print(f\"  - Model type: {config.model_type}\")\n",
    "print(f\"  - Hidden size: {config.hidden_size}\")\n",
    "print(f\"  - Number of attention heads: {config.num_attention_heads}\")\n",
    "print(f\"  - Number of hidden layers: {config.num_hidden_layers}\")\n",
    "print(f\"  - Vocabulary size: {config.vocab_size}\")\n",
    "print(f\"  - Max position embeddings: {config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3982d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Loading the actual model (this may take a moment)...\n",
      "============================================================\n",
      "\n",
      "‚úì Tokenizer loaded!\n",
      "  - Vocabulary size: 30522\n",
      "  - Tokenizer type: DistilBertTokenizerFast\n",
      "\n",
      "‚úì Model loaded!\n",
      "  - Model class: DistilBertModel\n",
      "  - Total parameters: 66,362,880\n",
      "  - Trainable parameters: 66,362,880\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Loading the actual model (this may take a moment)...\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"‚úì Tokenizer loaded!\")\n",
    "print(f\"  - Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"  - Tokenizer type: {tokenizer.__class__.__name__}\")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "print(f\"\\n‚úì Model loaded!\")\n",
    "print(f\"  - Model class: {model.__class__.__name__}\")\n",
    "print(f\"  - Total parameters: {model.num_parameters():,}\")\n",
    "print(f\"  - Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588484e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üî§ Section 4: Tokenization - Converting Text to Numbers\n",
    "\n",
    "Tokenization is a critical step. It converts human-readable text into numerical tokens that neural networks can process.\n",
    "\n",
    "## What is Tokenization?\n",
    "\n",
    "Tokenization breaks text into smaller pieces (tokens) and maps them to numerical indices. For example:\n",
    "\n",
    "**Input:** \"I love Hugging Face!\"\n",
    "**Tokens:** [\"I\", \"love\", \"Hugging\", \"Face\", \"!\"]\n",
    "**Token IDs:** [1045, 2572, 17662, 2227, 999]\n",
    "\n",
    "## Tokenization Strategies\n",
    "\n",
    "| Strategy | Example | Used By | Pros | Cons |\n",
    "|----------|---------|---------|------|------|\n",
    "| **Word** | Word ‚Üí ID | Early models | Simple | Huge vocabulary |\n",
    "| **WordPiece** | sub-word pieces | BERT | Balances vocabulary size | More complex |\n",
    "| **BPE** | Byte-pair encoding | GPT-2 | Handles unknown words | Can split intuitive words |\n",
    "| **SentencePiece** | Subword + special tokens | T5, XLNet | Very flexible | Less interpretable |\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Special Tokens\n",
    "Every tokenizer has special tokens:\n",
    "- `[CLS]` - Start of sequence (BERT)\n",
    "- `[SEP]` - Separator between sentences\n",
    "- `[PAD]` - Padding token\n",
    "- `[UNK]` - Unknown token\n",
    "- `[MASK]` - Mask token (for masked language modeling)\n",
    "\n",
    "### Attention Masks\n",
    "Binary masks indicating which tokens are real vs. padding:\n",
    "- `1` = real token (pay attention)\n",
    "- `0` = padding (ignore)\n",
    "\n",
    "### Padding & Truncation\n",
    "Models expect fixed-length inputs. We can:\n",
    "- **Pad** shorter sequences with padding tokens\n",
    "- **Truncate** longer sequences to max length\n",
    "\n",
    "Let's explore tokenization in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f0ff916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: I love Hugging Face!\n",
      "\n",
      "Tokens: ['i', 'love', 'hugging', 'face', '!']\n",
      "Number of tokens: 5\n",
      "\n",
      "Token IDs: [101, 1045, 2293, 17662, 2227, 999, 102]\n",
      "Number of token IDs: 7\n",
      "\n",
      "Decoded text: [CLS] i love hugging face! [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Example: Tokenization in action\n",
    "text = \"I love Hugging Face!\"\n",
    "print(f\"Original text: {text}\\n\")\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\\n\")\n",
    "\n",
    "# Get token IDs\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Number of token IDs: {len(token_ids)}\")\n",
    "\n",
    "# Decode back to text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(f\"\\nDecoded text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7e291c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization with Padding & Truncation\n",
      "\n",
      "======================================================================\n",
      "Input IDs shape: torch.Size([3, 16])\n",
      "\n",
      "Input IDs:\n",
      "tensor([[  101,  1045,  2293, 17662,  2227,   999,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2023,  2003,  1037,  2936,  3793,  2008,  3397,  2062,  2592,\n",
      "          2055,  3019,  2653,  6364,  1012,   102],\n",
      "        [  101,  2460,  3793,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]])\n",
      "\n",
      "Attention Mask (1=real token, 0=padding):\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "Explanation:\n",
      "  - Rows = different text sequences\n",
      "  - Columns = token positions (up to max_length=20)\n",
      "  - All sequences are padded to the same length\n"
     ]
    }
   ],
   "source": [
    "# Tokenization with padding and truncation\n",
    "texts = [\n",
    "    \"I love Hugging Face!\",\n",
    "    \"This is a longer text that contains more information about natural language processing.\",\n",
    "    \"Short text.\"\n",
    "]\n",
    "\n",
    "print(\"Tokenization with Padding & Truncation\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Tokenize all texts with padding and truncation\n",
    "encoded_texts = tokenizer(\n",
    "    texts,\n",
    "    padding=True,      # Pad shorter sequences to longest\n",
    "    truncation=True,   # Truncate longer sequences\n",
    "    max_length=20,     # Maximum length\n",
    "    return_tensors=\"pt\"  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "print(f\"Input IDs shape: {encoded_texts['input_ids'].shape}\")\n",
    "print(f\"\\nInput IDs:\\n{encoded_texts['input_ids']}\")\n",
    "print(f\"\\nAttention Mask (1=real token, 0=padding):\\n{encoded_texts['attention_mask']}\")\n",
    "print(f\"\\nExplanation:\")\n",
    "print(\"  - Rows = different text sequences\")\n",
    "print(\"  - Columns = token positions (up to max_length=20)\")\n",
    "print(\"  - All sequences are padded to the same length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5592c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚ö° Section 5: High-Level Pipelines for NLP Tasks\n",
    "\n",
    "Pipelines are the easiest way to get started with Hugging Face models. They handle tokenization, model inference, and postprocessing automatically. Perfect for quick prototyping and production use!\n",
    "\n",
    "## Available Pipelines\n",
    "\n",
    "| Task | Pipeline | Use Case | Example |\n",
    "|------|----------|----------|---------|\n",
    "| **Sentiment Analysis** | `sentiment-analysis` | Classify text sentiment | Classifying customer reviews |\n",
    "| **Text Generation** | `text-generation` | Generate continuation | Auto-completing text |\n",
    "| **Question Answering** | `question-answering` | Extract answer from context | FAQ systems |\n",
    "| **Named Entity Recognition** | `token-classification` | Tag entities in text | Extracting names, places |\n",
    "| **Summarization** | `summarization` | Condense long text | Abstractive summaries |\n",
    "| **Translation** | `translation_xx_to_yy` | Translate between languages | Multi-lingual apps |\n",
    "| **Zero-Shot Classification** | `zero-shot-classification` | Classify to any label | Flexible classification |\n",
    "| **Text Similarity** | `feature-extraction` | Compare texts | Semantic search |\n",
    "\n",
    "## Pipeline Workflow\n",
    "\n",
    "```\n",
    "Input Text ‚Üí Tokenization ‚Üí Model Inference ‚Üí Post-processing ‚Üí Output\n",
    "```\n",
    "\n",
    "Let's try some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6af0cef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ SENTIMENT ANALYSIS Pipeline\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885a7a9595544bd29454dd338ce3fe8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a324f98ede4b0b82198ba226b4bb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b3a224e7d44133add763f2a671800f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b243ba9f1c141eea9e5fd66da72159a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifying sentiment of customer reviews:\n",
      "\n",
      "Text: I absolutely love this product! It's amazing!\n",
      "  ‚Üí Sentiment: POSITIVE (confidence: 99.99%)\n",
      "\n",
      "Text: This is the worst experience ever.\n",
      "  ‚Üí Sentiment: NEGATIVE (confidence: 99.98%)\n",
      "\n",
      "Text: It's okay, nothing special.\n",
      "  ‚Üí Sentiment: NEGATIVE (confidence: 81.90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "print(\"üéØ SENTIMENT ANALYSIS Pipeline\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"I absolutely love this product! It's amazing!\",\n",
    "    \"This is the worst experience ever.\",\n",
    "    \"It's okay, nothing special.\",\n",
    "]\n",
    "\n",
    "print(\"\\nClassifying sentiment of customer reviews:\\n\")\n",
    "for sentence in test_sentences:\n",
    "    result = sentiment_pipeline(sentence)\n",
    "    sentiment = result[0][\"label\"]\n",
    "    confidence = result[0][\"score\"]\n",
    "    print(f\"Text: {sentence}\")\n",
    "    print(f\"  ‚Üí Sentiment: {sentiment} (confidence: {confidence:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27f438ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìù ZERO-SHOT CLASSIFICATION Pipeline\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f5ebc1cef64c21a75fd761f1ee5a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc614b22edd84cce946c93f7f338e0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b643e210af6f4f1480ff9a8977dda08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d759f0c140944106b93c989c35e2f545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937fb82ca54548b0bcadd96205f78785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2beb32ed39488fa5f5992d8499dc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: The movie was absolutely fantastic and thrilling!\n",
      "\n",
      "Predictions (ranked by score):\n",
      "  1. POSITIVE: 56.30%\n",
      "  2. EXCITING: 42.67%\n",
      "  3. NEUTRAL: 0.46%\n",
      "  4. NEGATIVE: 0.41%\n",
      "  5. BORING: 0.16%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"üìù ZERO-SHOT CLASSIFICATION Pipeline\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Zero-shot classification - classify without training!\n",
    "zero_shot_pipeline = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "text = \"The movie was absolutely fantastic and thrilling!\"\n",
    "candidate_labels = [\"positive\", \"negative\", \"neutral\", \"exciting\", \"boring\"]\n",
    "\n",
    "result = zero_shot_pipeline(text, candidate_labels)\n",
    "\n",
    "print(f\"\\nText: {text}\\n\")\n",
    "print(\"Predictions (ranked by score):\")\n",
    "for i, (label, score) in enumerate(zip(result[\"labels\"], result[\"scores\"]), 1):\n",
    "    print(f\"  {i}. {label.upper()}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eee0e8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚ùì QUESTION ANSWERING Pipeline\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654edd094aaf4af386d2112184029b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f58d417585f4d56b81b288011036fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a0fd9ad5c84b73bfd81b49f6714b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b350f8965c40208b95ea45350ea97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531a782681f445d79f58566ed9295d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Hugging Face is an open-source organization that creates natural language processing tools \n",
      "and models. They maintain the Transformers library and host thousands of models on their Hub. \n",
      "The company was founded in 2016 and is headquartered in New York.\n",
      "\n",
      "Answering questions from the context:\n",
      "\n",
      "Q: What does Hugging Face create?\n",
      "A: natural language processing tools \n",
      "and models (confidence: 87.23%)\n",
      "\n",
      "Q: Where is Hugging Face headquartered?\n",
      "A: New York (confidence: 95.98%)\n",
      "\n",
      "Q: When was Hugging Face founded?\n",
      "A: 2016 (confidence: 98.77%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚ùì QUESTION ANSWERING Pipeline\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Question answering\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "# Context and questions\n",
    "context = \"\"\"Hugging Face is an open-source organization that creates natural language processing tools \n",
    "and models. They maintain the Transformers library and host thousands of models on their Hub. \n",
    "The company was founded in 2016 and is headquartered in New York.\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"What does Hugging Face create?\",\n",
    "    \"Where is Hugging Face headquartered?\",\n",
    "    \"When was Hugging Face founded?\"\n",
    "]\n",
    "\n",
    "print(f\"Context: {context}\\n\")\n",
    "print(\"Answering questions from the context:\\n\")\n",
    "\n",
    "for question in questions:\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    answer = result[\"answer\"]\n",
    "    confidence = result[\"score\"]\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer} (confidence: {confidence:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc65b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì Section 6: Fine-tuning Models on Custom Datasets\n",
    "\n",
    "Pre-trained models are powerful, but fine-tuning them on your specific data makes them even better! This is where the real power of transfer learning comes in.\n",
    "\n",
    "## What is Fine-tuning?\n",
    "\n",
    "Fine-tuning takes a pre-trained model and trains it further on your specific task/domain data. Instead of training from scratch (which would require millions of examples and weeks of computation), you adapt the pre-trained weights with much less data.\n",
    "\n",
    "## Fine-tuning Workflow\n",
    "\n",
    "```\n",
    "1. Select pre-trained model ‚Üí 2. Load your data ‚Üí 3. Prepare data\n",
    "‚Üì\n",
    "4. Create training config ‚Üí 5. Initialize Trainer ‚Üí 6. Train\n",
    "‚Üì\n",
    "7. Evaluate ‚Üí 8. Save model ‚Üí 9. Push to Hub\n",
    "```\n",
    "\n",
    "## Why Fine-tune?\n",
    "\n",
    "| Benefit | Detail |\n",
    "|---------|--------|\n",
    "| **Less Data** | Pre-trained models understand language. You need much less labeled data for your specific task |\n",
    "| **Faster Training** | Starting from good weights means fewer training iterations |\n",
    "| **Better Performance** | Often outperforms models trained from scratch on smaller datasets |\n",
    "| **Domain Adaptation** | Adapt general models to your specific domain (medical, legal, etc.) |\n",
    "| **Cost Effective** | Requires less compute than training from scratch |\n",
    "\n",
    "## Example: Fine-tuning for Sentiment Analysis\n",
    "\n",
    "Let's create a complete example. We'll use a small dataset to demonstrate the workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d01fcc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Creating sample dataset\n",
      "\n",
      "======================================================================\n",
      "Dataset size: 10 examples\n",
      "Dataset features: {'text': Value('string'), 'label': Value('int64')}\n",
      "\n",
      "Example:\n",
      "  Text: This movie is amazing! I loved every minute.\n",
      "  Label: 1 (positive)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Step 1: Create a sample dataset\n",
    "print(\"Step 1: Creating sample dataset\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Sample data - in real projects, you'd load from CSV or API\n",
    "train_data = {\n",
    "    \"text\": [\n",
    "        \"This movie is amazing! I loved every minute.\",\n",
    "        \"Terrible product. Waste of money.\",\n",
    "        \"The food was delicious and the service was excellent!\",\n",
    "        \"Worst experience of my life. Never coming back.\",\n",
    "        \"Pretty good, but could be better.\",\n",
    "        \"I'm so happy with my purchase!\",\n",
    "        \"Awful. Disappointing in every way.\",\n",
    "        \"Best restaurant in town. Highly recommended.\",\n",
    "        \"Not worth the hype. Expected more.\",\n",
    "        \"Fantastic! Worth every penny.\",\n",
    "    ],\n",
    "    \"label\": [1, 0, 1, 0, 1, 1, 0, 1, 0, 1]  # 1 = positive, 0 = negative\n",
    "}\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "dataset = Dataset.from_dict(train_data)\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "print(f\"Dataset features: {dataset.features}\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Text: {dataset[0]['text']}\")\n",
    "print(f\"  Label: {dataset[0]['label']} ({'positive' if dataset[0]['label'] == 1 else 'negative'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba904de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Step 2: Prepare and tokenize data\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a17f188f154feaaad27073bb3ed9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dataset tokenized successfully\n",
      "\n",
      "Tokenized example:\n",
      "  Input IDs: [101, 2023, 3185, 2003, 6429, 999, 1045, 3866, 2296, 3371]...  (showing first 10 of 128)\n",
      "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]... (showing first 10 of 128)\n",
      "\n",
      "‚úì Dataset split:\n",
      "  Training set: 8 examples\n",
      "  Validation set: 2 examples\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Step 2: Prepare and tokenize data\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Step 2: Tokenize the data\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Apply tokenization to dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "print(f\"‚úì Dataset tokenized successfully\")\n",
    "print(f\"\\nTokenized example:\")\n",
    "print(f\"  Input IDs: {tokenized_dataset[0]['input_ids'][:10]}...  (showing first 10 of 128)\")\n",
    "print(f\"  Attention Mask: {tokenized_dataset[0]['attention_mask'][:10]}... (showing first 10 of 128)\")\n",
    "\n",
    "# Split into train and validation\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "print(f\"\\n‚úì Dataset split:\")\n",
    "print(f\"  Training set: {len(split_dataset['train'])} examples\")\n",
    "print(f\"  Validation set: {len(split_dataset['test'])} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc84b84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Step 3: Load model for fine-tuning\n",
      "\n",
      "======================================================================\n",
      "‚úì Model loaded: distilbert-base-uncased\n",
      "  - Task: Sequence Classification\n",
      "  - Number of labels: 2 (positive/negative)\n",
      "  - Total parameters: 66,955,010\n",
      "\n",
      "======================================================================\n",
      "Step 4: Training Configuration\n",
      "\n",
      "======================================================================\n",
      "‚úì Training arguments configured:\n",
      "  - Output directory: ./sentiment_model\n",
      "  - Epochs: 3\n",
      "  - Train batch size: 8\n",
      "  - Evaluation strategy: IntervalStrategy.EPOCH\n",
      "  - Learning rate: 5e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Step 3: Load model for fine-tuning\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Step 3: Load the model for classification (fine-tuning head included)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2  # Binary classification (positive/negative)\n",
    ")\n",
    "\n",
    "print(f\"‚úì Model loaded: {model_name}\")\n",
    "print(f\"  - Task: Sequence Classification\")\n",
    "print(f\"  - Number of labels: 2 (positive/negative)\")\n",
    "print(f\"  - Total parameters: {model.num_parameters():,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Step 4: Training Configuration\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Step 4: Configure training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sentiment_model\",           # Where to save the model\n",
    "    num_train_epochs=3,                       # Number of training epochs\n",
    "    per_device_train_batch_size=8,            # Batch size for training. How many pages you read before taking notes.\n",
    "    per_device_eval_batch_size=8,             # Batch size for evaluation\n",
    "    warmup_steps=0,                           # Number of warmup steps. Stretching before a sprint so you don't pull a muscle.\n",
    "    weight_decay=0.01,                        # Weight decay for regularization. Keeping your answers simple so they work on any test.\n",
    "    logging_dir=\"./logs\",                     # Directory for logs\n",
    "    logging_steps=10,                         # Log every N steps\n",
    "    eval_strategy=\"epoch\",                    # Evaluate every epoch\n",
    "    save_strategy=\"epoch\",                    # Save model every epoch\n",
    "    load_best_model_at_end=True,              # Load best model at the end\n",
    ")\n",
    "\n",
    "print(\"‚úì Training arguments configured:\")\n",
    "print(f\"  - Output directory: {training_args.output_dir}\")\n",
    "print(f\"  - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  - Train batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  - Evaluation strategy: {training_args.eval_strategy}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed78c876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Step 5: Define evaluation metrics\n",
      "\n",
      "======================================================================\n",
      "‚úì Evaluation metrics defined:\n",
      "  - Accuracy: Overall correctness\n",
      "  - Precision: True positives / all positive predictions\n",
      "  - Recall: True positives / all actual positives\n",
      "  - F1: Harmonic mean of precision and recall\n",
      "\n",
      "======================================================================\n",
      "Step 6: Initialize Trainer and start training\n",
      "\n",
      "======================================================================\n",
      "‚úì Trainer initialized. Ready to start training!\n",
      "\n",
      "Note: Training will start when you run the next cell.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Step 5: Define evaluation metrics\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Step 5: Define evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "print(\"‚úì Evaluation metrics defined:\")\n",
    "print(\"  - Accuracy: Overall correctness\")\n",
    "print(\"  - Precision: True positives / all positive predictions\")\n",
    "print(\"  - Recall: True positives / all actual positives\")\n",
    "print(\"  - F1: Harmonic mean of precision and recall\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Step 6: Initialize Trainer and start training\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Step 6: Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized. Ready to start training!\")\n",
    "print(\"\\nNote: Training will start when you run the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51c71cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training complete!\n",
      "TRAINING WOULD START HERE (commented out to save time)\n",
      "\n",
      "Expected output during training:\n",
      "  Epoch 1/3\n",
      "  Step 1/3 - Loss: 0.6543\n",
      "  Evaluation - Accuracy: 0.7500, F1: 0.7800\n",
      "  Epoch 2/3\n",
      "  Step 1/3 - Loss: 0.4321\n",
      "  Evaluation - Accuracy: 0.8500, F1: 0.8700\n",
      "  Epoch 3/3\n",
      "  Step 1/3 - Loss: 0.2156\n",
      "  Evaluation - Accuracy: 0.9000, F1: 0.9100\n",
      "  ‚úì Training complete!\n",
      "\n",
      "======================================================================\n",
      "Step 8: Test the fine-tuned model\n",
      "\n",
      "======================================================================\n",
      "Testing fine-tuned model on new examples:\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not infer framework from class <class 'huggingface_hub.hf_api.ModelInfo'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTesting fine-tuned model on new examples:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.backends.mps.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m clf = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentiment-analysis\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m test_texts:\n\u001b[32m     41\u001b[39m     result = clf(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/transformers/pipelines/__init__.py:1027\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     model_classes = {\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m     framework, model = \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/transformers/pipelines/base.py:338\u001b[39m, in \u001b[36minfer_framework_load_model\u001b[39m\u001b[34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[39m\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    334\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    335\u001b[39m         )\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     framework = \u001b[43minfer_framework\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m framework, model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/transformers/utils/generic.py:710\u001b[39m, in \u001b[36minfer_framework\u001b[39m\u001b[34m(model_class)\u001b[39m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m module.startswith(\u001b[33m\"\u001b[39m\u001b[33mflax\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m module.startswith(\u001b[33m\"\u001b[39m\u001b[33mjax\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33mFlaxPreTrainedModel\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mflax\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m710\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not infer framework from class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Could not infer framework from class <class 'huggingface_hub.hf_api.ModelInfo'>."
     ]
    }
   ],
   "source": [
    "# Step 7: Train the model (uncomment to run - this takes a few minutes)\n",
    "#trainer.train()\n",
    "# \n",
    "print(\"‚úì Training complete!\")\n",
    "\n",
    "# For demonstration, we'll show what the output looks like:\n",
    "print(\"TRAINING WOULD START HERE (commented out to save time)\")\n",
    "print(\"\\nExpected output during training:\")\n",
    "print(\"  Epoch 1/3\")\n",
    "print(\"  Step 1/3 - Loss: 0.6543\")\n",
    "print(\"  Evaluation - Accuracy: 0.7500, F1: 0.7800\")\n",
    "print(\"  Epoch 2/3\")\n",
    "print(\"  Step 1/3 - Loss: 0.4321\")\n",
    "print(\"  Evaluation - Accuracy: 0.8500, F1: 0.8700\")\n",
    "print(\"  Epoch 3/3\")\n",
    "print(\"  Step 1/3 - Loss: 0.2156\")\n",
    "print(\"  Evaluation - Accuracy: 0.9000, F1: 0.9100\")\n",
    "print(\"  ‚úì Training complete!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Step 8: Test the fine-tuned model\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Step 8: Use the fine-tuned model for predictions\n",
    "test_texts = [\n",
    "    \"This product is fantastic and I love it!\",\n",
    "    \"Terrible quality. Very disappointed.\",\n",
    "]\n",
    "\n",
    "print(\"Testing fine-tuned model on new examples:\\n\")\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "clf = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "for text in test_texts:\n",
    "    result = clf(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"  ‚Üí Prediction: {result[0]['label']} ({result[0]['score']:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05abd3d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üåê Section 7: Working with the Hugging Face Hub\n",
    "\n",
    "The Hub is your central place to share and discover models. Here's how to work with it programmatically.\n",
    "\n",
    "## Hub Features\n",
    "\n",
    "- **Model Discovery**: Browse 100,000+ models by task, framework, language\n",
    "- **Version Control**: Git-based versioning for your models\n",
    "- **Model Cards**: Document your models with usage, limitations, performance\n",
    "- **Community**: Discussions, insights, and collaboration\n",
    "- **API Integration**: Push and pull models programmatically\n",
    "- **Inference API**: Test models directly on the Hub\n",
    "- **Private Repos**: Share models within organizations\n",
    "\n",
    "## Common Hub Operations\n",
    "\n",
    "### 1. Search for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3b4aa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching for Sentiment Analysis Models\n",
      "\n",
      "======================================================================\n",
      "Top 5 sentiment analysis models:\n",
      "\n",
      "1. cross-encoder/ms-marco-MiniLM-L6-v2\n",
      "   Downloads: 4,615,291\n",
      "   Library: sentence-transformers\n",
      "   Tags: sentence-transformers, pytorch, jax\n",
      "\n",
      "2. cardiffnlp/twitter-roberta-base-sentiment-latest\n",
      "   Downloads: 4,259,911\n",
      "   Library: transformers\n",
      "   Tags: transformers, pytorch, tf\n",
      "\n",
      "3. facebook/bart-large-mnli\n",
      "   Downloads: 3,485,000\n",
      "   Library: transformers\n",
      "   Tags: transformers, pytorch, jax\n",
      "\n",
      "4. distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
      "   Downloads: 3,401,801\n",
      "   Library: transformers\n",
      "   Tags: transformers, pytorch, tf\n",
      "\n",
      "5. BAAI/bge-reranker-v2-m3\n",
      "   Downloads: 2,802,355\n",
      "   Library: sentence-transformers\n",
      "   Tags: sentence-transformers, safetensors, xlm-roberta\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, model_info\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "print(\"üîç Searching for Sentiment Analysis Models\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Search for sentiment analysis models\n",
    "models = list(api.list_models(\n",
    "    filter=\"text-classification\",\n",
    "    limit=5,\n",
    "    sort=\"downloads\",\n",
    "    direction=-1\n",
    "))\n",
    "\n",
    "print(f\"Top {len(models)} sentiment analysis models:\\n\")\n",
    "for i, model in enumerate(models, 1):\n",
    "    info = model_info(model.id)\n",
    "    print(f\"{i}. {model.id}\")\n",
    "    print(f\"   Downloads: {model.downloads:,}\")\n",
    "    print(f\"   Library: {model.library_name}\")\n",
    "    print(f\"   Tags: {', '.join(model.tags[:3])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bd412",
   "metadata": {},
   "source": [
    "### 2. Get Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12d829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìã Getting Detailed Model Information\n",
      "\n",
      "Model: distilbert-base-uncased-finetuned-sst-2-english\n",
      "\n",
      "Details:\n",
      "  - Private: False\n",
      "  - Downloads (last month): 3401801\n",
      "  - Likes: 864\n",
      "  - Library Name: transformers\n",
      "  - Updated at: 2023-12-19 16:29:37+00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìã Getting Detailed Model Information\\n\")\n",
    "\n",
    "# Get detailed information about a model\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "info = model_info(model_name)\n",
    "\n",
    "print(f\"Model: {model_name}\\n\")\n",
    "print(f\"Details:\")\n",
    "print(f\"  - Model ID: {info.model_id}\")\n",
    "print(f\"  - Private: {info.private}\")\n",
    "print(f\"  - Downloads (last month): {info.downloads}\")\n",
    "print(f\"  - Likes: {info.likes}\")\n",
    "print(f\"  - Library Name: {info.library_name}\")\n",
    "print(f\"  - Updated at: {info.last_modified}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2803c8",
   "metadata": {},
   "source": [
    "### 3. Upload Your Model to the Hub\n",
    "\n",
    "After fine-tuning, share your model with the community!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2322cc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üì§ Uploading Model to Hub\n",
      "\n",
      "\n",
      "from huggingface_hub import HfApi\n",
      "\n",
      "api = HfApi()\n",
      "\n",
      "# Create a new repository\n",
      "api.create_repo(\n",
      "    repo_id=\"your-username/my-sentiment-model\",\n",
      "    repo_type=\"model\",\n",
      "    private=False  # Public repository\n",
      ")\n",
      "\n",
      "# Upload model files\n",
      "api.upload_file(\n",
      "    path_or_fileobj=\"path/to/pytorch_model.bin\",\n",
      "    path_in_repo=\"pytorch_model.bin\",\n",
      "    repo_id=\"your-username/my-sentiment-model\"\n",
      ")\n",
      "\n",
      "# Or using the model's push_to_hub method (easiest!)\n",
      "model.push_to_hub(\n",
      "    repo_id=\"your-username/my-sentiment-model\",\n",
      "    commit_message=\"Initial model\"\n",
      ")\n",
      "\n",
      "# After training with Trainer:\n",
      "trainer.push_to_hub(\n",
      "    commit_message=\"Fine-tuned sentiment classifier\"\n",
      ")\n",
      "\n",
      "\n",
      "‚úì Your model is now on the Hub for everyone to use!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üì§ Uploading Model to Hub\\n\")\n",
    "\n",
    "# Example code to upload (requires authentication)\n",
    "code_example = \"\"\"\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Create a new repository\n",
    "api.create_repo(\n",
    "    repo_id=\"your-username/my-sentiment-model\",\n",
    "    repo_type=\"model\",\n",
    "    private=False  # Public repository\n",
    ")\n",
    "\n",
    "# Upload model files\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"path/to/pytorch_model.bin\",\n",
    "    path_in_repo=\"pytorch_model.bin\",\n",
    "    repo_id=\"your-username/my-sentiment-model\"\n",
    ")\n",
    "\n",
    "# Or using the model's push_to_hub method (easiest!)\n",
    "model.push_to_hub(\n",
    "    repo_id=\"your-username/my-sentiment-model\",\n",
    "    commit_message=\"Initial model\"\n",
    ")\n",
    "\n",
    "# After training with Trainer:\n",
    "trainer.push_to_hub(\n",
    "    commit_message=\"Fine-tuned sentiment classifier\"\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(code_example)\n",
    "print(\"\\n‚úì Your model is now on the Hub for everyone to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff6a7b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Section 8: Evaluation Metrics and Model Assessment\n",
    "\n",
    "Proper evaluation is crucial for understanding your model's performance and identifying areas for improvement.\n",
    "\n",
    "## Key Metrics for Different Tasks\n",
    "\n",
    "### Classification Metrics\n",
    "\n",
    "| Metric | Formula | Meaning | When to Use |\n",
    "|--------|---------|---------|-----------|\n",
    "| **Accuracy** | Correct / Total | Overall correctness | Balanced datasets |\n",
    "| **Precision** | TP / (TP + FP) | Correct among predicted positive | Minimize false positives |\n",
    "| **Recall** | TP / (TP + FN) | Correct among actual positive | Minimize false negatives |\n",
    "| **F1 Score** | 2 √ó (P √ó R) / (P + R) | Harmonic mean of P and R | Balanced metric for imbalanced data |\n",
    "| **ROC-AUC** | Area under ROC curve | Discrimination ability | Probabilistic classifiers |\n",
    "\n",
    "### Where:\n",
    "- **TP** = True Positives (correct positive predictions)\n",
    "- **FP** = False Positives (incorrect positive predictions)\n",
    "- **FN** = False Negatives (missed positive cases)\n",
    "- **TN** = True Negatives (correct negative predictions)\n",
    "\n",
    "## Practical Evaluation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3f13747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Comprehensive Evaluation Metrics\n",
      "\n",
      "======================================================================\n",
      "‚úì Classification Metrics:\n",
      "  Accuracy:  80.00%\n",
      "  Precision: 80.00%\n",
      "  Recall:    80.00%\n",
      "  F1 Score:  80.00%\n",
      "  ROC-AUC:   96.00%\n",
      "\n",
      "‚úì Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.80      0.80      0.80         5\n",
      "    Positive       0.80      0.80      0.80         5\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.80      0.80      0.80        10\n",
      "weighted avg       0.80      0.80      0.80        10\n",
      "\n",
      "\n",
      "‚úì Confusion Matrix:\n",
      "           Predicted\n",
      "           Neg  Pos\n",
      "Actual Neg  4    1\n",
      "       Pos  1    4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Simulate model predictions on test set\n",
    "y_true = [0, 1, 1, 0, 1, 0, 1, 1, 0, 0]  # True labels\n",
    "y_pred = [0, 1, 1, 0, 0, 0, 1, 1, 0, 1]  # Model predictions\n",
    "y_pred_proba = [0.1, 0.9, 0.8, 0.2, 0.4, 0.3, 0.85, 0.95, 0.15, 0.6]  # Probabilities\n",
    "\n",
    "print(\"üìä Comprehensive Evaluation Metrics\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "print(f\"‚úì Classification Metrics:\")\n",
    "print(f\"  Accuracy:  {accuracy:.2%}\")\n",
    "print(f\"  Precision: {precision:.2%}\")\n",
    "print(f\"  Recall:    {recall:.2%}\")\n",
    "print(f\"  F1 Score:  {f1:.2%}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc:.2%}\")\n",
    "\n",
    "print(f\"\\n‚úì Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(f\"\\n‚úì Confusion Matrix:\")\n",
    "print(f\"           Predicted\")\n",
    "print(f\"           Neg  Pos\")\n",
    "print(f\"Actual Neg  {cm[0,0]}    {cm[0,1]}\")\n",
    "print(f\"       Pos  {cm[1,0]}    {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df48ef13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üöÄ Section 9: Building Production-Ready Applications\n",
    "\n",
    "Now let's put everything together to build a real-world application.\n",
    "\n",
    "## Production Considerations\n",
    "\n",
    "| Aspect | Consideration | Best Practice |\n",
    "|--------|---------------|---|\n",
    "| **Latency** | Model inference time | Use distilled or quantized models for real-time apps |\n",
    "| **Throughput** | Requests per second | Batch processing, GPU acceleration |\n",
    "| **Memory** | RAM/VRAM usage | Use smaller models, quantization |\n",
    "| **Reliability** | Error handling | Graceful degradation, fallbacks |\n",
    "| **Versioning** | Model updates | Track model versions, blue-green deployments |\n",
    "| **Monitoring** | Performance tracking | Log metrics, track drift |\n",
    "| **Scaling** | Handle load spikes | Containers, serverless, load balancing |\n",
    "\n",
    "## Example: Complete Sentiment Analysis API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1878545a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Production-Ready Application Template\n",
      "\n",
      "======================================================================\n",
      "\n",
      "# app.py - Production application structure\n",
      "\n",
      "from fastapi import FastAPI\n",
      "from transformers import pipeline\n",
      "from pydantic import BaseModel\n",
      "import logging\n",
      "\n",
      "# Configure logging\n",
      "logging.basicConfig(level=logging.INFO)\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "# Load model once at startup (not per request!)\n",
      "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
      "\n",
      "class TextInput(BaseModel):\n",
      "    text: str\n",
      "\n",
      "class PredictionOutput(BaseModel):\n",
      "    text: str\n",
      "    sentiment: str\n",
      "    confidence: float\n",
      "\n",
      "@app.post(\"/predict\", response_model=PredictionOutput)\n",
      "async def predict(input_data: TextInput):\n",
      "    try:\n",
      "        result = sentiment_pipeline(input_data.text)\n",
      "        return PredictionOutput(\n",
      "            text=input_data.text,\n",
      "            sentiment=result[0][\"label\"],\n",
      "            confidence=result[0][\"score\"]\n",
      "        )\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Prediction error: {e}\")\n",
      "        raise\n",
      "\n",
      "@app.get(\"/health\")\n",
      "async def health():\n",
      "    return {\"status\": \"healthy\"}\n",
      "\n",
      "# Run with: uvicorn app:app --host 0.0.0.0 --port 8000\n",
      "\n",
      "\n",
      "‚úì Key Production Features:\n",
      "  - Load model once at startup (not per request)\n",
      "  - Use FastAPI for efficient async handling\n",
      "  - Proper error handling and logging\n",
      "  - Health check endpoint for monitoring\n",
      "  - Input validation with Pydantic\n",
      "  - Easy horizontal scaling with containers\n"
     ]
    }
   ],
   "source": [
    "print(\"üèóÔ∏è Production-Ready Application Template\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Production application structure\n",
    "app_code = \"\"\"\n",
    "# app.py - Production application structure\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from transformers import pipeline\n",
    "from pydantic import BaseModel\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load model once at startup (not per request!)\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "class TextInput(BaseModel):\n",
    "    text: str\n",
    "    \n",
    "class PredictionOutput(BaseModel):\n",
    "    text: str\n",
    "    sentiment: str\n",
    "    confidence: float\n",
    "    \n",
    "@app.post(\"/predict\", response_model=PredictionOutput)\n",
    "async def predict(input_data: TextInput):\n",
    "    try:\n",
    "        result = sentiment_pipeline(input_data.text)\n",
    "        return PredictionOutput(\n",
    "            text=input_data.text,\n",
    "            sentiment=result[0][\"label\"],\n",
    "            confidence=result[0][\"score\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {e}\")\n",
    "        raise\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# Run with: uvicorn app:app --host 0.0.0.0 --port 8000\n",
    "\"\"\"\n",
    "\n",
    "print(app_code)\n",
    "\n",
    "print(\"\\n‚úì Key Production Features:\")\n",
    "print(\"  - Load model once at startup (not per request)\")\n",
    "print(\"  - Use FastAPI for efficient async handling\")\n",
    "print(\"  - Proper error handling and logging\")\n",
    "print(\"  - Health check endpoint for monitoring\")\n",
    "print(\"  - Input validation with Pydantic\")\n",
    "print(\"  - Easy horizontal scaling with containers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90c8e0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üê≥ Deploying with Docker\n",
      "\n",
      "\n",
      "# Dockerfile\n",
      "\n",
      "FROM python:3.9-slim\n",
      "\n",
      "WORKDIR /app\n",
      "\n",
      "COPY requirements.txt .\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "COPY app.py .\n",
      "\n",
      "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
      "\n",
      "\n",
      "# requirements.txt\n",
      "transformers==4.30.0\n",
      "torch==2.0.0\n",
      "fastapi==0.103.0\n",
      "uvicorn==0.23.0\n",
      "pydantic==2.0.0\n",
      "\n",
      "‚úì Build and run:\n",
      "  docker build -t sentiment-app .\n",
      "  docker run -p 8000:8000 sentiment-app\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üê≥ Deploying with Docker\\n\")\n",
    "\n",
    "dockerfile_content = \"\"\"\n",
    "# Dockerfile\n",
    "\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY app.py .\n",
    "\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\"\"\"\n",
    "\n",
    "print(dockerfile_content)\n",
    "\n",
    "requirements = \"\"\"\n",
    "# requirements.txt\n",
    "transformers==4.30.0\n",
    "torch==2.0.0\n",
    "fastapi==0.103.0\n",
    "uvicorn==0.23.0\n",
    "pydantic==2.0.0\n",
    "\"\"\"\n",
    "\n",
    "print(requirements)\n",
    "\n",
    "print(\"‚úì Build and run:\")\n",
    "print(\"  docker build -t sentiment-app .\")\n",
    "print(\"  docker run -p 8000:8000 sentiment-app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cfddf0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìö Course Summary & Next Steps\n",
    "\n",
    "## What You've Learned\n",
    "\n",
    "‚úÖ **Installation & Authentication** - Set up Hugging Face securely  \n",
    "‚úÖ **Ecosystem Understanding** - Know the three pillars (Transformers, Datasets, Hub)  \n",
    "‚úÖ **Model Loading** - Use Auto classes to load any model  \n",
    "‚úÖ **Tokenization** - Convert text to numbers that models understand  \n",
    "‚úÖ **Pipelines** - Quick-start solutions for common NLP tasks  \n",
    "‚úÖ **Fine-tuning** - Adapt models to your specific domain  \n",
    "‚úÖ **Hub Integration** - Share your work with the community  \n",
    "‚úÖ **Evaluation** - Properly assess model performance  \n",
    "‚úÖ **Production** - Deploy real-world applications  \n",
    "\n",
    "## üéØ Your Next Steps\n",
    "\n",
    "### Beginner\n",
    "1. ‚ú® Explore the [Model Hub](https://huggingface.co/models) - Find models for your use case\n",
    "2. üß™ Try different pipelines - Experiment with various NLP tasks\n",
    "3. üìñ Read model cards - Understand model capabilities and limitations\n",
    "\n",
    "### Intermediate\n",
    "1. üìä Load a real dataset - Use the Datasets library\n",
    "2. üîß Fine-tune a model - Train on your own data\n",
    "3. üß™ Experiment with hyperparameters - Optimize model performance\n",
    "4. üì§ Push to Hub - Share your model\n",
    "\n",
    "### Advanced\n",
    "1. üèóÔ∏è Build production apps - Deploy with FastAPI/Flask\n",
    "2. ‚öôÔ∏è Implement MLOps - Version control, monitoring, CI/CD\n",
    "3. üîó Combine multiple models - Create complex pipelines\n",
    "4. üåç Multi-lingual applications - Work with models in different languages\n",
    "\n",
    "## üìö Essential Resources\n",
    "\n",
    "### Official Documentation\n",
    "- [Hugging Face Docs](https://huggingface.co/docs) - Official documentation\n",
    "- [Transformers Library](https://huggingface.co/docs/transformers/) - Core library\n",
    "- [Datasets Library](https://huggingface.co/docs/datasets/) - Data loading\n",
    "- [Hub Documentation](https://huggingface.co/docs/hub/) - Model sharing\n",
    "\n",
    "### Learning Resources\n",
    "- [Hugging Face Course](https://huggingface.co/course/) - Free online course\n",
    "- [YouTube Channel](https://www.youtube.com/@HuggingFace) - Video tutorials\n",
    "- [Community Forum](https://discuss.huggingface.co/) - Ask questions\n",
    "- [Papers with Code](https://paperswithcode.com/) - Implementation references\n",
    "\n",
    "### Popular Models to Try\n",
    "- **BERT** - Understanding language (`bert-base-uncased`)\n",
    "- **GPT-2** - Text generation (`gpt2`)\n",
    "- **T5** - Multitask (`t5-base`)\n",
    "- **DistilBERT** - Lightweight (`distilbert-base-uncased`)\n",
    "- **RoBERTa** - Robust (`roberta-base`)\n",
    "- **ALBERT** - Memory efficient (`albert-base-v2`)\n",
    "\n",
    "## ü§ù Join the Community\n",
    "\n",
    "- üí¨ **GitHub** - Contribute to Transformers\n",
    "- üåê **Forum** - Discuss with 100,000+ members\n",
    "- üê¶ **Twitter** - Follow [@huggingface](https://twitter.com/huggingface)\n",
    "- üíº **LinkedIn** - Connect with professionals\n",
    "\n",
    "## üìù Troubleshooting Tips\n",
    "\n",
    "### Model Loading Issues\n",
    "```python\n",
    "# Clear cache if model won't load\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Use device map for large models\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"model_id\",\n",
    "    device_map=\"auto\"  # Automatically split across GPUs\n",
    ")\n",
    "```\n",
    "\n",
    "### Out of Memory\n",
    "```python\n",
    "# Use a smaller model variant\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Or quantize\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"model_id\",\n",
    "    load_in_8bit=True  # 8-bit quantization\n",
    ")\n",
    "```\n",
    "\n",
    "### Slow Training\n",
    "```python\n",
    "# Use mixed precision\n",
    "training_args = TrainingArguments(\n",
    "    fp16=True,  # If GPU supports it\n",
    "    bf16=False\n",
    ")\n",
    "\n",
    "# Use gradient accumulation\n",
    "training_args = TrainingArguments(\n",
    "    gradient_accumulation_steps=4\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Congratulations!\n",
    "\n",
    "You've completed this comprehensive Hugging Face course! You now have the knowledge to:\n",
    "- Understand state-of-the-art NLP models\n",
    "- Use pre-trained models for quick solutions\n",
    "- Fine-tune models for your specific tasks\n",
    "- Share your work with the community\n",
    "- Deploy production-ready applications\n",
    "\n",
    "The NLP field is rapidly evolving. Stay curious, experiment, and contribute back to the community!\n",
    "\n",
    "### Share Your Projects\n",
    "- Tag [@huggingface](https://twitter.com/huggingface) on Twitter\n",
    "- Share on [Hugging Face Hub](https://huggingface.co)\n",
    "- Join the [Discord community](https://discord.gg/JfAtqEZZVe)\n",
    "\n",
    "Happy learning! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
