{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence (Seq2Seq) Model for Neural Machine Translation\n",
    "\n",
    "This notebook explains a complete implementation of a Seq2Seq model for translating German to English using PyTorch. The model uses an Encoder-Decoder architecture with LSTM networks.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Imports and Setup](#imports)\n",
    "2. [Data Preprocessing](#data)\n",
    "3. [Encoder Architecture](#encoder)\n",
    "4. [Decoder Architecture](#decoder)\n",
    "5. [Seq2Seq Model](#seq2seq)\n",
    "6. [Training Setup](#training)\n",
    "7. [Training Loop](#loop)\n",
    "8. [Evaluation](#evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup {#imports}\n",
    "\n",
    "First, let's import all necessary libraries and understand what each one does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k  # German-English translation dataset\n",
    "from torchtext.data import Field, BucketIterator  # Data preprocessing utilities\n",
    "import numpy as np\n",
    "import spacy  # Natural language processing library for tokenization\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter  # For visualization\n",
    "\n",
    "# Note: utils.py contains helper functions for translation, BLEU score, and checkpointing\n",
    "# from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Language Models\n",
    "\n",
    "We use spaCy for tokenization - breaking sentences into individual words/tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy language models for German and English\n",
    "# These need to be installed: python -m spacy download de_core_news_sm en_core_news_sm\n",
    "try:\n",
    "    spacy_ger = spacy.load(\"de_core_news_sm\")\n",
    "    spacy_eng = spacy.load(\"en_core_news_sm\")\n",
    "    print(\"SpaCy models loaded successfully!\")\n",
    "except OSError:\n",
    "    print(\"Please install spaCy models:\")\n",
    "    print(\"python -m spacy download de_core_news_sm\")\n",
    "    print(\"python -m spacy download en_core_news_sm\")\n",
    "    # Fallback to basic tokenization\n",
    "    spacy_ger = None\n",
    "    spacy_eng = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Functions\n",
    "\n",
    "Tokenization converts sentences into lists of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ger(text):\n",
    "    \"\"\"Tokenize German text into individual words\"\"\"\n",
    "    if spacy_ger:\n",
    "        return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
    "    else:\n",
    "        return text.split()  # Simple fallback\n",
    "\n",
    "def tokenize_eng(text):\n",
    "    \"\"\"Tokenize English text into individual words\"\"\"\n",
    "    if spacy_eng:\n",
    "        return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
    "    else:\n",
    "        return text.split()  # Simple fallback\n",
    "\n",
    "# Test tokenization\n",
    "german_sentence = \"Ein Mann geht zur Schule.\"\n",
    "english_sentence = \"A man goes to school.\"\n",
    "\n",
    "print(f\"German: {german_sentence}\")\n",
    "print(f\"Tokenized: {tokenize_ger(german_sentence)}\")\n",
    "print(f\"\\nEnglish: {english_sentence}\")\n",
    "print(f\"Tokenized: {tokenize_eng(english_sentence)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing {#data}\n",
    "\n",
    "### Field Definition\n",
    "\n",
    "Fields define how to process the text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fields for German and English\n",
    "german = Field(\n",
    "    tokenize=tokenize_ger,  # How to split text into tokens\n",
    "    lower=True,            # Convert to lowercase\n",
    "    init_token=\"<sos>\",     # Start of sequence token\n",
    "    eos_token=\"<eos>\"       # End of sequence token\n",
    ")\n",
    "\n",
    "english = Field(\n",
    "    tokenize=tokenize_eng,\n",
    "    lower=True,\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\"\n",
    ")\n",
    "\n",
    "print(\"Fields created successfully!\")\n",
    "print(\"Special tokens:\")\n",
    "print(f\"- Start of sequence: {german.init_token}\")\n",
    "print(f\"- End of sequence: {german.eos_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading\n",
    "\n",
    "Multi30k is a multilingual dataset with ~30k sentence pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Multi30k dataset\n",
    "try:\n",
    "    train_data, valid_data, test_data = Multi30k.splits(\n",
    "        exts=(\".de\", \".en\"),           # File extensions for German and English\n",
    "        fields=(german, english)       # Fields to use for processing\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Training examples: {len(train_data)}\")\n",
    "    print(f\"Validation examples: {len(valid_data)}\")\n",
    "    print(f\"Test examples: {len(test_data)}\")\n",
    "    \n",
    "    # Show a sample\n",
    "    print(\"\\nSample training example:\")\n",
    "    print(f\"German: {' '.join(train_data[0].src)}\")\n",
    "    print(f\"English: {' '.join(train_data[0].trg)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"This might be due to network issues or dataset availability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Building\n",
    "\n",
    "Build vocabularies from the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabularies\n",
    "german.build_vocab(\n",
    "    train_data,\n",
    "    max_size=10000,  # Maximum vocabulary size\n",
    "    min_freq=2       # Minimum frequency for a word to be included\n",
    ")\n",
    "\n",
    "english.build_vocab(\n",
    "    train_data,\n",
    "    max_size=10000,\n",
    "    min_freq=2\n",
    ")\n",
    "\n",
    "print(f\"German vocabulary size: {len(german.vocab)}\")\n",
    "print(f\"English vocabulary size: {len(english.vocab)}\")\n",
    "\n",
    "# Show special tokens\n",
    "print(\"\\nSpecial tokens in vocabulary:\")\n",
    "print(f\"Unknown token: {german.vocab.itos[0]}\")\n",
    "print(f\"Padding token: {german.vocab.itos[1]}\")\n",
    "print(f\"Start token: {german.vocab.itos[2]}\")\n",
    "print(f\"End token: {german.vocab.itos[3]}\")\n",
    "\n",
    "# Show most common words\n",
    "print(\"\\nMost common German words:\")\n",
    "for i in range(4, 14):\n",
    "    print(f\"{i-3}. {german.vocab.itos[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoder Architecture {#encoder}\n",
    "\n",
    "The encoder processes the input sequence and creates a context representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Store parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Layers\n",
    "        self.dropout = nn.Dropout(p)  # Regularization\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)  # Word embeddings\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, batch_size)\n",
    "        \n",
    "        # Convert word indices to embeddings\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, batch_size, embedding_size)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape: (seq_length, batch_size, hidden_size)\n",
    "        # hidden shape: (num_layers, batch_size, hidden_size)\n",
    "        # cell shape: (num_layers, batch_size, hidden_size)\n",
    "        \n",
    "        # Return final hidden and cell states (context)\n",
    "        return hidden, cell\n",
    "\n",
    "print(\"Encoder class defined!\")\n",
    "print(\"\\nEncoder Architecture:\")\n",
    "print(\"1. Embedding layer: converts word indices to dense vectors\")\n",
    "print(\"2. LSTM layers: process sequence and maintain memory\")\n",
    "print(\"3. Output: final hidden and cell states as context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Visualization\n",
    "\n",
    "Let's create a simple encoder to understand its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample encoder\n",
    "sample_encoder = Encoder(\n",
    "    input_size=1000,      # Vocabulary size\n",
    "    embedding_size=256,   # Embedding dimension\n",
    "    hidden_size=512,      # LSTM hidden size\n",
    "    num_layers=2,         # Number of LSTM layers\n",
    "    p=0.5                 # Dropout probability\n",
    ")\n",
    "\n",
    "print(f\"Sample Encoder:\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in sample_encoder.parameters()):,}\")\n",
    "print(f\"\\nLayer details:\")\n",
    "for name, layer in sample_encoder.named_children():\n",
    "    print(f\"- {name}: {layer}\")\n",
    "\n",
    "# Test with dummy input\n",
    "dummy_input = torch.randint(0, 1000, (10, 2))  # seq_len=10, batch_size=2\n",
    "hidden, cell = sample_encoder(dummy_input)\n",
    "\n",
    "print(f\"\\nInput shape: {dummy_input.shape}\")\n",
    "print(f\"Hidden state shape: {hidden.shape}\")\n",
    "print(f\"Cell state shape: {cell.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decoder Architecture {#decoder}\n",
    "\n",
    "The decoder generates the output sequence one word at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Store parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Layers\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)  # Output projection\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape: (batch_size) - single word input\n",
    "        # We need to add sequence dimension\n",
    "        x = x.unsqueeze(0)  # Shape: (1, batch_size)\n",
    "        \n",
    "        # Convert to embeddings\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, batch_size, embedding_size)\n",
    "        \n",
    "        # Pass through LSTM with previous context\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # outputs shape: (1, batch_size, hidden_size)\n",
    "        \n",
    "        # Project to vocabulary size\n",
    "        predictions = self.fc(outputs)\n",
    "        # predictions shape: (1, batch_size, vocab_size)\n",
    "        \n",
    "        # Remove sequence dimension\n",
    "        predictions = predictions.squeeze(0)\n",
    "        # predictions shape: (batch_size, vocab_size)\n",
    "        \n",
    "        return predictions, hidden, cell\n",
    "\n",
    "print(\"Decoder class defined!\")\n",
    "print(\"\\nDecoder Architecture:\")\n",
    "print(\"1. Embedding layer: converts word indices to dense vectors\")\n",
    "print(\"2. LSTM layers: generate next word using context\")\n",
    "print(\"3. Linear layer: project to vocabulary probabilities\")\n",
    "print(\"4. Process one word at a time during generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample decoder\n",
    "sample_decoder = Decoder(\n",
    "    input_size=1000,      # Input vocabulary size\n",
    "    embedding_size=256,   # Embedding dimension\n",
    "    hidden_size=512,      # LSTM hidden size (must match encoder)\n",
    "    output_size=1000,     # Output vocabulary size\n",
    "    num_layers=2,         # Number of LSTM layers (must match encoder)\n",
    "    p=0.5                 # Dropout probability\n",
    ")\n",
    "\n",
    "print(f\"Sample Decoder:\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in sample_decoder.parameters()):,}\")\n",
    "\n",
    "# Test with dummy input and context from encoder\n",
    "dummy_word = torch.randint(0, 1000, (2,))  # batch_size=2, single word\n",
    "predictions, new_hidden, new_cell = sample_decoder(dummy_word, hidden, cell)\n",
    "\n",
    "print(f\"\\nInput word shape: {dummy_word.shape}\")\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"New hidden state shape: {new_hidden.shape}\")\n",
    "print(f\"New cell state shape: {new_cell.shape}\")\n",
    "\n",
    "# Show prediction probabilities\n",
    "probs = torch.softmax(predictions[0], dim=0)\n",
    "top_words = torch.topk(probs, 5)\n",
    "print(f\"\\nTop 5 predicted word indices: {top_words.indices.tolist()}\")\n",
    "print(f\"Their probabilities: {top_words.values.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Seq2Seq Model {#seq2seq}\n",
    "\n",
    "The complete model combines encoder and decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        # source shape: (src_len, batch_size)\n",
    "        # target shape: (trg_len, batch_size)\n",
    "        \n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(english.vocab) if 'english' in globals() else 1000\n",
    "        \n",
    "        # Store decoder outputs\n",
    "        device = source.device\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        \n",
    "        # Encode the source sequence\n",
    "        hidden, cell = self.encoder(source)\n",
    "        \n",
    "        # First input to decoder is <SOS> token\n",
    "        x = target[0]  # Shape: (batch_size)\n",
    "        \n",
    "        # Generate target sequence\n",
    "        for t in range(1, target_len):\n",
    "            # Get prediction for current timestep\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            \n",
    "            # Store prediction\n",
    "            outputs[t] = output\n",
    "            \n",
    "            # Get best predicted word\n",
    "            best_guess = output.argmax(1)\n",
    "            \n",
    "            # Teacher forcing: use actual target word with probability teacher_force_ratio\n",
    "            # Otherwise use predicted word\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "print(\"Seq2Seq class defined!\")\n",
    "print(\"\\nSeq2Seq Architecture:\")\n",
    "print(\"1. Encoder processes entire source sequence\")\n",
    "print(\"2. Decoder generates target sequence word by word\")\n",
    "print(\"3. Teacher forcing helps training stability\")\n",
    "print(\"4. Context flows from encoder to decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher Forcing Explanation\n",
    "\n",
    "Teacher forcing is a training technique where we sometimes use the actual target word instead of the predicted word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate teacher forcing decisions\n",
    "def simulate_teacher_forcing(teacher_force_ratio, sequence_length=10):\n",
    "    decisions = []\n",
    "    for _ in range(sequence_length):\n",
    "        use_teacher = random.random() < teacher_force_ratio\n",
    "        decisions.append(\"Teacher\" if use_teacher else \"Predicted\")\n",
    "    return decisions\n",
    "\n",
    "# Test different ratios\n",
    "ratios = [0.0, 0.5, 1.0]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, ratio in enumerate(ratios):\n",
    "    decisions = simulate_teacher_forcing(ratio, 20)\n",
    "    teacher_count = decisions.count(\"Teacher\")\n",
    "    predicted_count = decisions.count(\"Predicted\")\n",
    "    \n",
    "    axes[i].bar([\"Teacher\", \"Predicted\"], [teacher_count, predicted_count])\n",
    "    axes[i].set_title(f\"Teacher Force Ratio: {ratio}\")\n",
    "    axes[i].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Teacher Forcing Benefits:\")\n",
    "print(\"- Ratio = 1.0: Always use correct words (fast training, but exposure bias)\")\n",
    "print(\"- Ratio = 0.0: Always use predictions (slow training, but realistic)\")\n",
    "print(\"- Ratio = 0.5: Balanced approach (commonly used)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Setup {#training}\n",
    "\n",
    "Now let's set up everything needed for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 5  # Reduced for demo\n",
    "learning_rate = 0.001\n",
    "batch_size = 32  # Reduced for demo\n",
    "\n",
    "# Model hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Vocabulary sizes\n",
    "if 'german' in globals() and 'english' in globals():\n",
    "    input_size_encoder = len(german.vocab)\n",
    "    input_size_decoder = len(english.vocab)\n",
    "    output_size = len(english.vocab)\n",
    "    print(f\"German vocab size: {input_size_encoder}\")\n",
    "    print(f\"English vocab size: {output_size}\")\n",
    "else:\n",
    "    # Fallback values for demo\n",
    "    input_size_encoder = 1000\n",
    "    input_size_decoder = 1000\n",
    "    output_size = 1000\n",
    "    print(\"Using fallback vocabulary sizes\")\n",
    "\n",
    "# Architecture parameters\n",
    "encoder_embedding_size = 256  # Reduced for demo\n",
    "decoder_embedding_size = 256\n",
    "hidden_size = 512  # Must be same for encoder and decoder\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"- Embedding size: {encoder_embedding_size}\")\n",
    "print(f\"- Hidden size: {hidden_size}\")\n",
    "print(f\"- Number of layers: {num_layers}\")\n",
    "print(f\"- Dropout: {enc_dropout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder\n",
    "encoder_net = Encoder(\n",
    "    input_size_encoder, \n",
    "    encoder_embedding_size, \n",
    "    hidden_size, \n",
    "    num_layers, \n",
    "    enc_dropout\n",
    ").to(device)\n",
    "\n",
    "# Create decoder\n",
    "decoder_net = Decoder(\n",
    "    input_size_decoder,\n",
    "    decoder_embedding_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    num_layers,\n",
    "    dec_dropout,\n",
    ").to(device)\n",
    "\n",
    "# Create complete model\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model created successfully!\")\n",
    "print(f\"Total trainable parameters: {count_parameters(model):,}\")\n",
    "print(f\"Encoder parameters: {count_parameters(encoder_net):,}\")\n",
    "print(f\"Decoder parameters: {count_parameters(decoder_net):,}\")\n",
    "\n",
    "# Setup optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Ignore padding tokens in loss calculation\n",
    "if 'english' in globals():\n",
    "    pad_idx = english.vocab.stoi[\"<pad>\"]\n",
    "else:\n",
    "    pad_idx = 1  # Fallback\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "print(f\"\\nTraining setup:\")\n",
    "print(f\"- Optimizer: Adam (lr={learning_rate})\")\n",
    "print(f\"- Loss function: CrossEntropyLoss\")\n",
    "print(f\"- Padding index ignored: {pad_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Iterators\n",
    "\n",
    "Create data loaders for efficient batch processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data iterators\n",
    "if 'train_data' in globals():\n",
    "    train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data),\n",
    "        batch_size=batch_size,\n",
    "        sort_within_batch=True,  # Sort by source length for efficiency\n",
    "        sort_key=lambda x: len(x.src),\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    print(f\"Data iterators created!\")\n",
    "    print(f\"Training batches: {len(train_iterator)}\")\n",
    "    print(f\"Validation batches: {len(valid_iterator)}\")\n",
    "    print(f\"Test batches: {len(test_iterator)}\")\n",
    "    \n",
    "    # Show a sample batch\n",
    "    sample_batch = next(iter(train_iterator))\n",
    "    print(f\"\\nSample batch:\")\n",
    "    print(f\"Source shape: {sample_batch.src.shape}\")\n",
    "    print(f\"Target shape: {sample_batch.trg.shape}\")\n",
    "    print(f\"Source (first sentence): {sample_batch.src[:, 0].tolist()}\")\n",
    "    print(f\"Target (first sentence): {sample_batch.trg[:, 0].tolist()}\")\n",
    "else:\n",
    "    print(\"Dataset not available - using dummy data for demonstration\")\n",
    "    train_iterator = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop {#loop}\n",
    "\n",
    "The main training loop with detailed explanations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions (simplified versions)\n",
    "def save_checkpoint(checkpoint, filename=\"checkpoint.pth.tar\"):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved to {filename}\")\n",
    "\n",
    "def translate_sentence(model, sentence, src_field, trg_field, device, max_length=50):\n",
    "    \"\"\"Translate a single sentence (simplified version)\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and convert to indices\n",
    "    tokens = src_field.preprocess(sentence)\n",
    "    tokens = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    \n",
    "    # Add batch dimension and convert to tensor\n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(1).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "        \n",
    "        # Start with <sos> token\n",
    "        outputs = [trg_field.vocab.stoi[\"<sos>\"]]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
    "            \n",
    "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
    "            best_guess = output.argmax(1).item()\n",
    "            \n",
    "            outputs.append(best_guess)\n",
    "            \n",
    "            # Stop if we predict <eos>\n",
    "            if best_guess == trg_field.vocab.stoi[\"<eos>\"]:\n",
    "                break\n",
    "    \n",
    "    # Convert indices back to words\n",
    "    translated_sentence = [trg_field.vocab.itos[idx] for idx in outputs[1:-1]]  # Remove <sos> and <eos>\n",
    "    return \" \".join(translated_sentence)\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "if train_iterator is not None:\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # Sample German sentence for translation testing\n",
    "    test_sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\"\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n[Epoch {epoch + 1}/{num_epochs}]\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict()\n",
    "        }\n",
    "        save_checkpoint(checkpoint, f\"checkpoint_epoch_{epoch+1}.pth.tar\")\n",
    "        \n",
    "        # Test translation\n",
    "        model.eval()\n",
    "        try:\n",
    "            translated = translate_sentence(\n",
    "                model, test_sentence, german, english, device, max_length=50\n",
    "            )\n",
    "            print(f\"Translation: {translated}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Translation error: {e}\")\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_iterator):\n",
    "            # Get data\n",
    "            inp_data = batch.src.to(device)  # Shape: (src_len, batch_size)\n",
    "            target = batch.trg.to(device)    # Shape: (trg_len, batch_size)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(inp_data, target)\n",
    "            # output shape: (trg_len, batch_size, vocab_size)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            # Remove first timestep (SOS token) and flatten\n",
    "            output = output[1:].reshape(-1, output.shape[2])\n",
    "            target = target[1:].reshape(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Print progress\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"  Batch {batch_idx}/{len(train_iterator)}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Break early for demo\n",
    "            if batch_idx >= 200:  # Process only first 200 batches for demo\n",
    "                break\n",
    "        \n",
    "        avg_loss = epoch_loss / min(len(train_iterator), 200)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', linewidth=2)\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Training skipped - dataset not available\")\n",
    "    print(\"In a real scenario, the model would train on German-English sentence pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process Explanation\n",
    "\n",
    "Let's break down what happens during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Process Breakdown:\")\n",
    "print(\"\\n1. Forward Pass:\")\n",
    "print(\"   - Encoder processes German sentence\")\n",
    "print(\"   - Decoder generates English translation word by word\")\n",
    "print(\"   - Teacher forcing used during training\")\n",
    "\n",
    "print(\"\\n2. Loss Calculation:\")\n",
    "print(\"   - Compare predicted words with actual target words\")\n",
    "print(\"   - CrossEntropyLoss measures prediction quality\")\n",
    "print(\"   - Padding tokens are ignored\")\n",
    "\n",
    "print(\"\\n3. Backward Pass:\")\n",
    "print(\"   - Calculate gradients using backpropagation\")\n",
    "print(\"   - Clip gradients to prevent exploding gradients\")\n",
    "print(\"   - Update model parameters using Adam optimizer\")\n",
    "\n",
    "print(\"\\n4. Key Techniques:\")\n",
    "print(\"   - Teacher forcing: helps training stability\")\n",
    "print(\"   - Gradient clipping: prevents exploding gradients\")\n",
    "print(\"   - Dropout: prevents overfitting\")\n",
    "print(\"   - Checkpointing: saves model progress\")\n",
    "\n",
    "# Visualize the training process\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Encoder-Decoder Flow\n",
    "ax1.text(0.1, 0.8, \"German:\\n'Ein Mann geht'\", fontsize=12, bbox=dict(boxstyle=\"round\", facecolor='lightblue'))\n",
    "ax1.arrow(0.3, 0.7, 0.2, 0, head_width=0.05, head_length=0.05, fc='black', ec='black')\n",
    "ax1.text(0.6, 0.8, \"Encoder\\n(LSTM)\", fontsize=12, bbox=dict(boxstyle=\"round\", facecolor='lightgreen'))\n",
    "ax1.arrow(0.8, 0.7, 0, -0.2, head_width=0.05, head_length=0.05, fc='black', ec='black')\n",
    "ax1.text(0.6, 0.4, \"Context\\n(hidden, cell)\", fontsize=12, bbox=dict(boxstyle=\"round\", facecolor='lightyellow'))\n",
    "ax1.arrow(0.5, 0.3, -0.2, 0, head_width=0.05, head_length=0.05, fc='black', ec='black')\n",
    "ax1.text(0.1, 0.2, \"Decoder\\n(LSTM)\", fontsize=12, bbox=dict(boxstyle=\"round\", facecolor='lightcoral'))\n",
    "ax1.arrow(0.3, 0.1, 0.2, 0, head_width=0.05, head_length=0.05, fc='black', ec='black')\n",
    "ax1.text(0.6, 0.05, \"English:\\n'A man goes'\", fontsize=12, bbox=dict(boxstyle=\"round\", facecolor='lightpink'))\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_title(\"Encoder-Decoder Architecture\")\n",
    "ax1.axis('off')\n",
    "\n",
    "# 2. Teacher Forcing\n",
    "steps = ['<SOS>', 'A', 'man', 'goes', '<EOS>']\n",
    "teacher_decisions = ['Teacher', 'Teacher', 'Predicted', 'Teacher', 'Predicted']\n",
    "colors = ['green' if d == 'Teacher' else 'red' for d in teacher_decisions]\n",
    "ax2.bar(range(len(steps)), [1]*len(steps), color=colors, alpha=0.7)\n",
    "ax2.set_xticks(range(len(steps)))\n",
    "ax2.set_xticklabels(steps)\n",
    "ax2.set_title(\"Teacher Forcing Example\")\n",
    "ax2.set_ylabel(\"Decision Type\")\n",
    "ax2.legend(['Teacher Forcing', 'Model Prediction'], loc='upper right')\n",
    "\n",
    "# 3. Loss Over Time (simulated)\n",
    "epochs = list(range(1, 11))\n",
    "loss_values = [2.5, 2.1, 1.8, 1.6, 1.4, 1.3, 1.2, 1.1, 1.0, 0.9]\n",
    "ax3.plot(epochs, loss_values, 'b-o', linewidth=2, markersize=6)\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.set_title('Training Loss Curve')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Model Components\n",
    "components = ['Embedding', 'LSTM Layers', 'Linear Layer', 'Dropout']\n",
    "encoder_params = [300*10000, 512*4*512*2, 0, 0]  # Approximate\n",
    "decoder_params = [300*10000, 512*4*512*2, 512*10000, 0]  # Approximate\n",
    "\n",
    "x = np.arange(len(components))\n",
    "width = 0.35\n",
    "\n",
    "ax4.bar(x - width/2, encoder_params, width, label='Encoder', alpha=0.8)\n",
    "ax4.bar(x + width/2, decoder_params, width, label='Decoder', alpha=0.8)\n",
    "ax4.set_xlabel('Components')\n",
    "ax4.set_ylabel('Parameters (approx)')\n",
    "ax4.set_title('Model Parameters by Component')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(components)\n",
    "ax4.legend()\n",
    "ax4.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation {#evaluation}\n",
    "\n",
    "Finally, let's evaluate the model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU Score Implementation (simplified)\n",
    "def calculate_bleu_score(reference, candidate):\n",
    "    \"\"\"Calculate BLEU score between reference and candidate sentences\"\"\"\n",
    "    # This is a simplified version - real BLEU is more complex\n",
    "    ref_words = reference.lower().split()\n",
    "    cand_words = candidate.lower().split()\n",
    "    \n",
    "    if len(cand_words) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate precision (simplified)\n",
    "    matches = sum(1 for word in cand_words if word in ref_words)\n",
    "    precision = matches / len(cand_words)\n",
    "    \n",
    "    # Length penalty (simplified)\n",
    "    length_penalty = min(1.0, len(cand_words) / len(ref_words))\n",
    "    \n",
    "    return precision * length_penalty\n",
    "\n",
    "# Test translation examples\n",
    "test_examples = [\n",
    "    {\n",
    "        \"german\": \"ein mann geht zur schule\",\n",
    "        \"english_ref\": \"a man goes to school\",\n",
    "        \"model_output\": \"a man goes to school\"  # Simulated perfect translation\n",
    "    },\n",
    "    {\n",
    "        \"german\": \"die katze ist schwarz\",\n",
    "        \"english_ref\": \"the cat is black\",\n",
    "        \"model_output\": \"the cat is dark\"  # Simulated imperfect translation\n",
    "    },\n",
    "    {\n",
    "        \"german\": \"ich liebe musik\",\n",
    "        \"english_ref\": \"i love music\",\n",
    "        \"model_output\": \"i like music\"  # Simulated close translation\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Translation Examples and BLEU Scores:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_bleu = 0\n",
    "for i, example in enumerate(test_examples, 1):\n",
    "    bleu_score = calculate_bleu_score(example[\"english_ref\"], example[\"model_output\"])\n",
    "    total_bleu += bleu_score\n",
    "    \n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"German:     {example['german']}\")\n",
    "    print(f\"Reference:  {example['english_ref']}\")\n",
    "    print(f\"Model:      {example['model_output']}\")\n",
    "    print(f\"BLEU Score: {bleu_score:.3f}\")\n",
    "\n",
    "avg_bleu = total_bleu / len(test_examples)\n",
    "print(f\"\\nAverage BLEU Score: {avg_bleu:.3f}\")\n",
    "print(f\"Average BLEU Score (%): {avg_bleu * 100:.1f}%\")\n",
    "\n",
    "# Visualize BLEU scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "examples_labels = [f\"Example {i+1}\" for i in range(len(test_examples))]\n",
    "bleu_scores = [calculate_bleu_score(ex[\"english_ref\"], ex[\"model_output\"]) for ex in test_examples]\n",
    "\n",
    "bars = plt.bar(examples_labels, bleu_scores, color=['green', 'orange', 'blue'], alpha=0.7)\n",
    "plt.axhline(y=avg_bleu, color='red', linestyle='--', label=f'Average: {avg_bleu:.3f}')\n",
    "plt.ylabel('BLEU Score')\n",
    "plt.title('BLEU Scores for Translation Examples')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, bleu_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Performance Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. BLEU Score Interpretation:\")\n",
    "print(\"   - 0.0-0.1: Very poor translation\")\n",
    "print(\"   - 0.1-0.3: Poor translation\")\n",
    "print(\"   - 0.3-0.5: Reasonable translation\")\n",
    "print(\"   - 0.5-0.7: Good translation\")\n",
    "print(\"   - 0.7-1.0: Excellent translation\")\n",
    "\n",
    "print(\"\\n2. Common Issues in Seq2Seq Models:\")\n",
    "print(\"   - Exposure bias: difference between training and inference\")\n",
    "print(\"   - Long sequence problems: information bottleneck\")\n",
    "print(\"   - Out-of-vocabulary words: unknown token handling\")\n",
    "print(\"   - Repetition: model may repeat phrases\")\n",
    "\n",
    "print(\"\\n3. Improvements and Extensions:\")\n",
    "print(\"   - Attention mechanism: focus on relevant input parts\")\n",
    "print(\"   - Beam search: better decoding strategy\")\n",
    "print(\"   - Transformer architecture: parallel processing\")\n",
    "print(\"   - Pre-trained models: transfer learning\")\n",
    "\n",
    "print(\"\\n4. Real-world Considerations:\")\n",
    "print(\"   - Larger datasets: millions of sentence pairs\")\n",
    "print(\"   - Longer training: hundreds of epochs\")\n",
    "print(\"   - GPU clusters: distributed training\")\n",
    "print(\"   - Evaluation metrics: BLEU, METEOR, ROUGE\")\n",
    "\n",
    "# Create a comparison chart of different architectures\n",
    "architectures = ['Basic Seq2Seq', 'Seq2Seq + Attention', 'Transformer', 'Pre-trained (mT5)']\n",
    "bleu_scores = [0.15, 0.35, 0.45, 0.65]  # Typical BLEU scores\n",
    "training_time = [1, 2, 3, 0.5]  # Relative training time\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# BLEU scores comparison\n",
    "bars1 = ax1.bar(architectures, bleu_scores, color=['red', 'orange', 'green', 'blue'], alpha=0.7)\n",
    "ax1.set_ylabel('BLEU Score')\n",
    "ax1.set_title('Translation Quality by Architecture')\n",
    "ax1.set_ylim(0, 0.7)\n",
    "plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "for bar, score in zip(bars1, bleu_scores):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{score:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Training time comparison\n",
    "bars2 = ax2.bar(architectures, training_time, color=['red', 'orange', 'green', 'blue'], alpha=0.7)\n",
    "ax2.set_ylabel('Relative Training Time')\n",
    "ax2.set_title('Training Efficiency by Architecture')\n",
    "plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "for bar, time in zip(bars2, training_time):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05, \n",
    "             f'{time}x', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered a complete implementation of a Seq2Seq model for neural machine translation:\n",
    "\n",
    "### Key Components:\n",
    "1. **Encoder**: Processes input sequence and creates context representation\n",
    "2. **Decoder**: Generates output sequence word by word using context\n",
    "3. **Teacher Forcing**: Training technique for stability\n",
    "4. **LSTM Networks**: Handle sequential data and maintain memory\n",
    "\n",
    "### Training Process:\n",
    "1. **Data Preprocessing**: Tokenization, vocabulary building, batching\n",
    "2. **Forward Pass**: Encoder → Context → Decoder → Predictions\n",
    "3. **Loss Calculation**: CrossEntropyLoss with padding ignored\n",
    "4. **Optimization**: Adam optimizer with gradient clipping\n",
    "\n",
    "### Evaluation:\n",
    "1. **BLEU Score**: Measures translation quality\n",
    "2. **Qualitative Analysis**: Manual inspection of translations\n",
    "3. **Performance Comparison**: Different architectures\n",
    "\n",
    "### Next Steps:\n",
    "- Implement attention mechanism for better performance\n",
    "- Try beam search for better decoding\n",
    "- Experiment with Transformer architecture\n",
    "- Use pre-trained models for transfer learning\n",
    "\n",
    "The basic Seq2Seq model provides a solid foundation for understanding neural machine translation, though modern approaches like Transformers have largely superseded this architecture in practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}