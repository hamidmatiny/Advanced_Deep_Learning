{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence (Seq2Seq) Model for Neural Machine Translation\n",
    "\n",
    "This notebook explains a complete implementation of a Seq2Seq model for translating German to English using PyTorch. The model uses an Encoder-Decoder architecture with LSTM networks.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Imports and Setup](#imports)\n",
    "2. [Data Preprocessing](#data)\n",
    "3. [Encoder Architecture](#encoder)\n",
    "4. [Decoder Architecture](#decoder)\n",
    "5. [Seq2Seq Model](#seq2seq)\n",
    "6. [Training Setup](#training)\n",
    "7. [Training Loop](#loop)\n",
    "8. [Evaluation](#evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup {#imports}\n",
    "\n",
    "First, let's import all necessary libraries with proper error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try importing torchtext and spacy with error handling\n",
    "try:\n",
    "    from torchtext.datasets import Multi30k\n",
    "    from torchtext.data import Field, BucketIterator\n",
    "    TORCHTEXT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"TorchText not available - using mock data\")\n",
    "    TORCHTEXT_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    SPACY_AVAILABLE = True\n",
    "except (ImportError, OSError) as e:\n",
    "    print(f\"SpaCy not available: {e}\")\n",
    "    SPACY_AVAILABLE = False\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"TorchText available: {TORCHTEXT_AVAILABLE}\")\n",
    "print(f\"SpaCy available: {SPACY_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Language Models\n",
    "\n",
    "We use spaCy for tokenization when available, otherwise fall back to simple splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy language models for German and English\n",
    "spacy_ger = None\n",
    "spacy_eng = None\n",
    "\n",
    "if SPACY_AVAILABLE:\n",
    "    try:\n",
    "        spacy_ger = spacy.load(\"de_core_news_sm\")\n",
    "        spacy_eng = spacy.load(\"en_core_news_sm\")\n",
    "        print(\"SpaCy models loaded successfully!\")\n",
    "    except (OSError, ImportError) as e:\n",
    "        print(f\"SpaCy models not available: {e}\")\n",
    "        print(\"Install with: python -m spacy download de_core_news_sm en_core_news_sm\")\n",
    "        spacy_ger = None\n",
    "        spacy_eng = None\n",
    "else:\n",
    "    print(\"Using simple tokenization (spaCy not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Functions\n",
    "\n",
    "Tokenization converts sentences into lists of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ger(text):\n",
    "    \"\"\"Tokenize German text into individual words\"\"\"\n",
    "    if spacy_ger:\n",
    "        return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
    "    else:\n",
    "        return text.split()  # Simple fallback\n",
    "\n",
    "def tokenize_eng(text):\n",
    "    \"\"\"Tokenize English text into individual words\"\"\"\n",
    "    if spacy_eng:\n",
    "        return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
    "    else:\n",
    "        return text.split()  # Simple fallback\n",
    "\n",
    "# Test tokenization\n",
    "german_sentence = \"Ein Mann geht zur Schule.\"\n",
    "english_sentence = \"A man goes to school.\"\n",
    "\n",
    "print(f\"German: {german_sentence}\")\n",
    "print(f\"Tokenized: {tokenize_ger(german_sentence)}\")\n",
    "print(f\"\\nEnglish: {english_sentence}\")\n",
    "print(f\"Tokenized: {tokenize_eng(english_sentence)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing {#data}\n",
    "\n",
    "### Field Definition\n",
    "\n",
    "Fields define how to process the text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fields for German and English (if TorchText is available)\n",
    "if TORCHTEXT_AVAILABLE:\n",
    "    german = Field(\n",
    "        tokenize=tokenize_ger,  # How to split text into tokens\n",
    "        lower=True,            # Convert to lowercase\n",
    "        init_token=\"<sos>\",     # Start of sequence token\n",
    "        eos_token=\"<eos>\"       # End of sequence token\n",
    "    )\n",
    "    \n",
    "    english = Field(\n",
    "        tokenize=tokenize_eng,\n",
    "        lower=True,\n",
    "        init_token=\"<sos>\",\n",
    "        eos_token=\"<eos>\"\n",
    "    )\n",
    "    \n",
    "    print(\"Fields created successfully!\")\n",
    "    print(\"Special tokens:\")\n",
    "    print(f\"- Start of sequence: {german.init_token}\")\n",
    "    print(f\"- End of sequence: {german.eos_token}\")\n",
    "else:\n",
    "    print(\"TorchText not available - using mock vocabulary\")\n",
    "    # Create mock vocabulary for demonstration\n",
    "    class MockVocab:\n",
    "        def __init__(self):\n",
    "            self.stoi = {'<unk>': 0, '<pad>': 1, '<sos>': 2, '<eos>': 3}\n",
    "            self.itos = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "            # Add some common words\n",
    "            words = ['the', 'a', 'man', 'woman', 'goes', 'to', 'school', 'house']\n",
    "            for word in words:\n",
    "                self.stoi[word] = len(self.itos)\n",
    "                self.itos.append(word)\n",
    "        def __len__(self):\n",
    "            return len(self.itos)\n",
    "    \n",
    "    class MockField:\n",
    "        def __init__(self):\n",
    "            self.vocab = MockVocab()\n",
    "            self.init_token = '<sos>'\n",
    "            self.eos_token = '<eos>'\n",
    "    \n",
    "    german = MockField()\n",
    "    english = MockField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading\n",
    "\n",
    "Multi30k is a multilingual dataset with ~30k sentence pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Multi30k dataset (if available)\n",
    "if TORCHTEXT_AVAILABLE:\n",
    "    try:\n",
    "        train_data, valid_data, test_data = Multi30k.splits(\n",
    "            exts=(\".de\", \".en\"),           # File extensions for German and English\n",
    "            fields=(german, english)       # Fields to use for processing\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset loaded successfully!\")\n",
    "        print(f\"Training examples: {len(train_data)}\")\n",
    "        print(f\"Validation examples: {len(valid_data)}\")\n",
    "        print(f\"Test examples: {len(test_data)}\")\n",
    "        \n",
    "        # Show a sample\n",
    "        print(\"\\nSample training example:\")\n",
    "        print(f\"German: {' '.join(train_data[0].src)}\")\n",
    "        print(f\"English: {' '.join(train_data[0].trg)}\")\n",
    "        DATASET_AVAILABLE = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        print(\"Using mock data for demonstration\")\n",
    "        DATASET_AVAILABLE = False\n",
    "else:\n",
    "    print(\"TorchText not available - using mock data\")\n",
    "    DATASET_AVAILABLE = False\n",
    "\n",
    "if not DATASET_AVAILABLE:\n",
    "    print(\"\\nMock dataset examples:\")\n",
    "    print(\"German: ein mann geht zur schule\")\n",
    "    print(\"English: a man goes to school\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Building\n",
    "\n",
    "Build vocabularies from the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabularies (if dataset is available)\n",
    "if DATASET_AVAILABLE:\n",
    "    german.build_vocab(\n",
    "        train_data,\n",
    "        max_size=10000,  # Maximum vocabulary size\n",
    "        min_freq=2       # Minimum frequency for a word to be included\n",
    "    )\n",
    "    \n",
    "    english.build_vocab(\n",
    "        train_data,\n",
    "        max_size=10000,\n",
    "        min_freq=2\n",
    "    )\n",
    "    \n",
    "    print(f\"German vocabulary size: {len(german.vocab)}\")\n",
    "    print(f\"English vocabulary size: {len(english.vocab)}\")\n",
    "    \n",
    "    # Show special tokens\n",
    "    print(\"\\nSpecial tokens in vocabulary:\")\n",
    "    print(f\"Unknown token: {german.vocab.itos[0]}\")\n",
    "    print(f\"Padding token: {german.vocab.itos[1]}\")\n",
    "    print(f\"Start token: {german.vocab.itos[2]}\")\n",
    "    print(f\"End token: {german.vocab.itos[3]}\")\n",
    "    \n",
    "    # Show most common words\n",
    "    print(\"\\nMost common German words:\")\n",
    "    for i in range(4, min(14, len(german.vocab.itos))):\n",
    "        print(f\"{i-3}. {german.vocab.itos[i]}\")\n",
    "else:\n",
    "    print(f\"German vocabulary size: {len(german.vocab)}\")\n",
    "    print(f\"English vocabulary size: {len(english.vocab)}\")\n",
    "    \n",
    "    print(\"\\nSpecial tokens in mock vocabulary:\")\n",
    "    for i, token in enumerate(german.vocab.itos):\n",
    "        print(f\"{i}. {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoder Architecture {#encoder}\n",
    "\n",
    "The encoder processes the input sequence and creates a context representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Store parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Layers\n",
    "        self.dropout = nn.Dropout(p)  # Regularization\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)  # Word embeddings\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, batch_size)\n",
    "        \n",
    "        # Convert word indices to embeddings\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, batch_size, embedding_size)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape: (seq_length, batch_size, hidden_size)\n",
    "        # hidden shape: (num_layers, batch_size, hidden_size)\n",
    "        # cell shape: (num_layers, batch_size, hidden_size)\n",
    "        \n",
    "        # Return final hidden and cell states (context)\n",
    "        return hidden, cell\n",
    "\n",
    "print(\"Encoder class defined!\")\n",
    "print(\"\\nEncoder Architecture:\")\n",
    "print(\"1. Embedding layer: converts word indices to dense vectors\")\n",
    "print(\"2. LSTM layers: process sequence and maintain memory\")\n",
    "print(\"3. Output: final hidden and cell states as context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decoder Architecture {#decoder}\n",
    "\n",
    "The decoder generates the output sequence one word at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Store parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Layers\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)  # Output projection\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape: (batch_size) - single word input\n",
    "        # We need to add sequence dimension\n",
    "        x = x.unsqueeze(0)  # Shape: (1, batch_size)\n",
    "        \n",
    "        # Convert to embeddings\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, batch_size, embedding_size)\n",
    "        \n",
    "        # Pass through LSTM with previous context\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # outputs shape: (1, batch_size, hidden_size)\n",
    "        \n",
    "        # Project to vocabulary size\n",
    "        predictions = self.fc(outputs)\n",
    "        # predictions shape: (1, batch_size, vocab_size)\n",
    "        \n",
    "        # Remove sequence dimension\n",
    "        predictions = predictions.squeeze(0)\n",
    "        # predictions shape: (batch_size, vocab_size)\n",
    "        \n",
    "        return predictions, hidden, cell\n",
    "\n",
    "print(\"Decoder class defined!\")\n",
    "print(\"\\nDecoder Architecture:\")\n",
    "print(\"1. Embedding layer: converts word indices to dense vectors\")\n",
    "print(\"2. LSTM layers: generate next word using context\")\n",
    "print(\"3. Linear layer: project to vocabulary probabilities\")\n",
    "print(\"4. Process one word at a time during generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Seq2Seq Model {#seq2seq}\n",
    "\n",
    "The complete model combines encoder and decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        # source shape: (src_len, batch_size)\n",
    "        # target shape: (trg_len, batch_size)\n",
    "        \n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(english.vocab)\n",
    "        \n",
    "        # Store decoder outputs\n",
    "        device = source.device\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        \n",
    "        # Encode the source sequence\n",
    "        hidden, cell = self.encoder(source)\n",
    "        \n",
    "        # First input to decoder is <SOS> token\n",
    "        x = target[0]  # Shape: (batch_size)\n",
    "        \n",
    "        # Generate target sequence\n",
    "        for t in range(1, target_len):\n",
    "            # Get prediction for current timestep\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            \n",
    "            # Store prediction\n",
    "            outputs[t] = output\n",
    "            \n",
    "            # Get best predicted word\n",
    "            best_guess = output.argmax(1)\n",
    "            \n",
    "            # Teacher forcing: use actual target word with probability teacher_force_ratio\n",
    "            # Otherwise use predicted word\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "print(\"Seq2Seq class defined!\")\n",
    "print(\"\\nSeq2Seq Architecture:\")\n",
    "print(\"1. Encoder processes entire source sequence\")\n",
    "print(\"2. Decoder generates target sequence word by word\")\n",
    "print(\"3. Teacher forcing helps training stability\")\n",
    "print(\"4. Context flows from encoder to decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Setup {#training}\n",
    "\n",
    "Now let's set up everything needed for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 3  # Reduced for demo\n",
    "learning_rate = 0.001\n",
    "batch_size = 32  # Reduced for demo\n",
    "\n",
    "# Model hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Vocabulary sizes\n",
    "input_size_encoder = len(german.vocab)\n",
    "input_size_decoder = len(english.vocab)\n",
    "output_size = len(english.vocab)\n",
    "print(f\"German vocab size: {input_size_encoder}\")\n",
    "print(f\"English vocab size: {output_size}\")\n",
    "\n",
    "# Architecture parameters\n",
    "encoder_embedding_size = 256  # Reduced for demo\n",
    "decoder_embedding_size = 256\n",
    "hidden_size = 512  # Must be same for encoder and decoder\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"- Embedding size: {encoder_embedding_size}\")\n",
    "print(f\"- Hidden size: {hidden_size}\")\n",
    "print(f\"- Number of layers: {num_layers}\")\n",
    "print(f\"- Dropout: {enc_dropout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder\n",
    "encoder_net = Encoder(\n",
    "    input_size_encoder, \n",
    "    encoder_embedding_size, \n",
    "    hidden_size, \n",
    "    num_layers, \n",
    "    enc_dropout\n",
    ").to(device)\n",
    "\n",
    "# Create decoder\n",
    "decoder_net = Decoder(\n",
    "    input_size_decoder,\n",
    "    decoder_embedding_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    num_layers,\n",
    "    dec_dropout,\n",
    ").to(device)\n",
    "\n",
    "# Create complete model\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model created successfully!\")\n",
    "print(f\"Total trainable parameters: {count_parameters(model):,}\")\n",
    "print(f\"Encoder parameters: {count_parameters(encoder_net):,}\")\n",
    "print(f\"Decoder parameters: {count_parameters(decoder_net):,}\")\n",
    "\n",
    "# Setup optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Ignore padding tokens in loss calculation\n",
    "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "print(f\"\\nTraining setup:\")\n",
    "print(f\"- Optimizer: Adam (lr={learning_rate})\")\n",
    "print(f\"- Loss function: CrossEntropyLoss\")\n",
    "print(f\"- Padding index ignored: {pad_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop {#loop}\n",
    "\n",
    "The main training loop with detailed explanations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions (simplified versions)\n",
    "def save_checkpoint(checkpoint, filename=\"checkpoint.pth.tar\"):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved to {filename}\")\n",
    "\n",
    "print(\"Helper functions defined!\")\n",
    "\n",
    "# Training demonstration with dummy data\n",
    "print(\"Training demonstration with dummy data:\")\n",
    "\n",
    "# Simulate training with dummy data\n",
    "model.train()\n",
    "dummy_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n[Demo Epoch {epoch + 1}/{num_epochs}]\")\n",
    "    \n",
    "    # Create dummy batch\n",
    "    dummy_src = torch.randint(0, len(german.vocab), (10, 4)).to(device)\n",
    "    dummy_trg = torch.randint(0, len(english.vocab), (8, 4)).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(dummy_src, dummy_trg)\n",
    "    \n",
    "    # Calculate dummy loss\n",
    "    output_flat = output[1:].reshape(-1, output.shape[2])\n",
    "    target_flat = dummy_trg[1:].reshape(-1)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(output_flat, target_flat)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    dummy_losses.append(loss.item())\n",
    "    print(f\"  Demo Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Plot demo loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(dummy_losses) + 1), dummy_losses, 'r-o', linewidth=2)\n",
    "plt.title('Demo Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: In a real scenario, the model would train on German-English sentence pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process Explanation\n",
    "\n",
    "Let's break down what happens during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Process Breakdown:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. Forward Pass:\")\n",
    "print(\"   - Encoder processes German sentence\")\n",
    "print(\"   - Decoder generates English translation word by word\")\n",
    "print(\"   - Teacher forcing used during training\")\n",
    "\n",
    "print(\"\\n2. Loss Calculation:\")\n",
    "print(\"   - Compare predicted words with actual target words\")\n",
    "print(\"   - CrossEntropyLoss measures prediction quality\")\n",
    "print(\"   - Padding tokens are ignored\")\n",
    "\n",
    "print(\"\\n3. Backward Pass:\")\n",
    "print(\"   - Calculate gradients using backpropagation\")\n",
    "print(\"   - Clip gradients to prevent exploding gradients\")\n",
    "print(\"   - Update model parameters using Adam optimizer\")\n",
    "\n",
    "print(\"\\n4. Key Techniques:\")\n",
    "print(\"   - Teacher forcing: helps training stability\")\n",
    "print(\"   - Gradient clipping: prevents exploding gradients\")\n",
    "print(\"   - Dropout: prevents overfitting\")\n",
    "print(\"   - Checkpointing: saves model progress\")\n",
    "\n",
    "# Visualize the training process\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Encoder-Decoder Flow\n",
    "ax1.text(0.1, 0.8, \"German:\\n'Ein Mann geht'\", fontsize=12, bbox=dict(boxstyle=\"round\", facecolor='lightblue'))\n",
    "ax1.arrow(0.3, 0.7, 0.2, 0, head_width=0.05, head_length=0.05, fc='black', ec='black')\n",
    "ax1.text(0.6, 0.8, \"Encoder\\n(LSTM)\", fontsize=12, bbox=dict(boxstyle=\"round\", facecolor='lightgreen'))\n",
    "ax1.arrow(0.8, 0.7, 0, -0.2, head_width=0.05, head_length=0.05, fc='black', ec='black')\n",
    "ax1.text(0.6, 0.4, \"Context\\n(hidden, cell)\", fontsize=12, bbox=dict(boxstyle=\"round\", facecolor='lightyellow'))\n",
    "ax1.arrow(0.5, 0.3, -0.2, 0, head_width=0.05, head_length=0.05, fc='black', ec='black')\n",
    "ax1.text(0.1, 0.2, \"Decoder\\n(LSTM)\", fontsize=12, bbox=dict(boxstyle=\"round\", facecolor='lightcoral'))\n",
    "ax1.arrow(0.3, 0.1, 0.2, 0, head_width=0.05, head_length=0.05, fc='black', ec='black')\n",
    "ax1.text(0.6, 0.05, \"English:\\n'A man goes'\", fontsize=12, bbox=dict(boxstyle=\"round\", facecolor='lightpink'))\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_title(\"Encoder-Decoder Architecture\")\n",
    "ax1.axis('off')\n",
    "\n",
    "# 2. Teacher Forcing\n",
    "steps = ['<SOS>', 'A', 'man', 'goes', '<EOS>']\n",
    "teacher_decisions = ['Teacher', 'Teacher', 'Predicted', 'Teacher', 'Predicted']\n",
    "colors = ['green' if d == 'Teacher' else 'red' for d in teacher_decisions]\n",
    "ax2.bar(range(len(steps)), [1]*len(steps), color=colors, alpha=0.7)\n",
    "ax2.set_xticks(range(len(steps)))\n",
    "ax2.set_xticklabels(steps)\n",
    "ax2.set_title(\"Teacher Forcing Example\")\n",
    "ax2.set_ylabel(\"Decision Type\")\n",
    "ax2.legend(['Teacher Forcing', 'Model Prediction'], loc='upper right')\n",
    "\n",
    "# 3. Loss Over Time (simulated)\n",
    "epochs = list(range(1, 11))\n",
    "loss_values = [2.5, 2.1, 1.8, 1.6, 1.4, 1.3, 1.2, 1.1, 1.0, 0.9]\n",
    "ax3.plot(epochs, loss_values, 'b-o', linewidth=2, markersize=6)\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.set_title('Training Loss Curve')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Model Components\n",
    "components = ['Embedding', 'LSTM Layers', 'Linear Layer']\n",
    "encoder_params = [256*len(german.vocab), 512*4*512*2, 0]  # Approximate\n",
    "decoder_params = [256*len(english.vocab), 512*4*512*2, 512*len(english.vocab)]  # Approximate\n",
    "\n",
    "x = np.arange(len(components))\n",
    "width = 0.35\n",
    "\n",
    "ax4.bar(x - width/2, encoder_params, width, label='Encoder', alpha=0.8)\n",
    "ax4.bar(x + width/2, decoder_params, width, label='Decoder', alpha=0.8)\n",
    "ax4.set_xlabel('Components')\n",
    "ax4.set_ylabel('Parameters (approx)')\n",
    "ax4.set_title('Model Parameters by Component')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(components)\n",
    "ax4.legend()\n",
    "ax4.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation {#evaluation}\n",
    "\n",
    "Finally, let's evaluate the model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU Score Implementation (simplified)\n",
    "def calculate_bleu_score(reference, candidate):\n",
    "    \"\"\"Calculate BLEU score between reference and candidate sentences\"\"\"\n",
    "    # This is a simplified version - real BLEU is more complex\n",
    "    ref_words = reference.lower().split()\n",
    "    cand_words = candidate.lower().split()\n",
    "    \n",
    "    if len(cand_words) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate precision (simplified)\n",
    "    matches = sum(1 for word in cand_words if word in ref_words)\n",
    "    precision = matches / len(cand_words)\n",
    "    \n",
    "    # Length penalty (simplified)\n",
    "    length_penalty = min(1.0, len(cand_words) / len(ref_words))\n",
    "    \n",
    "    return precision * length_penalty\n",
    "\n",
    "# Test translation examples\n",
    "test_examples = [\n",
    "    {\n",
    "        \"german\": \"ein mann geht zur schule\",\n",
    "        \"english_ref\": \"a man goes to school\",\n",
    "        \"model_output\": \"a man goes to school\"  # Simulated perfect translation\n",
    "    },\n",
    "    {\n",
    "        \"german\": \"die katze ist schwarz\",\n",
    "        \"english_ref\": \"the cat is black\",\n",
    "        \"model_output\": \"the cat is dark\"  # Simulated imperfect translation\n",
    "    },\n",
    "    {\n",
    "        \"german\": \"ich liebe musik\",\n",
    "        \"english_ref\": \"i love music\",\n",
    "        \"model_output\": \"i like music\"  # Simulated close translation\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Translation Examples and BLEU Scores:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_bleu = 0\n",
    "for i, example in enumerate(test_examples, 1):\n",
    "    bleu_score = calculate_bleu_score(example[\"english_ref\"], example[\"model_output\"])\n",
    "    total_bleu += bleu_score\n",
    "    \n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"German:     {example['german']}\")\n",
    "    print(f\"Reference:  {example['english_ref']}\")\n",
    "    print(f\"Model:      {example['model_output']}\")\n",
    "    print(f\"BLEU Score: {bleu_score:.3f}\")\n",
    "\n",
    "avg_bleu = total_bleu / len(test_examples)\n",
    "print(f\"\\nAverage BLEU Score: {avg_bleu:.3f}\")\n",
    "print(f\"Average BLEU Score (%): {avg_bleu * 100:.1f}%\")\n",
    "\n",
    "# Visualize BLEU scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "examples_labels = [f\"Example {i+1}\" for i in range(len(test_examples))]\n",
    "bleu_scores = [calculate_bleu_score(ex[\"english_ref\"], ex[\"model_output\"]) for ex in test_examples]\n",
    "\n",
    "bars = plt.bar(examples_labels, bleu_scores, color=['green', 'orange', 'blue'], alpha=0.7)\n",
    "plt.axhline(y=avg_bleu, color='red', linestyle='--', label=f'Average: {avg_bleu:.3f}')\n",
    "plt.ylabel('BLEU Score')\n",
    "plt.title('BLEU Scores for Translation Examples')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, bleu_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered a complete implementation of a Seq2Seq model for neural machine translation:\n",
    "\n",
    "### Key Components:\n",
    "1. **Encoder**: Processes input sequence and creates context representation\n",
    "2. **Decoder**: Generates output sequence word by word using context\n",
    "3. **Teacher Forcing**: Training technique for stability\n",
    "4. **LSTM Networks**: Handle sequential data and maintain memory\n",
    "\n",
    "### Training Process:\n",
    "1. **Data Preprocessing**: Tokenization, vocabulary building, batching\n",
    "2. **Forward Pass**: Encoder → Context → Decoder → Predictions\n",
    "3. **Loss Calculation**: CrossEntropyLoss with padding ignored\n",
    "4. **Optimization**: Adam optimizer with gradient clipping\n",
    "\n",
    "### Evaluation:\n",
    "1. **BLEU Score**: Measures translation quality\n",
    "2. **Qualitative Analysis**: Manual inspection of translations\n",
    "3. **Performance Comparison**: Different architectures\n",
    "\n",
    "### Next Steps:\n",
    "- Implement attention mechanism for better performance\n",
    "- Try beam search for better decoding\n",
    "- Experiment with Transformer architecture\n",
    "- Use pre-trained models for transfer learning\n",
    "\n",
    "The basic Seq2Seq model provides a solid foundation for understanding neural machine translation, though modern approaches like Transformers have largely superseded this architecture in practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}