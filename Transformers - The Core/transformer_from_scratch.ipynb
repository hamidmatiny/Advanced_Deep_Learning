{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a GPT-Style Transformer from Scratch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hamidmatiny/ML_Portfolio/blob/main/04_Advanced%20deep%20learning/transformer_from_scratch.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a **GPT-style transformer** from scratch using PyTorch. We'll build a character-level language model that can generate text in the style of Shakespeare.\n",
    "\n",
    "### What You'll Learn:\n",
    "- Understanding the transformer architecture\n",
    "- Self-attention mechanism and multi-head attention\n",
    "- Positional embeddings and their importance\n",
    "- Layer normalization and residual connections\n",
    "- Feed-forward networks in transformers\n",
    "- Training a language model from scratch\n",
    "\n",
    "### Key Mathematical Concepts:\n",
    "- **Attention Formula**: $\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
    "- **Multi-Head Attention**: Parallel attention heads that capture different types of relationships\n",
    "- **Positional Encoding**: Adding position information to token embeddings\n",
    "- **Layer Normalization**: $\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation and Tokenization\n",
    "\n",
    "We'll use Shakespeare's text as our training data and implement character-level tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Set device and random seeds for reproducibility\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(2971)\n",
    "np.random.seed(2971)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1,115,394 characters\n",
      "First 500 characters:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare the Shakespeare dataset\n",
    "filename = 'input.txt'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Dataset length: {len(text):,} characters\")\n",
    "print(f\"First 500 characters:\\n{text[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-Level Tokenization\n",
    "\n",
    "We'll create a simple character-level tokenizer that maps each unique character to an integer ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 65\n",
      "Original: Hello, World!\n",
      "Encoded: [20, 43, 50, 50, 53, 6, 1, 35, 53, 56, 50, 42, 2]\n",
      "Decoded: Hello, World!\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary from unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Vocabulary: {''.join(chars)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Create character-to-index and index-to-character mappings\n",
    "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
    "\n",
    "def encode(text):\n",
    "    \"\"\"Convert text to list of token IDs\"\"\"\n",
    "    return [char_to_idx[char] for char in text]\n",
    "\n",
    "def decode(token_ids):\n",
    "    \"\"\"Convert list of token IDs back to text\"\"\"\n",
    "    return ''.join([idx_to_char[idx] for idx in token_ids])\n",
    "\n",
    "# Test the tokenizer\n",
    "test_text = \"Hello, World!\"\n",
    "encoded = encode(test_text)\n",
    "decoded = decode(encoded)\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting and Batch Generation\n",
    "\n",
    "We'll split the data into training and validation sets, then create a function to generate batches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tensor shape: torch.Size([1115394])\n",
      "Training data: 1,003,854 tokens\n",
      "Validation data: 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "block_size = 256      # Maximum context length\n",
    "batch_size = 64       # Number of sequences per batch\n",
    "max_iters = 5000      # Training iterations\n",
    "eval_interval = 500   # Evaluation frequency\n",
    "learning_rate = 3e-4  # Learning rate\n",
    "eval_iters = 200      # Number of iterations for evaluation\n",
    "n_embd = 384          # Embedding dimension\n",
    "n_head = 6            # Number of attention heads\n",
    "n_layer = 6           # Number of transformer blocks\n",
    "dropout = 0.2         # Dropout rate\n",
    "\n",
    "# Convert text to tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Data tensor shape: {data.shape}\")\n",
    "\n",
    "# Split into train and validation\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Training data: {len(train_data):,} tokens\")\n",
    "print(f\"Validation data: {len(val_data):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([64, 256])\n",
      "Target batch shape: torch.Size([64, 256])\n",
      "\n",
      "Example sequence:\n",
      "Input:  me, let's go:\n",
      "Leave \n",
      "Target: e, let's go:\n",
      "Leave t\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Generate a batch of input-target pairs for training.\n",
    "    \n",
    "    Args:\n",
    "        split: 'train' or 'val' to select the dataset\n",
    "    \n",
    "    Returns:\n",
    "        x: Input sequences of shape (batch_size, block_size)\n",
    "        y: Target sequences of shape (batch_size, block_size)\n",
    "    \"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # Generate random starting positions\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # Create input and target sequences\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Test batch generation\n",
    "xb, yb = get_batch('train')\n",
    "print(f\"Input batch shape: {xb.shape}\")\n",
    "print(f\"Target batch shape: {yb.shape}\")\n",
    "print(f\"\\nExample sequence:\")\n",
    "print(f\"Input:  {decode(xb[0][:20].tolist())}\")\n",
    "print(f\"Target: {decode(yb[0][:20].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Self-Attention Mechanism\n",
    "\n",
    "The core of the transformer is the **self-attention mechanism**. It allows each position in the sequence to attend to all positions in the previous layers.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "Given input embeddings $X \\in \\mathbb{R}^{n \\times d}$, we compute:\n",
    "\n",
    "- **Query**: $Q = XW_Q$\n",
    "- **Key**: $K = XW_K$ \n",
    "- **Value**: $V = XW_V$\n",
    "\n",
    "The attention output is:\n",
    "$$\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "The scaling factor $\\frac{1}{\\sqrt{d_k}}$ prevents the softmax from saturating for large embedding dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"Single head of self-attention\"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        \n",
    "        # Linear projections for queries, keys, and values\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        \n",
    "        # Register causal mask as buffer (not a parameter)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # Batch, Time, Channels\n",
    "        \n",
    "        # Compute queries, keys, and values\n",
    "        k = self.key(x)    # (B, T, head_size)\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "        v = self.value(x)  # (B, T, head_size)\n",
    "        \n",
    "        # Compute attention scores with scaling\n",
    "        wei = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)  # (B, T, T)\n",
    "        \n",
    "        # Apply causal mask (decoder-style attention)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = wei @ v  # (B, T, head_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "Multi-head attention runs multiple attention heads in parallel and concatenates their outputs:\n",
    "\n",
    "$$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel\"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)  # Projection layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Concatenate outputs from all heads\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        \n",
    "        # Apply projection and dropout\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feed-Forward Network\n",
    "\n",
    "After attention, each transformer block applies a position-wise feed-forward network:\n",
    "\n",
    "$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "This is applied to each position separately and identically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # Expand by factor of 4\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),  # Project back down\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformer Block\n",
    "\n",
    "A transformer block combines multi-head attention and feed-forward networks with residual connections and layer normalization:\n",
    "\n",
    "$$\\text{Block}(x) = x + \\text{FFN}(\\text{LayerNorm}(x + \\text{MultiHead}(\\text{LayerNorm}(x))))$$\n",
    "\n",
    "### Layer Normalization\n",
    "\n",
    "Layer normalization normalizes across the feature dimension:\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta$$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ are the mean and standard deviation computed across the feature dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)  # Self-attention\n",
    "        self.ffwd = FeedForward(n_embd)                  # Feed-forward\n",
    "        self.ln1 = nn.LayerNorm(n_embd)                  # Layer norm 1\n",
    "        self.ln2 = nn.LayerNorm(n_embd)                  # Layer norm 2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pre-norm formulation (more stable training)\n",
    "        x = x + self.sa(self.ln1(x))    # Residual connection around attention\n",
    "        x = x + self.ffwd(self.ln2(x))  # Residual connection around FFN\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete GPT Model\n",
    "\n",
    "Now we'll combine all components into a complete GPT-style language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 10,788,929\n"
     ]
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\"GPT-style transformer language model\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        \n",
    "        # Final layer norm and language modeling head\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        x = self.blocks(x)  # (B, T, C)\n",
    "        x = self.ln_f(x)    # (B, T, C)\n",
    "        \n",
    "        # Language modeling head\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generate new tokens using the trained model.\n",
    "        \n",
    "        Args:\n",
    "            idx: Starting context of shape (B, T)\n",
    "            max_new_tokens: Number of tokens to generate\n",
    "        \n",
    "        Returns:\n",
    "            Generated sequence of shape (B, T + max_new_tokens)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context to last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # (B, C)\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            \n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "# Initialize model and move to device\n",
    "model = GPTLanguageModel()\n",
    "model = model.to(device)\n",
    "\n",
    "# Print number of parameters\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Setup and Evaluation\n",
    "\n",
    "We'll set up the training loop with proper evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation before training:\n",
      "\n",
      "sQHN!ZIQcmC'o'h,jGcJ-QWJtWzFuL\n",
      "\n",
      "  KTq,mHPnF:yJdUVAbXkQgJc&hhAYXLTJAEBxQ$?pPXt';;yreE;fcKetP3&eG,pZTj\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"Estimate loss on train and validation sets\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Test generation before training\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(\"\\nGeneration before training:\")\n",
    "print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n",
    "\n",
    "Now we'll train our GPT model using the AdamW optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Step    0: train loss 4.2638, val loss 4.2678\n",
      "Step  500: train loss 1.8920, val loss 1.9977\n",
      "Step 1000: train loss 1.5397, val loss 1.7253\n",
      "Step 1500: train loss 1.3954, val loss 1.6105\n",
      "Step 2000: train loss 1.3077, val loss 1.5502\n",
      "Step 2500: train loss 1.2509, val loss 1.5301\n",
      "Step 3000: train loss 1.2026, val loss 1.4972\n",
      "Step 3500: train loss 1.1624, val loss 1.4940\n",
      "Step 4000: train loss 1.1220, val loss 1.4829\n",
      "Step 4500: train loss 1.0893, val loss 1.4936\n",
      "Step 4999: train loss 1.0549, val loss 1.4987\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # Evaluate loss periodically\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        loss_dict = estimate_loss()\n",
    "        print(f\"Step {iter:4d}: train loss {loss_dict['train']:.4f}, val loss {loss_dict['val']:.4f}\")\n",
    "        losses.append(loss_dict['train'].item())\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results and Text Generation\n",
    "\n",
    "Let's visualize the training progress and generate some text with our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAIjCAYAAAA0vUuxAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZpZJREFUeJzt3QeUFFXaxvGnyTlnGQQkSM7IgIquCKIiuKy6qAuuETPqGlgjuoprzmFNrDmDfiooIEFyRkBEUZJKEsl5mP7Oe2t7EpOZmaru/v/OqaW6urr7Tndtzzzee98bCofDYQEAAAAAslQs67sAAAAAAAQnAAAAAMgFepwAAAAAIAcEJwAAAADIAcEJAAAAAHJAcAIAAACAHBCcAAAAACAHBCcAAAAAyAHBCQAAAAByQHACAATO6tWrFQqFNGrUqHw93h57zz33FHi7AADxi+AEAAG3atUqXXPNNWrWrJnKlSvntpYtW+rqq6/Wt99+m+5cCwsWGiJb5Nw77rhDO3bscOekvT+7bfLkyYe1JePzZ7WddNJJiufAF9mKFy+uBg0a6Oyzz9aiRYv8bh4A4AiUOJIHAwAK12effabzzjtPJUqU0AUXXKB27dqpWLFi+v777/Xxxx/r+eefd8Hq6KOPTvc4O16hQgXt2rVLX331le6//359/fXXmj59ut544410577++usaP378YcdbtGhxWHv+/Oc/q0mTJim37fmvvPJKFwzsvojatWsf0c9tP8/evXtVsmTJfD3eHmvvmV8GDRqk008/XYcOHdLy5cvd5zF27FjNmjVL7du3961dAID8IzgBQED99NNP+utf/+pCxMSJE1W3bt109//73//Wc88954JURn/5y19Uo0YNtz906FANHDjQBS37w/3CCy9Md64ds+CU8Xhm2rZt67aI33//3QUnO5bd4/ft26dSpUpl2tbMWG9NmTJllF9H8tiC0LFjx3TvR48ePXTWWWe5APXiiy9m+pjdu3erfPnyRdK+vH4eAACG6gFAYD300EPuj+nXXnvtsNBkrEfluuuuU0JCQo7P9ac//cn9a71Thc2G+Fnweffdd90QwaOOOsoNGbShgn/88Yf+8Y9/qE2bNq5HrFKlSurbt68WL16c4xyniy66yD3m119/1YABA9x+zZo13fNZz052c5wiQwxXrlzpnqdKlSqqXLmy/v73v2vPnj2H9VbZ+2rBs2LFii7w2GseybypjO+//Vz2fFOmTNFVV12lWrVqqX79+innWyBu1aqVSpcurXr16rlhmdu2bTvseZ999lk1btxYZcuWVdeuXfXNN9+4YZJph0pm93mY2bNn67TTTnPvhx3v2bOn65lMa+fOnRo2bJgaNmzo2mTtPfXUU7VgwYKUc3788UcX0OvUqeOCq/08Fvy3b9+er/cMAIKGHicACPAwPRsWd9xxxxVI75WpXr26isp9993nejUs2Ozfv9/tf/fddxozZozOOeccNWrUSBs3bnQ9MPbHut1nISE7FpD69Onj3pNHHnlEEyZM0KOPPqpjjjnG9Xzl5Nxzz3WvO3LkSPdH/8svv+xCgPXeRViwev/99/W3v/1N3bp1c+HmjDPOKJT330KThb+77rrLhWRj4WzEiBHq1auX+5lWrFjheqrmzp3rAk1k+KIds7lvJ5xwgm644QYXNi1QVq1aNV0Iy+7zsOGbFlw7deqku+++2/VAWVC3oGchzMJYpNfyww8/dK9nc+a2bNmiadOmuWGI1rt24MAB97nY81577bUuPFnYtGvYAp+FMgCIemEAQOBs3749bF/RAwYMOOy+rVu3hjdv3pyy7dmzJ+W+u+++2z1uxYoV7r5Vq1aFX3zxxXDp0qXDtWvXDu/evfuw57v66qvdY/LDXsMea68bMWnSJHescePG6dpm9u3bFz506FC6Y9ZGa9+9996b7pg9x2uvvZZybMiQIe5Y2vNMhw4dwp06dUp3LGObIu/LxRdfnO68s88+O1y9evWU2/Pnz3fnDRs2LN15F1100WHPmZlIu0eMGOHemw0bNoQnT57s2mjHP/roI3ee/Vx2+/jjjw8nJSWlPH7Tpk3hUqVKhXv37p3ufXrmmWfc+a+++qq7vX//ftfuLl26hA8ePJhy3qhRo9x5PXv2zPHzSE5ODjdt2jTcp08ftx9h5zRq1Ch86qmnphyrXLmyu06ysnDhQvcaH3zwQbbvDwBEMwY3A0AARYZR2XC0jGwYlvVSRDYbrpVR8+bN3X3Wu3LFFVe4nqvPP//cDcUqKkOGDHFDyNKyYV6ReTXWe2Q9F/YzWnvTDvvKjvV+pGU9Lj///HO+H2ttiLzf48aNS+kJSst6UfLCem/s/beeF/u8rMfJerXSFtAwl112mau8F2E9aNZ7Y8Pi0s4/svNsWKN9hmbevHmu3XY8bREMKyBiPU65+Tysyp8Nrzv//PPdc9l8Ndus5+uUU07R1KlTlZyc7M61oY02pO+3337L9LkjPUpffvnlYUMfASBWMFQPAALI5tZEqtZlZEPbbM6JDXPLqiDDRx995P7QtmFdNmzLhrIVNQttGdkf4k8++aSbw2PzfdLOTcrNMEKbO2OBJC0LClu3bs1Vm6w0eMbHGnu8vV9r1qxxgSVj29NWEsyNyy+/3A1HtOey0BGZr5RRxtex1zcWJNOyYXU2lylyf+TfjO2yEGXzkDKT8bUsNEUCVVZsfpK9Rzbfzs6z+XQ2rM8qBg4ePNi1KfLcN954ox577DG99dZbLpDa3DC7PhmmByBWEJwAIIDsj00rCLF06dLD7ovMebI5LVk58cQTU6rq+SVjb5N54IEHdOedd+riiy92c26qVavmwoX1sER6N7KTtncmP7J6vDe6r+A0bdrUzVHKz3tUWDK+VuT9fvjhh7MskR7p8bS5YRaGRo8e7crb22OsB80qNdocKWNzzWx+2CeffOLOsQIbNpfMqjZmNucKAKINQ/UAIKCsIIFVgZszZ45ihRUYOPnkk/XKK6+4imu9e/d2ASOzinF+sNLvFigyVh+0z6GoXt9YQYi0bPhe2vW6Iv9mbFdSUlK2gTqtSC+k9bTZZ5DZlnYdLQvyNoTRintYW6yH0NYHS8uqJVrlPhvmZ8UlrEDECy+8kK/3AgCChuAEAAF1yy23uDlJ1jtjw/IKu5ekKFiPT8Z2f/DBB+4P7CCwynDGhhKm9fTTTxfJ61tYsWF5Tz31VLr3yYKmDZuLVPfr3LmzCy4vvfSSC0sRNkwut8MWbcidhSerTpjZkNDNmze7f204ZcaS4laJ0CogWhU9Y3PE0rYjEqKsNzFyDgBEO4bqAUBA2XCvt99+W4MGDXJzXmzif7t27dwf1PZf/O0++8M0moZBnXnmmbr33nvd+kndu3fXkiVL3B/7kbkyfrMwYWsRPfHEE65gQqQc+Q8//ODut/WQCpPN3xo+fLgrR25rK9k8Iet9siDXpUuXlDltFq6sbLkVrbDS4TaUznqabH0oC0O5aaddO1aO3Yba2Rws+0xsjScLsZMmTXI9Uf/3f//n5tPZNWaLKtv1Z8P3rIiFlUe34XnGyppbqXKb19WsWTMXot544w0XlO39BIBYQHACgADr37+/Cxf2B6rNG3n11VfdH8U2VMt6H6xKnP0xGy3++c9/uqptFvree+89twaQVYq77bbbFBSvv/66q4b3zjvvuDk91gtkbbXwasUpCpsFIgtQzzzzjFufyeaBWbEJmx+WduicBRUL0XZt2NpMdh18+umnbm5RbttpFf9mzpzp5pvZ61nPk/3sNo/OqjEa6/W0IXp2/dmcJhvKaEUpLMxF1s6y17beOgtaFrzsMXZs7NixLnwCQCwIWU1yvxsBAECQWenuDh066M0333Q9f0FlocZCl5U9t2F8AICCwxwnAADS2Lt372Hvhw3ds6FtVq0wKPbt23fYfDHrLfvjjz9cTxIAoGAxVA8AgDRszaL58+e76n+2LpINN7PNhsvZOkZBYWW+bSifzSuyQhG2gLAVkWjdurU7BgAoWAzVAwAgjfHjx7viDN99952b82OL5v7tb3/T7bff7oJUUFgxCJvPZOXqrZfJ5kLZwrQPPvigq3oHAChYBCcAAAAAyAFznAAAAAAgBwQnAAAAAMhBcAZrF2Gp1t9++00VK1Ys9IUMAQAAAASXVSe1hb7r1avnqqdmJ+6Ck4WmIFVFAgAAAOCvdevWqX79+tmeE3fByXqaIm9OpUqVAtEDtnnzZrdgYU4pF+B6Ad8xKGx8z4BrBvH0PbNjxw7XqRLJCNmJu+AUGZ5noSkowckWMbS2+H3hIPi4XsA1A75nEDT8bkIsXDO5mcITjJYCAAAAQIARnAAAAAAgBwQnAAAAAMhB3M1xAgAAQOw4dOiQDh486HczkMc5TvaZ2TynopjjVLJkSRUvXvyIn4fgBAAAgKi0a9cu/fLLL24tHkSPcDjswpOtn1QU66raa1ip8QoVKhzR8xCcAAAAEJU9TRaaypUr58paF8Uf4Ci44JSUlKQSJUoU+udmr2Wlz+1aadq06RH1PBGcAAAAEHVsqJf9UWyhqWzZsn43BwENTsaukdWrV7tr5kiCE8UhAAAAELXoaUJRXSMEJwAAAADIAcEJAAAAAHJAcAIAAACiWMOGDfXEE0/k+vzJkye74Wvbtm0r1HbFGoITAAAAUAQsrGS33XPPPfl63rlz5+ryyy/P9fndu3fX+vXrVblyZRWmyTEW0KiqBwAAABQBCysR7733nu666y6tWLEi5VjadYas8pyVXLfKc7mpGpcXpUqVUp06dfL0GNDjBAAAABQJCyuRzXp7rDcmcvv7779XxYoVNXbsWHXq1EmlS5fWtGnT9NNPP6l///6qXbu2C1ZdunTRhAkTsh2qZ8/78ssv6+yzz3brXNn6RZ9++mmWPUGjRo1SlSpV9OWXX6pFixbudU477bR0QS8pKUnXXXedO6969eq69dZbNWTIEA0YMCDf78fWrVs1ePBgVa1a1bWzb9+++vHHH1PuX7Nmjfr16+fuL1++vFq1aqUvvvgi5bEXXHBBSjl6+xlfe+01FSZ6nAAAABATOneWNmwo+te1zpt58wrmuW677TY98sgjaty4sQsM69at0+mnn67777/fhanXX3/dhQnrqWrQoEGWzzNixAg99NBDevjhh/X000+7kGFBpFq1apmev2fPHve6b7zxhooVK6YLL7xQ//jHP/TWW2+5+//973+7fQsnFq6efPJJjRkzRieffHK+f9a///3vLihZqKtUqZILY/azfvfddypZsqSuvvpqHThwQFOnTnXByY5HeuXuvPNOd9uCZo0aNbRy5Urt3btXhYngBAAAgJhgoenXXxXV7r33Xp166qkpty3otGvXLuX2fffdp9GjR7uwcc0112T5PBdddJEGDRrk9h944AE99dRTmjNnjutJyowtDvvCCy/omGOOcbftua0tEU8//bSGDx/uerHMM888k9L7kx+RwDR9+nQ358pYMEtISHCB7JxzztHatWs1cOBAtWnTxt1vYTLC7uvQoYM6W1r+X69bYSM4+WT3bunJJ6Xp00OqWbOSXn3Vr5YAAADEBr+m7RTk60aCQMSuXbtc0YjPP//cDZ2zIXPWs2LBITtt27ZN2bfeGuvR2bRpU5bn21C5SGgydevWTTl/+/bt2rhxo7p27Zpyf/Hixd2QwuTk5Hz9nDY00eZvHXfccSnHbAhg8+bNtXz5cnfbhgZeeeWV+uqrr9SrVy8XoiI/lx232wsWLFDv3r3dkMFIACssBCeflCol3X+/dYuGVL9+ab+aAQAAEDMKaricnyzkpGXD5caPH++G0TVp0sTN5/nLX/7ihrBlx4a6pWVzmrILOZmdbwUq/HTppZeqT58+LjRaeBo5cqQeffRRXXvttW4+lA09tF4ve39OOeUUN7TP3qfCQjlyn9i1GQntv/xSPOq7lQEAAFDwbCibDbuzIXI2ZM0KSaxevbpI32orZFG7dm1X9jzCKv5Zb09+HXvssa73bPbs2SnHtmzZ4uZutWzZMuWYDd0bOnSoPv74Y91000166aWXUu6zwhBWoOLNN990xTH+85//qDDR4+SjHj2sqom3P2OGdN55frYGAAAAQWPV4iw0WEEI6wWyogj5HR53JK699lrX42O9XhZ6bM6TVbazNuVkyZIlrmJgWlYhz6oFXnbZZXrxxRfd/VYY46ijjnLHzbBhw1zPUrNmzdxrTZo0yRWmMFbK3YYK2vPs379fn332Wcp9hYXg5KO0wzBnzgwRnAAAAJDOY489posvvtjN37HqcVZ5bseOHUX+Lt16663asGGDKx9u85tswV0bRmf7OTnxxBPT3bbH2DytV1991YWjM8880w09tPNs6F1k2KD1atnwu19++cXN0bLCFo8//njKWlRWrMJ632z44gknnKB3331XhSkU9nvwYhGzC826G22Sm30AfvrjD5sE5+137RrW7Nk5J3bEN/svTDZRs1atWq5UKMA1A75nEK+/m/bt26dVq1apUaNGKlOmTJG9LlI/d+vhOffcc12lv7yw+GHD9Kw4RG56rArzWslLNqDHyUdWRr9Fi7CWLw/Jhoha6fmyZf1sEQAAAHC4NWvWuAINPXv2dEPjrBy5hZHzzz8/bt4u/pO1zxITvX+TkkIxUQkGAAAAsadYsWIaNWqUunTpoh49erh5SxMmTCj0eUVBQo+TzxITw3r11VBKgYgTTvC7RQAAAEB6CQkJrsJfPKPHKUAFIiw4AQAAAAgegpPPmjWTqlZNTglO8VWqAwAA4MjEWZ0z+HiNEJx8ZsVnOnXyVn7+/Xdp5Uq/WwQAABB8kTLYVsYayE7kGslN6fTsMMcpADp3PqgJE8qk9Do1bep3iwAAAILNSlmXK1dOmzdvduv+sExH9AgXYTlyK5tu14hdK/Z6R4LgFABduqT+lxKbczdkiK/NAQAACDz7g7tu3bquJLaVykZ0Bafk5GQXdotiHSd7nQYNGhzxaxGcAqB9+4MqXjysQ4dCFIgAAADIpVKlSqlp06YM14syycnJ2rJli6pXr14kPYV2nRTE6xCcAqBcOalDB7l1nJYtk7Ztk6pU8btVAAAAwWd/EJcp4015QPQEp5IlS7rPLZqGWEZPS+NkIVwza5afLQEAAACQEcEpQAvhRrCeEwAAABAsBKeAYCFcAAAAILgITgGRkCDVr+/tz54tJSX53SIAAAAAEQSnAPY67dolLV3qd2sAAAAARBCcAoThegAAAEAwEZwChOAEAAAABBPBKUDat5fKlvX2p0/3uzUAAAAAIghOAVKypNS1q7e/erX0229+twgAAACAITgFeLjezJl+tgQAAABABMEpYJjnBAAAAAQPwSlgunVL3Z8xw8+WAAAAAIggOAVMjRpS8+be/vz50r59frcIAAAAAMEpwMP1Dh70whMAAAAAfxGcAoh5TgAAAECwEJwCiOAEAAAABEtggtODDz6oUCikYcOGZXveBx98oGOPPVZlypRRmzZt9MUXXyjWHHusVKVKaoGIcNjvFgEAAADxLRDBae7cuXrxxRfVtm3bbM+bMWOGBg0apEsuuUQLFy7UgAED3LZ06VLFkmLFUnudNm2SfvrJ7xYBAAAA8c334LRr1y5dcMEFeumll1S1atVsz33yySd12mmn6eabb1aLFi103333qWPHjnrmmWcUaxiuBwAAAARHCb8bcPXVV+uMM85Qr1699K9//Svbc2fOnKkbb7wx3bE+ffpozJgxWT5m//79bovYsWOH+zc5OdltfrM2hMPhw9rirefk5drp08O68ELG6yHr6wXI63cMwDWDgsL3DKL5mslLG3wNTu+++64WLFjghurlxoYNG1S7du10x+y2Hc/KyJEjNWLEiMOOb968WfsCsEiSfVjbt293F08xG6P3Pw0bhlS8eC0dOhTSN98kadOmLb62E8GQ1fUCcM2A7xn4hd9NiOZrZufOncEPTuvWrdP111+v8ePHu0IPhWX48OHpeqmsxykhIUE1a9ZUpUqVFIQLx4piWHsyXjjt2kkLFkjff19CpUvXUuXKvjUTAZHd9QJwzYDvGfiB302I5msmLznEt+A0f/58bdq0yc1Rijh06JCmTp3q5izZ8LrixYune0ydOnW0cePGdMfsth3PSunSpd2WkX1Ifn9QEXbhZNYem+dkwSkcDmnu3JB69/atiQiQrK4XgGsGfM/AL/xuQrReM3l5fd9aesopp2jJkiVatGhRyta5c2dXKML2M4Ymk5iYqIkTJ6Y7Zj1WdjwWUSACAAAACAbfepwqVqyo1q1bpztWvnx5Va9ePeX44MGDddRRR7l5SsaG9vXs2VOPPvqoKyhhc6TmzZun//znP4pFBCcAAAAgGAI91mft2rVav359yu3u3bvr7bffdkGpXbt2+vDDD11FvYwBLFY0aCDVq+ftz5plQxn9bhEAAAAQn3wvR57W5MmTs71tzjnnHLfFg1DI63X68EOr+CEtWyblsEYwAAAAgHjrcYLUo0fquzB9Ou8IAAAA4AeCU8AxzwkAAADwH8Ep4Nq3t/ry3v6MGX63BgAAAIhPBKeAK1VK6tLF2//5Z2nDBr9bBAAAAMQfglOUDdebOdPPlgAAAADxieAUBZjnBAAAAPiL4BQFEhNT95nnBAAAABQ9glMUqFlTatrU2583T9q/3+8WAQAAAPGF4BRlw/UOHJAWLPC7NQAAAEB8IThFCeY5AQAAAP4hOEWJHj1S96dP97MlAAAAQPwhOEWJFi2kypVTC0SEw363CAAAAIgfBKcoUaxYanW9jRulVav8bhEAAAAQPwhOUYR5TgAAAIA/CE5RhOAEAAAA+IPgFEW6dvWG7BkWwgUAAACKDsEpilSsKLVt6+0vWSLt2OF3iwAAAID4QHCK0uF6ycnSnDl+twYAAACIDwSnKMM8JwAAAKDoEZyiDMEJAAAAKHoEpyjTsKFUt663P3OmdOiQ3y0CAAAAYh/BKcqEQqm9TlYc4rvv/G4RAAAAEPsITlGI4XoAAABA0SI4RSGCEwAAAFC0CE5RqEMHqXRpb5+FcAEAAIDCR3CKQhaaOnf29leulDZt8rtFAAAAQGwjOMXAcD2rrgcAAACg8BCcohTznAAAAICiQ3CKUomJqfvMcwIAAAAKF8EpStWuLR1zjLc/d6504IDfLQIAAABiF8EpBobr7d8vLVjgd2sAAACA2EVwimI9eqTuM1wPAAAAKDwEpyhGgQgAAACgaBCcoljLllKlSt7+9OlSOOx3iwAAAIDYRHCKYsWLS926efsbNkhr1vjdIgAAACA2EZyiHMP1AAAAgMJHcIpyBCcAAACg8BGcotxxx0mhkLdPZT0AAACgcBCcopwVh2jTxttfvFjatcvvFgEAAACxh+AUQ8P1kpOlOXP8bg0AAAAQewhOMTbPycqSAwAAAChYBKcY0KNH6j7znAAAAICCR3CKAY0aSbVre/szZ3pD9gAAAAAUHIJTDLCqepHhetu3S8uX+90iAAAAILYQnGIE6zkBAAAAhYfgFCMITgAAAEDhITjFiI4dpVKlvH0KRAAAAAAFi+AUI8qUkTp18vZ/+EH6/Xe/WwQAAADEDoJTjA7Xs+p6AAAAAAoGwSmGsBAuAAAAUDgITjGEAhEAAABA4SA4xZA6daTGjb39uXOlAwf8bhEAAAAQGwhOMdrrtG+ftGiR360BAAAAYgPBKcYwXA8AAAAoeASnGENwAgAAAGIsOD3//PNq27atKlWq5LbExESNHTs2y/NHjRqlUCiUbitjCxghRevWUoUK3v706VI4zJsDAAAARHVwql+/vh588EHNnz9f8+bN05/+9Cf1799fy5Yty/IxFrDWr1+fsq1Zs6ZI2xx0xYtL3bp5+7/9Jq1b53eLAAAAgOhXws8X79evX7rb999/v+uFmjVrllq1apXpY6yXqY6Vj0O2w/UmTPD2Z8yQGjTgzQIAAACiNjildejQIX3wwQfavXu3G7KXlV27dunoo49WcnKyOnbsqAceeCDLkGX279/vtogdO3a4f+3xtvnN2hAOhwu0LV6Pk9eZOH16WOeey3i9WFEY1wtiG9cMuGbA9wyCJjlAf8/kpQ2+B6clS5a4oLRv3z5VqFBBo0ePVsuWLTM9t3nz5nr11VfdvKjt27frkUceUffu3d3QPhv2l5mRI0dqxIgRhx3fvHmze80gfFj2s9jFU6xYwYycbNzY5n/VUjgc0pQpSdq0aUuBPC8Uk9cLYhvXDLhmwPcMgiY5QH/P7Ny5M9fnhsLWYh8dOHBAa9eudW/ehx9+qJdffllTpkzJMjyldfDgQbVo0UKDBg3Sfffdl+sep4SEBG3dutXNlwrChWMhrmbNmgV64bRrF9LSpSEVLx7WH3+EUwpGILoV1vWC2MU1A64Z8D2DoEkO0N8zlg2qVq3qskhO2cD3HqdSpUqpSZMmbr9Tp06aO3eunnzySb344os5PrZkyZLq0KGDVq5cmeU5pUuXdltG9iH5/UGlnbdV0O3p0UNautSGQIY0f35IJ59cYE+NGLxeENu4ZsA1A75nEDShgPw9k5fXLxbEBJq2hyineVE21K9u3bqF3q5ow3pOAAAAQMHxtcdp+PDh6tu3rxo0aODGF7799tuaPHmyvvzyS3f/4MGDddRRR7l5Subee+9Vt27dXA/Vtm3b9PDDD7ty5JdeeqmfP0YgEZwAAACAGAlOmzZtcuHI1mOqXLmyK/pgoenUU09199vcp7TdZzYv6bLLLtOGDRvcWEQb2jdjxoxczYeKN8ccI9WsaUUwpJkzrSfPuiL9bhUAAAAQnXwNTq+88kq291vvU1qPP/6425CzUMjrdfrkEwuc0ooVUosWvHMAAABAftAHEcMYrgcAAAAUDIJTDCM4AQAAAAWD4BTDOnWyku3e/owZfrcGAAAAiF4EpxhWtqzUsaO3//330pYtfrcIAAAAiE4EpzgarmfV9QAAAADkHcEpxvXokbrPcD0AAAAgfwhOMS4xMXWf4AQAAADkD8EpxtWrJzVs6O3PmSMdPOh3iwAAAIDoQ3CKo3lOe/dKixf73RoAAAAg+hCc4gDrOQEAAABHhuAUBwhOAAAAwJEhOMWBNm2k8uW9fQpEAAAAAHlHcIoDJUpIxx3n7a9b520AAAAAco/gFCcYrgcAAADkH8EpThCcAAAAgPwjOMUJFsIFAAAA8o/gFCeqVJFatfL2Fy6Udu/2u0UAAABA9CA4xeFwvUOHpHnz/G4NAAAAED0ITnGEeU4AAABA/hCc4gjBCQAAAMgfglMcadpUql49dSHccNjvFgEAAADRgeAUR0Kh1F6nP/6QfvjB7xYBAAAA0YHgFGcYrgcAAADkHcEpjoPT9Ol+tgQAAACIHgSnONO5s1SiROo8JwAAAAA5IzjFmXLlpI4dvf3ly725TgAAAACyR3CK8+F6s2b52RIAAAAgOhCc4hAFIgAAAIC8ITjFocTE1H3mOQEAAAA5IzjFofr1pQYNvP3Zs6WkJL9bBAAAAAQbwSnOh+vt2SN9+63frQEAAACCjeAUp5jnBAAAAOQewSlOEZwAAACA3CM4xam2bb01ncz06X63BgAAAAg2glOcKllS6trV21+7VvrlF79bBAAAAAQXwSmO9eiRuj9zpp8tAQAAAIKN4BTHmOcEAAAA5A7BKY5165a6z0K4AAAAQNYITnGsWjWpRQtvf8ECae9ev1sEAAAAxEhw+u9//6vPP/885fYtt9yiKlWqqHv37lqzZk1Btw9FNFwvKUmaN4+3GwAAACiQ4PTAAw+obNmybn/mzJl69tln9dBDD6lGjRq64YYb8vp08BnznAAAAICclVAerVu3Tk2aNHH7Y8aM0cCBA3X55ZerR48eOumkk/L6dPAZwQkAAAAohB6nChUqaMuWLW7/q6++0qmnnur2y5Qpo71Mkok6zZp5c50iBSLCYb9bBAAAAMRAcLKgdOmll7rthx9+0Omnn+6OL1u2TA0bNiyMNqIQFSsmJSZ6+7//Lv34I283AAAAcMTByeY0JSYmavPmzfroo49UvXp1d3z+/PkaNGhQXp8OAcBwPQAAAKCA5zhZBb1nnnnmsOMjRozI61MhIHr0SN234XoXXeRnawAAAIAY6HEaN26cpk2blq4Hqn379jr//PO1devWgm4fikCXLlLx4t4+C+ECAAAABRCcbr75Zu3YscPtL1myRDfddJOb57Rq1SrdeOONeX06BEC5clKHDt7+smXStm1+twgAAACI8uBkAally5Zu3+Y4nXnmmW5tJ+t5Gjt2bGG0EUU8z2nWLN5yAAAA4IiCU6lSpbRnzx63P2HCBPXu3dvtV6tWLaUnCtGHAhEAAABAARaHOP74492QPFvwds6cOXrvvffccStNXr9+/bw+HQKC4AQAAAAUYI+TVdQrUaKEPvzwQz3//PM66qij3HEbpnfaaafl9ekQEAkJUiT3zp4tJSX53SIAAAAginucGjRooM8+++yw448//nhBtQk+9jq9/760a5e0dKnUvj0fBQAAAJCv4GQOHTqkMWPGaPny5e52q1atdNZZZ6l4pKY1ojo4menTCU4AAABAvofqrVy5Ui1atNDgwYP18ccfu+3CCy904emnn37K03PZUL+2bduqUqVKbktMTMyxMt8HH3ygY489VmXKlFGbNm30xRdf5PVHQBaY5wQAAAAUUHC67rrrdMwxx2jdunVasGCB29auXatGjRq5+/LCikk8+OCDmj9/vubNm6c//elP6t+/v5bZYkKZmDFjhgYNGqRLLrlECxcu1IABA9y21MaV4YjZ0LyyZSPvNW8oAAAAEBEKh8Nh5UH58uU1a9Ys19uT1uLFi12lvV02QeYIWFnzhx9+2IWjjM477zzt3r073Ryrbt26qX379nrhhRdy9fxWMr1y5cravn276+XyW3JysjZt2qRatWqpWLE859gCd9JJ0pQp3v6vv0r16vndIgT5ekHwcc2AawZ8zyBokgP090xeskGe5ziVLl1aO3fuPOy4BSZb4ym/bN6UDcOzYGRD9jIzc+ZMVwo9rT59+rj5VlnZv3+/2yIia03ZB2ab36wNll2D0BaTmBjSlCkhtz99erIGDvS7RQjy9YLg45oB1wz4nkHQJAfo75m8tCHPwenMM8/U5ZdfrldeeUVdu3Z1x2bPnq2hQ4e6AhF5tWTJEheU9u3bpwoVKmj06NFq2bJlpudu2LBBtWvXTnfMbtvxrIwcOVIjRow47PjmzZvdawbhw7KEaxeP34nbtGxZWlJVtz9x4l6dcMLhIRn+Cdr1guDjmgHXDPieQdAkB+jvmcw6hAosOD311FMaMmSICzslS5Z0x5KSklxoeuKJJ/L6dGrevLkWLVrk3jxbG8qee8qUKVmGp7waPnx4ul4q63FKSEhQzZo1AzNULxQKufb4feGYPn1S9xcvLqdatf436QmBELTrBcHHNQOuGfA9g6BJDtDfM1ZwrtCCU5UqVfTJJ5+46nqRcuRWZa9JkybKDxveF3lsp06dNHfuXD355JN68cUXDzu3Tp062rhxY7pjdtuOZze00LaM7EPy+4OKsAsnKO2pVcvCrLRihTR/fkgHDoSUh+sJcXa9IDpwzYBrBnzPIGhCAfl7Ji+vn++WWtjp16+f22z/22+/PaI5TmkTaNo5SWlZL9fEiRPTHRs/fnyWc6JwZGXJDx608MS7CAAAABRYxLMxilbgIa/D6KZOnarVq1e7uU52e/Lkybrgggvc/bZWlB2LuP766zVu3Dg9+uij+v7773XPPfe4MubXXHMNn2QhredkC+ECAAAA8S7PQ/UKkpUhtHC0fv16VwbQFsP98ssvdeqpp7r7bX2otN1n3bt319tvv6077rhD//znP9W0aVNXUa9169Y+/hSxh4VwAQAAgAAFJ6vMlx3rfcronHPOcRsKz7HH2lw2ads2byFcW+kr5FUoBwAAAOJSroNTZP2jgijlh2CzTj7rdfriCyvbLv30k81p87tVAAAAQBQEJ6umZ9UvspvjlN39iC6R4GSs14ngBAAAgHiW6+A0adKkwm0JAj3PafBgP1sDAAAARElw6tmzZ+G2BIHSpYtUvLhkhRItOAEAAADxjBU0kakKFaR27bz9pUul7dt5owAAABC/CE7IcbieVdWbPZs3CgAAAPGL4IQssZ4TAAAA4CE4IVfBafp03igAAADErzwHp9dee0179uwpnNYgUBo0kOrV8/ZnzfIKRQAAAADxKM/B6bbbblOdOnV0ySWXaAbl1mKaLcsV6XXatcsrEgEAAADEozwHp19//VX//e9/9fvvv+ukk07Sscceq3//+9/asGFD4bQQvurRI3WfnAwAAIB4lefgVKJECZ199tn65JNPtG7dOl122WV666231KBBA5111lnueHJycuG0FkWOAhEAAADAERaHqF27to4//nglJiaqWLFiWrJkiYYMGaJjjjlGkydP5v2NAe3bS2XKePv0OAEAACBe5Ss4bdy4UY888ohatWrlhuvt2LFDn332mVatWuWG8p177rkuQCH6lSoldeni7f/8s8SITAAAAMSjPAenfv36KSEhQaNGjXLD9CwovfPOO+rVq5e7v3z58rrpppvcMD7E3nC9mTP9bAkAAADgjxJ5fUCtWrU0ZcoUNzwvKzVr1nS9T4jNeU5nn+1nawAAAIAoCE6vvPJKjueEQiEdffTR+W0TAiZtRmaeEwAAAOJRvuY4TZw4UWeeeaYrAmGb7U+YMKHgW4dAqFlTatrU2583T9q3z+8WAQAAAAEPTs8995xOO+00VaxYUddff73bKlWqpNNPP13PPvts4bQSgRmud+CAtGCB360BAAAAAj5U74EHHtDjjz+ua665JuXYddddpx49erj7rr766oJuIwISnP7739ThemnnPQEAAACxLs89Ttu2bXM9Thn17t1b27dvL6h2IWB69EjdZ54TAAAA4k2eg9NZZ52l0aNHH3b8k08+cXOdEJtatJAqV04NTuGw3y0CAAAAAjxUr2XLlrr//vs1efLklJLks2bN0vTp0936TU899VS6IXyIDcWKedX1xo2zBZAlqzbfuLHfrQIAAAACXI68atWq+u6779wWUaVKlXSlyq0kOcEptti8JgtOkV4nghMAAADiRZ6DEwvbxq+MC+FeeKGfrQEAAAACvo5TRDgcdhviQ9eu3pA9Q4EIAAAAxJN8BafXX39dbdq0UdmyZd3Wtm1bvfHGGwXfOgRKxYpS27be/pIl0o4dfrcIAAAACGhweuyxx3TllVe6BW/ff/99t1l58qFDh7r1nRAfw/WSk6XZs/1uDQAAABDQOU5PP/20nn/+eQ0ePDhdifJWrVrpnnvu0Q033FDQbUTAgtNzz6UO1zv1VL9bBAAAAASwx2n9+vXqnrZKwP/YMbsP8VUgAgAAAIgHeQ5OTZo0ccPzMnrvvffUtGnTgmoXAqphQ6luXW9/1izp0CG/WwQAAAAEcKjeiBEjdN5552nq1Knq0aOHO2aL306cODHTQIXYEgp5vU4ffeQVh7ClvNq08btVAAAAQMB6nAYOHKg5c+aoRo0aGjNmjNts346dffbZhdNKBArD9QAAABBv8tTjdPDgQV1xxRW688479eabbxZeqxBVwemKK/xsDQAAABCwHqeSJUvqIxujhbjWoYNUurS3T4EIAAAAxIM8D9UbMGCAG56H+GWhqXNnb3/lSmnTJr9bBAAAAASsOIRVzrv33ntdQYhOnTqpfPny6e6/7rrrCrJ9CPBwvenTvf2ZM6X+/f1uEQAAABCg4PTKK6+oSpUqmj9/vtvSCoVCBKc4nOdkAYrgBAAAgFiW5+C0atWqwmkJokpiYuo+85wAAAAQ6/I8x8mG6e3Zs+ew43v37nX3IT7Uri0dc4y3P2+etH+/3y0CAAAAAhScbAHcXbt2HXbcwpTdh/jxv/WPXWhauNDv1gAAAAABCk7hcNjNZcpo8eLFqlatWkG1C1GAhXABAAAQL3I9x6lq1aouMNnWrFmzdOHp0KFDrhdq6NChhdVOREFwuvFGP1sDAAAABCA4PfHEE6636eKLL3ZD8ipXrpxyX6lSpdSwYUMlpq0YgJjXsqVUqZK0Y4dXWS8ctsqKfrcKAAAA8DE4DRkyxP3bqFEjde/eXSVLliyE5iCaFC8udesmffWVtGGDtGaN1LCh360CAAAAAlCOvGfPnkpOTtYPP/ygTZs2uf20TjzxxIJsH6JguJ4Fp8hwPYITAAAAYlGeg9OsWbN0/vnna82aNW7oXlo278nmOyF+5zmdf76frQEAAAACEpysAETnzp31+eefq27duplW2EP8OO44b16TZWib5wQAAADEojwHpx9//FEffvihmjRpUjgtQlSx4hBt2kjffuttO3dKFSv63SoAAADA53WcjjvuOK1cubKAm4FYGK5n093mzPG7NQAAAEAAepyuvfZa3XTTTdqwYYPatGlzWHW9tm3bFmT7EAV69JBeeCF1ntMpp/jdIgAAAMDn4DRw4ED3r63nFGHznKxQBMUh4lPGAhEAAACA4j04rVq1qnBagqjVqJFUu7a0caM0c6Y3ZK9YngeBAgAAADEUnI4++ujCaQmillXVs16n0aOl7dul5culVq38bhUAAABQcHLdL3DVVVdp165dKbffeecd7d69O+X2tm3bdPrppxdg0xBNGK4HAACAWJbr4PTiiy9qz549KbevuOIKbbSxWf+zf/9+ffnll3l68ZEjR6pLly6qWLGiatWqpQEDBmjFihXZPmbUqFFuLlXarUyZMnl6XRQ8ghMAAABiWa6DkxV/yO52fkyZMkVXX321Zs2apfHjx+vgwYPq3bt3up6szFSqVEnr169P2dasWXPEbcGR6dhRKlXK26dABAAAABTvc5wK0rhx4w7rTbKep/nz5+vEE0/M8nHWy1SnTp0iaCFyyzr9OnXyikP88IO0ebNUsybvHwAAAGKDr8Epo+1WWUBStWrVsj3P5lpZkYrk5GR17NhRDzzwgFplUY3AhhDaFrFjxw73rz3WNr9ZG6z3LghtOVKJiSHNnBly+9OnJ+uss/xuUeyJpesFRYNrBlwz4HsGQZMcoL9n8tKGPAWnu+66S+XKlXP7Bw4c0P3336/KlSu722nnP+W30cOGDVOPHj3UunXrLM9r3ry5Xn31VbfQrgWtRx55RN27d9eyZctUv379TOdRjRgx4rDjmzdv1r59++Q3+7nt57CLp1iU1/Bu2bK0pKpuf+LEPerWLbWYCApGLF0vKBpcM+CaAd8zCJrkAP09s3PnzlyfGwrncrLSSSed5IbI5WTSpEnKjyuvvFJjx47VtGnTMg1AWbF5US1atNCgQYN033335arHKSEhQVu3bnVzpYJw4ViIq1mzpu8XzpHasEE66ijvZzjhhLAmTz7yeXCI3esFRYNrBlwz4HsGQZMcoL9nLBtUrVrVBbmcskGue5wmT56swnLNNdfos88+09SpU/MUmkzJkiXVoUMHrVy5MtP7S5cu7baM7EPy+4OKsEAapPbkV716UuPG0s8/S3PnhpSUFEopGIGCEyvXC4oO1wy4ZsD3DIImFJC/Z/Ly+kfU0unTp6frzckr6+yy0DR69Gh9/fXXatSoUZ6f49ChQ1qyZInq1q2b73ag4MuS2yjIRYt4ZwEAABAbjig49e3bV7/++mu+H2+lyN988029/fbbbi2nDRs2uG3v3r0p5wwePFjDhw9PuX3vvffqq6++0s8//6wFCxbowgsvdOXIL7300iP5UVBAWM8JAAAAseiIgtORruX0/PPPu/GENn/Keowi23vvvZdyztq1a91aTRE2N+myyy5z85pOP/10Ny5xxowZatmy5RG1BQWD4AQAAIBY5Gs58twEr4xzqx5//HG3IZisIGKFClYy3oZy2mdsY1j9bhUAAADgY4/Tiy++qNq1ax9hExBLiheXunXz9n/7TVq3zu8WAQAAAD4Hp/PPP98VZxgzZoyWL19eAM1BrA3Xs14nAAAAIO6C07nnnqtnnnnG7VsRh86dO7tjtiDtRx99VBhtRJRhnhMAAAAU78HJ1lo64YQT3L6VEbd5Stu2bdNTTz2lf/3rX4XRRkQZG6oXmdc0Y4bfrQEAAAB8CE5WBa9atWpuf9y4cRo4cKDKlSunM844Qz/++GMBNAnRrnJlr0iEWbzYKxQBAAAAxFVwSkhI0MyZM7V7924XnHr37p1SJrxMmTKF0UZE8XC9Q4ekuXP9bg0AAABQxMFp2LBhuuCCC1S/fn3Vq1fPrcEUGcLXpk2bI2wOYgXznAAAABDX6zhdddVV6tq1q9atW6dTTz1VxYp52atx48bMcUIKghMAAAAU7wvgWiU924yVI1+yZIm6d++uqlWrFnT7EKWOOUaqWVPavFmaOVNKTpb+l7EBAACA+Biq98orr6SEpp49e6pjx45u7tPkyZMLo42IQlZVL9LrtHWrtGKF3y0CAAAAijA4ffjhh2rXrp3b/7//+z+tWrVK33//vW644QbdfvvtR9AUxBqG6wEAACBug9Pvv/+uOnXquP0vvvhC55xzjpo1a6aLL77YDdkDMgtO06fzvgAAACCOglPt2rX13XffuWF6Vo7cCkSYPXv2qHjx4oXRRkSpTp2kkiW9fRbCBQAAQFwFp7///e8699xz1bp1a4VCIfXq1csdnz17to499tjCaCOiVNmyUseO3r7Ncfr9d79bBAAAABRRVb177rnHhSYrR27D9EqXLu2OW2/Tbbfdls9mIJaH682e7e3PmiWdeabfLQIAAACKqBz5X/7yl8OODRkyJD9PhRjXo4f0+OOpw/UITgAAAIhG+VpZZ8qUKerXr5+aNGnitrPOOkvffPNNwbcOUS8xMXWfeU4AAACIm+D05ptvunlN5cqV03XXXee2smXL6pRTTtHbb79dOK1E1KpXT2rY0NufM0c6eNDvFgEAAABFMFTv/vvv10MPPeTWbYqw8PTYY4/pvvvu0/nnn5+PZiDW5zmtXi3t3SstXix17ux3iwAAAIBC7nH6+eef3TC9jGy4ni2GC2TEQrgAAACIu+CUkJCgiRMnHnZ8woQJ7j4gIxbCBQAAQNwN1bvpppvc0LxFixap+//+Ip4+fbpGjRqlJ598sjDaiCjXpo1Uvry0ezcFIgAAABAnwenKK69UnTp19Oijj+r99993x1q0aKH33ntP/fv3L4w2IsqVKCEdd5z09dfSL79I69ZZz6XfrQIAAAAKKTglJSXpgQce0MUXX6xp06bl5aGIc9Y5acEpUpb8vPP8bhEAAABQSHOcSpQo4SrqWYAC8oICEQAAAIir4hC2XpMtgAvkBQvhAgAAIK7mOPXt21e33XablixZok6dOqm8zfrPUJYcyKhKFalVK2nZMmnhQq9QRIZLBwAAAIid4HTVVVe5f23B24xCoZAOHTpUMC1DTA7Xs+Bkl8i8eVLPnn63CAAAACikoXrJyclZboQmZId5TgAAAIib4ATkF8EJAAAAMR+cvv76a7Vs2VI7duw47L7t27erVatWmjp1akG3DzGkaVOpevXUkuTJyX63CAAAACjg4PTEE0/osssuU6VKlQ67r3Llyrriiiv0+OOP5/bpEIdCodRepz/+kH74we8WAQAAAAUcnBYvXqzTTjsty/t79+6t+fPn5/bpEKcYrgcAAICYDk4bN25UyZIls10cd/PmzQXVLsQoghMAAABiOjgdddRRWrp0aZb3f/vtt6pbt25BtQsxqnNnC9mp85wAAACAmApOp59+uu68807t27fvsPv27t2ru+++W2eeeWZBtw8xplw5qWNHb3/5cm+uEwAAABAzC+Decccd+vjjj9WsWTNdc801at68uTv+/fff69lnn3VrON1+++2F2VbE0HC9OXO8/VmzLJT73SIAAACggIJT7dq1NWPGDF155ZUaPny4wuGwOx4KhdSnTx8XnuwcIDfB6YknUofrEZwAAAAQM8HJHH300friiy+0detWrVy50oWnpk2bqmrVqoXXQsScxMTUfeY5AQAAIOaCU4QFpS5duhR8axAX6teXGjSQ1q6VZs+WkpJSC0YAAAAAUV0cAiiMsuR79tgaYby3AAAACDaCE3zBek4AAACIJgQn+ILgBAAAgGhCcIIv2rb11nQyFIgAAABA0BGc4IuSJaWuXb19KxLxyy98EAAAAAgughN806NH6v7MmXwQAAAACC6CE3zDPCcAAABEC4ITfNOtW+o+85wAAAAQZAQn+KZaNalFC29/wQJp714+DAAAAAQTwQmBGK6XlCTNm8eHAQAAgGAiOCEw85ymT/ezJQAAAEDWCE7wFQUiAAAAEA0ITvBVs2beXKdIgYhwmA8EAAAAwUNwgr8XYDEpMdHb37JF+vFHPhAAAAAEj6/BaeTIkerSpYsqVqyoWrVqacCAAVqxYkWOj/vggw907LHHqkyZMmrTpo2++OKLImkvCgfD9QAAABB0vganKVOm6Oqrr9asWbM0fvx4HTx4UL1799bu3buzfMyMGTM0aNAgXXLJJVq4cKELW7YtXbq0SNuOgtOjR+o+6zkBAAAgiELhcHBmlWzevNn1PFmgOvHEEzM957zzznPB6rPPPks51q1bN7Vv314vvPBCjq+xY8cOVa5cWdu3b1elSpXkt+TkZG3atMn93MVs3Foc2rNHso/i0CGpVSuJDJw1rhfkFdcMuGZQ2PieQTRfM3nJBiUUINZgUy1SLSATM2fO1I033pjuWJ8+fTRmzJhMz9+/f7/b0r45kQ/MNr9ZGyy7BqEtfilTRurQIaR580Jatkz6449kVanid6uCiesFXDPgewZBw+8mRPM1k5c2BCY4WaOHDRumHj16qHXr1lmet2HDBtWuXTvdMbttx7OaRzVixIhMe7f27dunIPzcFhjt4vE7cfupXbuKmjevvNsfN26b/vSnA343KZC4XsA1A75nEDT8bkI0XzM7d+6MvuBkc51sntK0adMK9HmHDx+erofKepwSEhJUs2bNwAzVC4VCrj1+Xzh+OuUU6ZVXvP3ly6vqr38NzAjSQOF6AdcM+J5B0PC7CdF8zVixuagKTtdcc42bszR16lTVr18/23Pr1KmjjRs3pjtmt+14ZkqXLu22jOxD8vuDirALJ0jt8cPxx6fuz5hh70fIz+YEGtcLuGbA9wyCht9NiNZrJi+v72tLrXvOQtPo0aP19ddfq1GjRjk+JjExURMnTkx3zCry2XFEr4QEKZKZZ8+WkpL8bhEAAAAQkOBkw/PefPNNvf32224tJ5unZNvevXtTzhk8eLAbbhdx/fXXa9y4cXr00Uf1/fff65577tG8efNcAENsrOdk1eiXLPG7NQAAAEBAgtPzzz/vJoaddNJJqlu3bsr23nvvpZyzdu1arV+/PuV29+7dXdD6z3/+o3bt2unDDz90FfWyKyiB6MBCuAAAAAgqX+c45WYJqcmTJx927JxzznEbYjs4XX21n60BAAAAUsVvNQIETvv2UtmyqcEJAAAACAqCEwKjZEmpa1dvf/Vq6bff/G4RAAAA4CE4IbDD9WbO9LMlAAAAQCqCEwKFAhEAAAAIIoITAqVbt9R95jkBAAAgKAhOCJQaNaTmzb39+fOlNEt6AQAAAL4hOCGww/UOHpTuvVc6cMDvFgEAACDeEZwQOGeckbr/4INSly5e7xMAAADgF4ITAufPf5buvFMqXty7/e230nHHScOHS/v2+d06AAAAxCOCEwInFPKG6M2ZI7Vr5x07dMjrfbJFcikaAQAAgKJGcEJgdewozZ0r3XeftziuWbFCOv546frrpd27/W4hAAAA4gXBCYFmgemOO6SFC6WuXb1j4bD01FNSmzbSxIl+txAAAADxgOCEqNCqlTdE75FHpDJlvGOrVkm9ekmXXy5t3+53CwEAABDLCE6IGlYs4qabvGIRJ56Yevyll7xg9fnnfrYOAAAAsYzghKjTtKk0aZL03HNShQresV9/lc48U/rb36QtW/xuIQAAAGINwQlRqVgx6corpaVLpT59Uo+/+abUsqX04Yd+tg4AAACxhuCEqHb00dLYsdJrr0lVqnjHNm2SzjlHGjhQ2rDB7xYCAAAgFhCcEBPrPl10kfTdd1L//qnHP/7Y6316/XWvEh8AAACQXwQnxIy6daXRo6V335Vq1PCObd0qDRkinXGGtG6d3y0EAABAtCI4IeZ6n847z+t9GjQo9bgN57PKey++KCUn+9lCAAAARCOCE2JSzZrS229Ln3zi9USZnTuloUO9tZ9++snvFgIAACCaEJwQ0846y+t9uuSS1GNWyrxNG+mJJ6RDh/xsHQAAAKIFwQkxz6rtvfyy9NVXXhU+s3evdMMN0gknSMuX+91CAAAABB3BCXHj1FO9dZ+uuSb12MyZUvv20gMPSAcP+tk6AAAABBnBCXGlQgXp6aelqVOlpk29YwcOSLffLh13nLRokd8tBAAAQBARnBCXbIje4sXSzTdLxf73/4KFC6UuXaQ775T27/e7hQAAAAgSghPiVtmy0kMPSbNmSa1be8eSkqR//Uvq2FGaPdvvFgIAACAoCE6Ie9bLNH++dPfdUokS3tthlfi6d5f+8Q9pz564f4sAAADiHsEJkFSqlHTPPV6A6tTJe0tsodxHH5XatZOmTOFtAgAAiGcEJyCNtm29oXsPPiiVLu0dW7lSOukk6aqrvEV0AQAAEH8ITkAGNlzv1lu94hE2XC/i+ee9uVBffslbBgAAEG8ITkAWmjf3ypY/+aRUrpx3bO1a6bTTpL//Xdq6lbcOAAAgXhCcgGwULy5dd520ZIl0yimpx0eNklq2lMaM4e0DAACIBwQnIBcaN5bGj5deekmqVMk7tmGDdPbZ0l//Km3ezNsIAAAQywhOQC6FQtKll0rLlklnnpl6/L33pBYtpHfekcJh3k4AAIBYRHAC8qh+fenTT6U335SqVfOObdkinX++1L+/9OuvvKUAAACxhuAE5LP36YILvIVyzzkn9fj//Z/UqpX0yiv0PgEAAMQSghNwBGrXlt5/X/roI2/fbN/uDenr3VtavZq3FwAAIBYQnIAC8Oc/e71PgwenHpswwVv36ZlnpORk3mYAAIBoRnACCojNd/rvf6UvvpASErxju3dL114r9ewp/fADbzUAAEC0IjgBBaxvX2npUmno0NRj06ZJ7dpJDz8sJSXxlgMAAEQbghNQCGytp+eflyZN8taAMvv2SbfcIiUmegvqAgAAIHoQnIBCdNJJ0rffSjfc4FXiM/PmSZ06SSNGSAcO8PYDAABEA4ITUMjKl5cee0yaPt1bKNccPCjdc4/UubMXpAAAABBsBCegiNgQvQULpH/+Uype3DtmQ/aOO0667TZp714+CgAAgKAiOAFFqEwZ6f77pblzpfbtvWNWqvzf//ZuW68UAAAAgofgBPigQwdpzhzpX/+SSpXyjlm58hNOkK6/Xtq1i48FAAAgSAhOgE9KlpRuv11auNAbrmfCYempp6Q2baSJE/loAAAAgoLgBPisZUtviN6jj0ply3rHVq+WevWSLrtM2r7d7xYCAACA4AQEgBWLuPFGr3R5z56px19+2QtWn33mZ+sAAABAcAICpEkT6euvvcVzK1Twjv32m9Svn3ThhdLvv/vdQgAAgPhEcAICplgxaehQadky6bTTUo+/9ZbUunVIn35axs2FAgAAQNEhOAEB1aCB9MUX0qhRUpUq3rHNm0O64ooqatw4pEsvld57z4753VIAAIDYR3ACAiwUkoYMkb77Tjr77NTja9eG9Mor0l//KtWqJXXsKN16qzR+PAvpAgAAFAaCExAF6taVPvpI+uCDZJ1wwn6VLp1+rJ6VNH/oIal3b6lqVenUU71FdRcs8BbYBQAAQBQHp6lTp6pfv36qV6+eQqGQxowZk+35kydPdudl3DZs2FBkbQb87H3685+l99/fqi1bwvrqK+nmm73FdNPav1+aMEG67TapUyevR+q887wKfWvW+NV6AACA6FbCzxffvXu32rVrp4svvlh/tr8Ic2nFihWqVKlSyu1a9pchEEdsvSfrVbLN2Dwnq8ZnQ/VsW7s29dwtWyxseVukcl/ksSefnDp/CgAAAAENTn379nVbXllQqpLLv/b279/vtogdO3a4f5OTk93mN2tDOBwORFsQfFldL9WrS+ec421WcW/lSq/Xafz4kCZNsus+lHKu3WeblTwvViysLl2kU06xBXfDSkyUSpXy4QdDoeE7BlwzKGx8zyCar5m8tMHX4JRf7du3d2GodevWuueee9SjR48szx05cqRGjBhx2PHNmzdr3759CsKHtX37dnfxFLM61EABXC+VK0sDB3pbUpK0eHFJTZlSSt98U1rz5pVUUpIXpJKTQ5o9W2574IGQypZNVmLiQZ144n6deOIBHXtskhsiiOjFdwy4ZsD3DIImOUB//+7cuTPX54bC1uIAsLlKo0eP1oABA7IdomfznDp37uyC08svv6w33nhDs2fPVkcrK5bLHqeEhARt3bo13XA/Py8cC3E1a9b0/cJB8BXE9bJrlzRlijRxYsj1Si1blnUyqlMnnNIb1auXVK/eETQevuA7Blwz4HsGQZMcoL9/LRtUrVrVBbmcskFU9Tg1b97cbRHdu3fXTz/9pMcff9wFqMyULl3abRnZh+T3B5U2NAapPQi2I71e7DuhXz9vM7/9ZiHKmxtlQWr9+tRzN2wIuYV333rLC1ctW3pzoyxE9ewpVaxYID8SChnfMeCaAd8zCJpQQP7+zcvrR1VwykzXrl01bdo0v5sBRC3rRfrb37zN+p9tzahIiJo82Yq4pJ5r99n25JNSiRJSt26phSZsrpQdAwAAiEVR/2fOokWLVNcWuQFwxGw+U6tW3jZsmHTggDRrVmqQmjMndV0omztl/83Ctrvv9nqyrEpfpEeqWTPv+QAAAGKBr8Fp165dWmnlvf5n1apVLghVq1ZNDRo00PDhw/Xrr7/q9ddfd/c/8cQTatSokVq1auUKO9gcp6+//lpf2YI2AAqcVdg78URvu+8+ads2rxcqUvb8xx9Tz7WClZ984m0mISE1RNk8KVYNAAAA0czX4DRv3jydbP+J+n9uvPFG9++QIUM0atQorV+/XmvTLEhz4MAB3XTTTS5MlStXTm3bttWECRPSPQeAwmOrAFj9lkgNF1tQ1yt77s2T+v331HPXrZNefdXbTPv2XoiyMHX88VK5cnxSAAAgegSmql5RVs6oXLlyripnFFVVkU2bNrm1qfyeHIfgC/L1YkP4Fi9OHdb3zTdSVhX/rV6LrSIQ6ZHq0EEqXryoWxwfgnzNIJi4ZsA1g3j6ntmRh2zAb1EABcK+9ywA3XKLZKNn//jDC1G33irZagFp5zvZCgFffy0NH+4VlbBhfOeeK/3nPzZklw8EAAAET9QXhwAQTGXLer1Jtj34oDeMz8JSZH6UDfOLsJD1wQfeZo45JnVY35/+JFWt6tuPAQAA4BCcABSJGjW8XiXbbIDwTz+lhigLVNu3p55r99n24oteT1anTqnD+rp394b6AQAAFCWCE4AiZ8P2mjTxtiuv9Eqbz5+fOj9qxgzp4MHUuVNz53rbAw94RSWsyl+kR6pNG8qeAwCAwkdwAuA7Wzj3uOO87Y47bKkCaerU1Ip9S5emnrtnjzRunLeZ2rW9cucWoqzgROPGFJoAAAAFj+AEIHAqVJBOP93bzPr1XrnzyNA+ux2xcaP09tveZmwYX/PmUsuWUosW3r+2We+WrUsFAACQHwQnAIFXt6504YXeZvOjli9PHdZnC/JaD1Xain3ffuttGXu1LDxFglQkVFnIskIWAAAA2SE4AYi6+VGR8HP99bYwtjR7tldgYskS6bvvpB9/9OZNpWW3v//e2z7+OP3z2fC+tL1Ttm9bxYpF/uMBAICAIjgBiGo2/O6EE7wtwsLUypVez5QFKdts30KT9UilFanwZ9tnn6W/LyEhfaCKhKpq1YrmZwMAAMFBcAIQk2EqEnQGDkw9fuiQt8Bu2kAVCVW7dx/+POvWeZst6JuWFaTIOIfKNlvIN+1CvwAAIHYQnADEjeLFU8ug9+uXetxKnv/yS2qIShuqtm07/HmsIIVtkyalP24L9WacQ2Vb/foEKgAAoh3BCUDcs0V2GzTwttNOSz+MzwJS2p6pyP6mTYe/bVu3StOne1vGKoEZ51DZvw0bUjodAIBoQXACgCzYsLs6dbztT39Kf9+WLYfPobJ/recqI6v6F1nEN60yZVJLp6cNVdYjVrIkHwsAAEFCcAKAfKheXTr+eG9La8cOrwhFxjlUNrfKerDS2rdPWrzY29J9MZeQmjU7fA6VHbOwBQAAih7BCQAKUKVKUteu3pbWnj3SihWHz6Gy6n9WtCJj6fTI/R99lH5IoZVOzziH6thjveGAAACg8BCcAKAIlCsndejgbWlZ6XRbdyrjHCoLWXZfWlbEwoKWbZ9+mv4+m5+VcQ6V/Vu5cuH/bAAAxAOCEwD4XDq9VStvy9jrZMP7Ms6hsn+t9yqjtWu9bdy49Mfr1AmpceOqatw45Kr7HXVU+s1Kq9vQQAAAkD1+XQJAAFmYadrU2/r3T9/rZGtLZZxDZf9u337482zYENKGDaU1Y0bmr2PD/6z4RcZAlXGrWLHwflYAAKIBwQkAoogFnaOP9ra+fVOPW+GJ9eszq/QX1ubNWa/Ka0Hst9+8LWPVv7QsOOUUrqz3ytbKAgAgFhGcACBGSqfXq+dtp5ySejw5OayVKzfpwIGaWr++mH79VZlutl5Vxqp/ae3c6VULtC0r9F4BAGIZwQkAYlylSmHVqiW1bp31OQcP2rC+zENV2i2z+VUR9F4BAGIZwQkA4BbcTUjwtqxYj5TNo8opXG3adOS9VzbkLzdzryjDDgAoKgQnAECuhwNWqeJtGasAZuy9svlWkSBl86fy2ntla1tFzstp3aycwpX1tjH3CgBwpAhOAIAC772ydaVsK+zeqx07vM0KYWTFQlPdupmHKuvVsqIWttWo4c3TAgAgMwQnAEDU9F5lte3dm33v1S+/eFt2LDTVrJkapLLb7DzWvwKA+EJwAgBEfe/Vtm25673KjhW3sOqCtuUm+FWvnruQZUMFbaFjAEB0IzgBAKKahZiqVb0tu8qBBw4c3ntllQQjYSmyWcCynq7sWFj7/XdvW7Ys5zZa23IbssqWzft7AAAofAQnAEBcsF6fyOLB2Yn0YGUMVFlt+/bl/Npbt3pbdpUE0y42nJuQZRtVBQGg6BCcAADIogfr2GNzDllWXj23IWv37pzfans+21auzPnccuVyH7KsAqH9bACA/CE4AQCQTxZELJDY1rRpzudbcMptyLJqgTmxku6rVnlbTkqXzn3hi+wqGQJAvCI4AQBQRMqXlxo39rac2BDAzAKVzcHKeOyPP3J+vv37pbVrvS17xVSiRG1Vq+YVwIhsVq497e2Mm51vxTwAIFYRnAAACKAyZXI3JytS+GLz5tz1ZFlBi5x6lJKSQi6g5VSJMKPKlbMOVlkFLxtuyBBCANGA4AQAQAwUvogs6puTpCQvPGUVrDZsCGv9+iTt2FFCv/8ecsMBc8sWNbbt559z/xgbQpibHq2099v6XyxWDKCoEZwAAIgjtnBvnTrelpnk5LA2bdqiWrVqqVixkBsyuGVL1puFsIzHrIJgbudJ2RDC337zttyy0GTFO/IylNA2C2kAkF8EJwAAkO2Qwdz2ZkUcOuSVdM8qWGUVvmzIYW7YYsWRx+aFlW/Pba9WZLPy8AwlBGAITgAAoEAVL54aPJo1y91jrIfKqg7mpWfLttxUH4zYtcvb1qzJ/WOs4EWkUEZmwSrjMbttvWH2HgCILQQnAADgO+vVsR4h23JTECPi4EGvqmBeerbsfOsVy+3zR+Z/5eVnsXlY2YWrzHq8rHcPQHARnAAAQNSyHqHIGlS5ZUP9rKcqLz1btuW2UIb1ntk8L9tys5Bx2nL12YUrhhIC/iI4AQCAuGLFJaxHyLZjjsn94/buzV3YSnvb5nrllg1VtC3ntbbSB8echg5mDGAMJQTyh+AEAACQC2XLSvXre1tuWfl3GxqYl7BlW16GEm7Y4G15GUqYtiphbnu3qEqIeEdwAgAAKKw/tEpItWp5W27ZUD9bDyuvYct6xHL7/BbmbPvxx/xXJYyEq2rVQipZspwLlFZIwxZCtt68yL+VKrHuFmIDwQkAACBAIsUl8jqU0OZg5TVsWUA78qqEIUmVsn2shae0YSrjvzndR28XgoDgBAAAEAPKlfO2hIS8VyXMS9jKS1XCCCvGkZfS8RlZcMpP4IrsW2+ZzW0DjgTBCQAAIE7ltyph2qGEmzcna/VqS0WVtH17MXefFcWwLbKf9tj+/Xlvpz0mr2XhM/biWYjKa+BK+2+pUvl7bcQOghMAAAByzXpurLiEbU2aeEFq06Z9qlWrUq56dfbt84JUxlCVXeBKe19+eq5sXlfk8XlZADljcZC8Bq60m/UGWoBD9CI4AQAAoMjYQr+25aWXKy0bJrhzZ94DV9p9G6KYV1Z8w7b16/Pfu5c2SFnwzMs+PV7+IzgBAAAgahQvnhomjj46f71PkV6vvASutP9acMsrC2ubN3tbfliPVX5Dl/WAMcfryBGcAAAAEDdsuJwNu7OtTp3893rZkMHcBK/ItnVr+vMtwOWFVU207bff8vczRyobpg1UuQ1e5cszzNAQnAAAAIA89npF5nnlh80LSxu80oaq3Ozv3p2314usDWZbfuZ42XpkVfLZ22X7sTLMkOAEAAAAFCEbNhcJFvlhw/6y6s3Kad/+zescr6Qkryy9bUdSWCMSqCpXDqls2coaPFjq319Rg+AEAAAARBErNFGzprfld45XXnu5tqbZz+sww8MLa1h5wbLq1i1Z0YTgBAAAAMThHK969fI3zHDnzvz3eO3alfpcVrQimhCcAAAAAOR6mGHl/y0mnJ+qhjbs748/kvXTT1vUrFn1qHrXCU4AAAAAiiZ8lJBq1LCeq0P5Lq7hl1ys71x4pk6dqn79+qlevXoKhUIaM2ZMjo+ZPHmyOnbsqNKlS6tJkyYaNWpUkbQVAAAAQPzyNTjt3r1b7dq107PPPpur81etWqUzzjhDJ598shYtWqRhw4bp0ksv1ZdfflnobQUAAAAQv3wdqte3b1+35dYLL7ygRo0a6dFHH3W3W7RooWnTpunxxx9Xnz59CrGlAAAAAOJZVM1xmjlzpnr16pXumAUm63nKyv79+90WscNWG3MVQZLd5jdrQzgcDkRbEHxcL+CaAd8zCBp+NyGar5m8tCGqgtOGDRtUu3btdMfstoWhvXv3qqzVVcxg5MiRGjFixGHHN2/erH1WxD4AH9b27dvdxVPMypQAXC/gOwY+4vcSuGYQT98zO622eiwGp/wYPny4brzxxpTbFrISEhJUs2ZNVapUSUG4cKwwhrXH7wsHwcf1Aq4Z8D2DoOF3E6L5milTpkxsBqc6depo48aN6Y7ZbQtAmfU2Gau+Z1tG9iH5/UFF2IUTpPYg2LhewDUDvmcQNPxuQrReM3l5/aj6Sz0xMVETJ05Md2z8+PHuOAAAAAAUFl+D065du1xZcdsi5cZtf+3atSnD7AYPHpxy/tChQ/Xzzz/rlltu0ffff6/nnntO77//vm644QbffgYAAAAAsc/X4DRv3jx16NDBbcbmItn+XXfd5W6vX78+JUQZK0X++eefu14mW//JypK//PLLlCIHAAAAUKh8neN00kknuWoaWRk1alSmj1m4cGEhtwwAAAAAonSOEwAAAAD4geAEAAAAADkgOAEAAABADghOAAAAAJADghMAAAAA5IDgBAAAAABBLkfuh0j58x07digIkpOTtXPnTpUpU0bFipFjwfUCvmPgL34vgWsG8fQ9s+N/mSC7JZLiNjjZh2QSEhL8bgoAAACAgGSEypUrZ3tOKJybeBVjCfe3335TxYoVFQqF/G6OS7kW4tatW6dKlSr53RwEHNcLuGbA9wyCht9NiOZrxqKQhaZ69erl2PsVdz1O9obUr19fQWMXjd8XDqIH1wu4ZsD3DIKG302I1msmp56mCCbVAAAAAEAOCE4AAAAAkAOCk89Kly6tu+++2/0LcL2A7xj4jd9L4JoB3zOZi7viEAAAAACQV/Q4AQAAAEAOCE4AAAAAkAOCEwAAAADkgOAEAAAAADkgOPno2WefVcOGDVWmTBkdd9xxmjNnjp/NQRGZOnWq+vXr51aoDoVCGjNmTLr7rV7LXXfdpbp166ps2bLq1auXfvzxx3Tn/PHHH7rgggvconFVqlTRJZdcol27dqU759tvv9UJJ5zgri9bnfuhhx4qkp8PBW/kyJHq0qWLKlasqFq1amnAgAFasWJFunP27dunq6++WtWrV1eFChU0cOBAbdy4Md05a9eu1RlnnKFy5cq557n55puVlJSU7pzJkyerY8eOrrJakyZNNGrUKD7SKPT888+rbdu2KYtLJiYmauzYsSn3c70gOw8++KD7/TRs2DCuGWTpnnvucddJ2u3YY4+N7WvGquqh6L377rvhUqVKhV999dXwsmXLwpdddlm4SpUq4Y0bN/JxxLgvvvgifPvtt4c//vhjq2gZHj16dLr7H3zwwXDlypXDY8aMCS9evDh81llnhRs1ahTeu3dvyjmnnXZauF27duFZs2aFv/nmm3CTJk3CgwYNSrl/+/bt4dq1a4cvuOCC8NKlS8PvvPNOuGzZsuEXX3yxSH9WFIw+ffqEX3vtNfdZLlq0KHz66aeHGzRoEN61a1fKOUOHDg0nJCSEJ06cGJ43b164W7du4e7du6fcn5SUFG7dunW4V69e4YULF7rrsEaNGuHhw4ennPPzzz+Hy5UrF77xxhvD3333Xfjpp58OFy9ePDxu3Dg+yijz6aefhj///PPwDz/8EF6xYkX4n//8Z7hkyZLuGjJcL8jKnDlzwg0bNgy3bds2fP3116cc55pBRnfffXe4VatW4fXr16dsmzdvjulrhuDkk65du4avvvrqlNuHDh0K16tXLzxy5Ei/mgQfZAxOycnJ4Tp16oQffvjhlGPbtm0Lly5d2oUfY18c9ri5c+emnDN27NhwKBQK//rrr+72c889F65atWp4//79Kefceuut4ebNmxfRT4bCtGnTJncNTJkyJeUasT+KP/jgg5Rzli9f7s6ZOXOmu22/kIoVKxbesGFDyjnPP/98uFKlSinXyS233OJ+CaZ13nnnueCG6GffCS+//DLXC7K0c+fOcNOmTcPjx48P9+zZMyU48R2DrIJTu3btMr0vVq8Zhur54MCBA5o/f74bghVRrFgxd3vmzJl+NAkBsWrVKm3YsCHdtVG5cmU3lDNybdi/Njyvc+fOKefY+XYNzZ49O+WcE088UaVKlUo5p0+fPm5419atW4v0Z0LB2759u/u3WrVq7l/7Pjl48GC668aGSzRo0CDdddOmTRvVrl073TWxY8cOLVu2LOWctM8ROYfvpeh26NAhvfvuu9q9e7cbssf1gqzYsCobNpXxe4BrBln58ccf3dSDxo0buykENvQulq8ZgpMPfv/9d/eLLO2FYuy2/dGM+BX5/LO7NuxfGwecVokSJdwf0WnPyew50r4GolNycrKbd9CjRw+1bt065TO1kGyBOrvrJqdrIqtz7JfY3r17C/XnQsFbsmSJm1dg8wKGDh2q0aNHq2XLllwvyJSF6wULFrg5lRnxHYPMHHfccW6+0bhx49y8SvuPvza3eufOnTF7zZQo8lcEABzRfxFeunSppk2bxruIbDVv3lyLFi1yPZQffvihhgwZoilTpvCu4TDr1q3T9ddfr/Hjx7uCQkBu9O3bN2XfitFYkDr66KP1/vvvu+JWsYgeJx/UqFFDxYsXP6yyiN2uU6eOH01CQEQ+/+yuDft306ZN6e63CjRWaS/tOZk9R9rXQPS55ppr9Nlnn2nSpEmqX79+ynH7TG0I8LZt27K9bnK6JrI6x6qyxeovwVhm/7XXKlB16tTJ9SK0a9dOTz75JNcLDmPDquz3ilUusxEMtlnIfuqpp9y+/Rd+vmOQE+tdatasmVauXBmz3zMEJ59+mdkvsokTJ6YbfmO3bfw54lejRo3cl0Taa8O6o23uUuTasH/ti8h+0UV8/fXX7hqy/9oTOcfKntv44gj7L4n2X6CrVq1apD8TjpzVEbHQZEOt7LO26yQt+z4pWbJkuuvG5rPZWPO0140N3Uobuu2asF8+Nnwrck7a54icw/dSbLDviP3793O94DCnnHKK+36wHsrIZvNobc5KZJ/vGOTElkX56aef3HIqMft7yZeSFHDlyK1S2qhRo1yVtMsvv9yVI09bWQSxW7XIym7aZv8XfOyxx9z+mjVrUsqR27XwySefhL/99ttw//79My1H3qFDh/Ds2bPD06ZNc1WQ0pYjt2o2Vo78b3/7mys/bNeblfOkHHl0uvLKK12J+smTJ6cr+7pnz550ZV+tRPnXX3/tyr4mJia6LWPZ1969e7uS5lbKtWbNmpmWfb355ptd9aNnn32WcuRR6rbbbnNVF1etWuW+R+y2Vd786quv3P1cL8hJ2qp6XDPIzE033eR+L9n3zPTp011ZcSsnbpVfY/WaITj5yGrR2wVl6zlZeXJbkwexb9KkSS4wZdyGDBmSUpL8zjvvdMHHwvUpp5zi1mFJa8uWLS4oVahQwZXt/Pvf/+4CWVq2BtTxxx/vnuOoo45ygQzRKbPrxTZb2ynCgvVVV13lSk7bL5mzzz7bhau0Vq9eHe7bt69b08t+udkvvYMHDx52fbZv3959LzVu3DjdayB6XHzxxeGjjz7afY72h4h9j0RCk+F6QV6DE9cMMrKy4HXr1nXfM/Z3ht1euXJlTF8zIfsff/q6AAAAACA6MMcJAAAAAHJAcAIAAACAHBCcAAAAACAHBCcAAAAAyAHBCQAAAAByQHACAAAAgBwQnAAAAAAgBwQnAAAAAMgBwQkAECgNGzbUE088kevzJ0+erFAopG3bthVquwAA8Y3gBADIFwsr2W333HNPvp537ty5uvzyy3N9fvfu3bV+/XpVrlxZhe2ll15Su3btVKFCBVWpUkUdOnTQyJEjU+6/6KKLNGDAgEJvBwCg6JXw4TUBADHAwkrEe++9p7vuuksrVqxIOWbhIiIcDuvQoUMqUSLnXzs1a9bMUztKlSqlOnXqqLC9+uqrGjZsmJ566in17NlT+/fv17fffqulS5cW+msDAPxHjxMAIF8srEQ26+2xXqbI7e+//14VK1bU2LFj1alTJ5UuXVrTpk3TTz/9pP79+6t27douWHXp0kUTJkzIdqiePe/LL7+ss88+W+XKlVPTpk316aefZjlUb9SoUa436Msvv1SLFi3c65x22mnpgl5SUpKuu+46d1716tV16623asiQIdn2FtlrnnvuubrkkkvUpEkTtWrVSoMGDdL999/v7rcetv/+97/65JNPUnrdrG1m3bp17rH2etWqVXPvwerVqw/rqRoxYoQLjpUqVdLQoUN14MCBlHM+/PBDtWnTRmXLlnVt7tWrl3bv3s3VCwBFhOAEACg0t912mx588EEtX75cbdu21a5du3T66adr4sSJWrhwoQs0/fr109q1a7N9HgsUFjysh8cef8EFF+iPP/7I8vw9e/bokUce0RtvvKGpU6e65//HP/6Rcv+///1vvfXWW3rttdc0ffp07dixQ2PGjMm2DRYIZ82apTVr1mR6vz2/tTES0myzYYQHDx5Unz59XJD85ptv3OtFwlzaYGTvib1PFrbeeecdffzxx+7nNvZcFtIuvvjilHP+/Oc/u548AEARCQMAcIRee+21cOXKlVNuT5o0yf6iD48ZMybHx7Zq1Sr89NNPp9w++uijw48//njKbXueO+64I+X2rl273LGxY8eme62tW7emtMVur1y5MuUxzz77bLh27dopt23/4YcfTrmdlJQUbtCgQbh///5ZtvO3334Ld+vWzT13s2bNwkOGDAm/99574UOHDqWcY8cyPscbb7wRbt68eTg5OTnl2P79+8Nly5YNf/nllymPq1atWnj37t0p5zz//PPhChUquOefP3++e93Vq1fn+H4CAAoHPU4AgELTuXPndLetx8l6ZmwInQ1bs54X60HJqcfJeqsiypcv74aybdq0KcvzbUjfMccck3K7bt26Kedv375dGzduVNeuXVPuL168uBtSmB17jpkzZ2rJkiW6/vrr3XA/G95nPUfJyclZPm7x4sVauXKl63Gyn9c2G663b98+N3QxwopOWLsjEhMT3ftlw/zsvlNOOcUN1TvnnHNckYqtW7dm214AQMGiOAQAoNBYyEnLQtP48ePdMDqbJ2Tzdf7yl7+kG7KWmZIlS6a7bfOHsgsrmZ1fUMPaWrdu7barrrrKzUM64YQTNGXKFJ188smZnm/hx0KZDQ3MbyEMC3b2vs2YMUNfffWVnn76ad1+++2aPXu2GjVqdMQ/EwAgZ/Q4AQCKjM3vsUIIVujBek9s3lDaIglFwQpZWHEKK3seYRX/FixYkOfnatmypfs3UqTBKvzZc6XVsWNH/fjjj6pVq5YLi2m3tCXUrWdq7969KbdtPpX1TiUkJKSEvx49erh5TzY/zF5r9OjR+XgHAAD5QXACABQZq4hnRQ8WLVrkgsL555+fbc9RYbn22mvd+ktWAc9KqNvQOxv6ZuEkK1deeaXuu+8+F/6sQIQFm8GDB7teIxtWF6kIaAUs7Dl///13VxjCClnUqFHDVdKz4hCrVq1yxR2sqt8vv/yS8vzW62YV+7777jt98cUXuvvuu3XNNdeoWLFirmfpgQce0Lx589ywRnsPN2/e7IY8AgCKBsEJAFBkHnvsMVWtWtVVm7NqelZtznpkipqVH7cqdRZ8LPRYz461pUyZMlk+xsp/W1iyOUbNmjXTwIED3flWDc/Kg5vLLrtMzZs3d3O7LFBZyLJ5S1bZr0GDBq4SnoUdC0g2x8nmakXYHCYLlieeeKLOO+88nXXWWSmLCNt59hxWUdBe+4477tCjjz6qvn37FsG7BQAwIasQwVsBAIhn1utlgcbKiVuvUlGz4Yu2DlVOJdEBAP6hOAQAIO7YUDsrstCzZ0/t379fzzzzjBtCZ0MHAQDIDEP1AABxx+YNjRo1Sl26dHEFF6zE+IQJE5gzBADIEkP1AAAAACAH9DgBAAAAQA4ITgAAAACQA4ITAAAAAOSA4AQAAAAAOSA4AQAAAEAOCE4AAAAAkAOCEwAAAADkgOAEAAAAAMre/wNR0W2XWIAuwAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss: 1.0549\n"
     ]
    }
   ],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "steps = [i * eval_interval for i in range(len(losses))]\n",
    "plt.plot(steps, losses, 'b-', linewidth=2, label='Training Loss')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('GPT Training Progress')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Shakespeare-style text:\n",
      "==================================================\n",
      "\n",
      "HORTENSE:\n",
      "Awarl, my lord! do yout sir, care, couse to be gone.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Be so you are too.\n",
      "But, but then, my good lady to London,\n",
      "Who at Warwick timely guests shall left this?\n",
      "\n",
      "HASTINGS:\n",
      "Up, if it put in the dead. Tutus, Murderer,\n",
      "Cannot look without a trembling with him;\n",
      "Which now let me be glad only love to not to;\n",
      "Endured, though trive spoken will ungreet;\n",
      "For they have been still and interruptial.\n",
      "The multitude was to-night, or my eldes\n",
      "Like pathful man came of themal-stoppined.\n",
      "What\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate text with the trained model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(model.generate(context, max_new_tokens=500)[0].tolist())\n",
    "\n",
    "print(\"Generated Shakespeare-style text:\")\n",
    "print(\"=\" * 50)\n",
    "print(generated_text)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Analysis and Attention Visualization\n",
    "\n",
    "Let's analyze what our model has learned by examining the attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: 'To be or not to be'\n",
      "Model prediction for next character: ' '\n"
     ]
    }
   ],
   "source": [
    "def visualize_attention(model, text, layer_idx=0, head_idx=0):\n",
    "    \"\"\"\n",
    "    Visualize attention weights for a given text.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained GPT model\n",
    "        text: Input text to analyze\n",
    "        layer_idx: Which transformer layer to examine\n",
    "        head_idx: Which attention head to examine\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode text and add batch dimension\n",
    "    tokens = torch.tensor(encode(text), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Forward pass with hooks to capture attention weights\n",
    "    attention_weights = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        # This is a simplified version - in practice you'd need to modify\n",
    "        # the attention modules to return weights\n",
    "        pass\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(tokens)\n",
    "    \n",
    "    print(f\"Input text: '{text}'\")\n",
    "    print(f\"Model prediction for next character: '{decode([logits[0, -1].argmax().item()])}'\") \n",
    "\n",
    "# Example usage\n",
    "sample_text = \"To be or not to be\"\n",
    "visualize_attention(model, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Comparison and Analysis\n",
    "\n",
    "Let's compare our model's performance with different architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Model Architecture Summary\n",
      "========================================\n",
      "Vocabulary size: 65\n",
      "Block size (context length): 256\n",
      "Embedding dimension: 384\n",
      "Number of layers: 6\n",
      "Number of attention heads: 6\n",
      "Head size: 64\n",
      "Dropout rate: 0.2\n",
      "Total parameters: 10,788,929\n",
      "Model size: 41.16 MB\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count the number of trainable parameters in a model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def model_summary():\n",
    "    \"\"\"Print a summary of the model architecture\"\"\"\n",
    "    print(\"GPT Model Architecture Summary\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Vocabulary size: {vocab_size:,}\")\n",
    "    print(f\"Block size (context length): {block_size}\")\n",
    "    print(f\"Embedding dimension: {n_embd}\")\n",
    "    print(f\"Number of layers: {n_layer}\")\n",
    "    print(f\"Number of attention heads: {n_head}\")\n",
    "    print(f\"Head size: {n_embd // n_head}\")\n",
    "    print(f\"Dropout rate: {dropout}\")\n",
    "    print(f\"Total parameters: {count_parameters(model):,}\")\n",
    "    \n",
    "    # Calculate model size in MB\n",
    "    param_size = count_parameters(model) * 4  # 4 bytes per float32\n",
    "    print(f\"Model size: {param_size / (1024**2):.2f} MB\")\n",
    "\n",
    "model_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Insights and Improvements\n",
    "\n",
    "### What We've Learned:\n",
    "\n",
    "1. **Self-Attention**: The model learns to attend to relevant parts of the input sequence\n",
    "2. **Positional Encoding**: Essential for the model to understand sequence order\n",
    "3. **Multi-Head Attention**: Different heads capture different types of relationships\n",
    "4. **Residual Connections**: Enable training of deep networks\n",
    "5. **Layer Normalization**: Stabilizes training and improves convergence\n",
    "\n",
    "### Potential Improvements:\n",
    "\n",
    "- **Larger Model**: More layers, heads, and embedding dimensions\n",
    "- **Better Tokenization**: Subword tokenization (BPE, SentencePiece)\n",
    "- **Advanced Techniques**: \n",
    "  - Gradient clipping\n",
    "  - Learning rate scheduling\n",
    "  - Weight decay\n",
    "  - Rotary positional embeddings (RoPE)\n",
    "- **Data Augmentation**: More diverse training data\n",
    "- **Regularization**: Dropout, weight decay, early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "We've successfully implemented a GPT-style transformer from scratch! This model demonstrates the key concepts:\n",
    "\n",
    "- **Self-attention mechanism** for capturing long-range dependencies\n",
    "- **Multi-head attention** for learning diverse representations\n",
    "- **Positional embeddings** for sequence understanding\n",
    "- **Residual connections and layer normalization** for stable training\n",
    "\n",
    "The transformer architecture has revolutionized natural language processing and continues to be the foundation for state-of-the-art language models like GPT-3, GPT-4, and beyond.\n",
    "\n",
    "### Next Steps:\n",
    "1. Experiment with different hyperparameters\n",
    "2. Try different datasets and tokenization schemes\n",
    "3. Implement more advanced transformer variants\n",
    "4. Explore fine-tuning pre-trained models\n",
    "5. Study attention patterns and model interpretability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
